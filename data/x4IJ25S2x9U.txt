[Music] to begin by acknowledging the traditional owners of the land on which we meet today the gadigal people of the eora nation I'd like to pay my respects to their eldest Elders past present and emerging welcome to today's session welcome to resilient architectures at scale cop 302 real life use cases from amazon.com my name is eigor I'm a principal technologist with AWS I've been with the company for over four years now and since joining AWS I've been helping uh big Enterprises um navigate resilience challenges and tradeoffs when deploying their mission critical applications onto AWS I'm super thrilled to be joined today by my colleague Rovan Rovan would you like to introduce yourself great to co-present with you today on C EO hi everybody my name is Roven I'm a principal technologist I've been working AWS for almost 8 years and I work with customers to help them run their workload in AWS in the most secure and resilient way uh Amazon is one of our customers similar to many of you in the room here and we work with them very closely to ensure that they run their workload and their day-to-day operations also scale during the large event days and able to maintain their workload scalable and available and we compiled a couple of examples across multiple Amazon products for you so you know what those teams have done from a resiliency perspective to be able to maintain theability that they have let's get started awesome indeed we're often asked a lot how does Amazon do X and um we're hoping to inspire you with some examples let's start with some terminology and level level setting what is resilience look in simple terms resilience is the ability of your application or workload to absorb recover uh all the various things that life can throw at it abnormal load crashes poison pills coming from users and so on most importantly the app has to remain functional which means provide business value to your customers the second part of the title is at scale so at scale is all about building for X as I like to call it when you design your application you usually build for a certain expected amount of load but ideally as your business grows and or or there is um unprecedented Spike because of something that's happening out in the market you want to be in a position such that your application remains available go above that X and the best way to demonstrate It Is by looking at some of the numbers from Amazon's Prime day these quite staggering numbers were made possible by the AWS services that underpin amazon.com all those millions of requests per second on sqs all those billions of requests queries on Aurora that's all that made it reality and you're probably asking right here in the audience hey look we're not amazon.com why would we bother we will never be at the scale of amazon.com well guess what learnings that you'll take from today's session are applicable to your businesses no matter the size no matter the scale the next point I suppose I need to make here is that it's the AWS services that will help you achieve resilience at scale so that when you design for X your application will continue working when this x is exceeded far far beyond now we started humbly we started with only two servers back in 1994 today what you have on the screen right now in front of you is a is a screenshot from an internal system called axon each of the dots represents a microservice and uh there's tens of thousands of them so the question is why the seaming complexity how in what way does this help amazon.com remain resilient at scale let's take a real example seeing is believing After all you've probably seen this page we call it a detail page you go to this page to shop to explore if we were to look at this page which by the way you can all do if you fire up your browser Dev tools to some extent you'll see that the the the there's there's lots of calls that are happening in order for this page to be rendered so these calls go to a um what we call widgets it's essentially it's it's it's instances of these microservices that each serves a very distinct purpose one brings the title onto the page another one brings the star rating another one brings the image so now to the resilience topic if one of them or some of them are having a bad day as long as it's probably not the title or or the price he can still transact the you can still check out you can still make a purchase albe it with some degraded user experience so this is called graceful degradation it's pretty fundamental to staying resilient at a scale of [Music] amazon.com speaking of scale each of these microservices or widgets is scaled independently so if the team that owns these services this service for example um rolls out a new change that changes its performance profile or suddenly there's a spike of some nature or a bad deployment it can scale independently of others therefore absorb the load therefore remain functional so with this simple example of microservices based architecture that underpins amazon.com I want to move on to the next topic which is a little bit more complex and a little bit more interesting I'd like a show of hands I know it's really tricky in this silent disco setup but who's here in the audience heard or dealt with uh cell-based architectures all right great I can see some hands out there so let's probably introduce the concept of cell-based architectures and then I talk through how Prime video and Amazon music use this architectural pattern to achieve resilience at scale you're probably all familiar with traditional scaleout approach you've got users you've got a pull off or a fleet of workers and you scale it in and out now if something were to happen and for example one of your users it's always the cat it's always the cat pushes a poison pill or a an event that crashes one of your one of your workers if they continue doing so chances are they will essentially take down the whole Fleet as long as as long as and it often is the case he cannot recover the fleet quickly enough enough so what happens next all of your customer base is affected so the last rate is for this event is all of your customer base how do you fix it well one way is to subdivide your workers into groups let's take this example if we subdivide them into groups of two into Pairs and if the same event were to reoccur suddenly it's just the users that are pinned or routed to this cell and these indeed are called cells that are affected the remainder of your customers will continue operating therefore we've just reduced our blast radius now there's one thing I need to mention that is crucially important I mentioned pinning or routing there is something that needs to route your request to those cells and it's a cell router and this thing and I'll give a couple of examples subsequently how you can implement it has to be built to the atmost level of resilience because if you think about it if it's down you can't route anything the second thing which crucially important is that cells have to share nothing nothing when I say nothing I mean nothing if there is state it cannot be shared because if you think about it if your cells have dependencies between each other that means we can face a cascading failure which means if I were to put it in in other terms if a cell serves a function then one cell can serve should be able to service this whole business function and so should the second and the third now with this knowledge with this knowledge in mind let's just talk about the two businesses of Amazon uh Prime video and music and how they implemented this architectural Paradigm to increase their resilience at scale and also solve for another business challenge which is actually quite distinct in those two cases those two systems that we'll be talking about deal with somewhat similar um uh uh concerns they ingest realtime Telemetry from um users or devices when you're watching video real-time Telemetry feed comes back that helps us make decisions around CDN hydration Etc to ultimately improve your usability that they needed to be able to burn into other R regions when the regions that were running in were running a little bit hot on specific E2 instance types it happens right and they wanted to do this without dropping a single message now Amazon music's business challenge was a little bit different they also deal with real time ingestion uh coming from all the various devices using uh the music app iOS Android um Alexis and whatnot they needed to solve for Noisy Neighbor situation they needed to ensure that they different device types and different message types types based in criticality do not impact the ingestion process so let's in the form of decisions that they had to make let's talk you through the solution that they adopted um both are examples of cell-based architectures in action so the first question that um Prime Engineers asked is how to slice up an AAL based region in into cells the answer for them was simple we will create cells that span all available availability zones in a given region so we'll slice it horizontally why do they have to do that well the answer is because they used Regional services in their architecture if you are using zonal services such as ec2 for example um and by the way the regional service that they relied on was Lambda uh if you are using a exclusively zonal Services then you can potentially look at creating slicing up vertically for example creating cell aligned to azs or subdividing A's into cells now I've just used two important terms Regional zonal there also Global I really encourage you to take your phones out and scan that QR code there is a white paper that is fundamental for every resilience specialist and practitioner to read and internalize it's the fault isolation boundaries white paper which talks about these Concepts and then some so the second decision that Prime video had to make is about the routing logic which Services which approach to use to Route requests and for them they chose to use Route 53 they choose to use chose to use two layers of Route 53 the first one with a round robin logic that routes to a Zone aligned um route three Zone which was using GE proximity routing which if you think serves the purpose and solves the business challenge that they exactly had if one of the cells bursts into if they're bursting into another region and create a new cell there you want to be routing to the closest region now you don't want to Route stuff to a cell that is either bootstrapping hasn't finished bootstrapping yet or is unhealthy for whatever reason so they had to devis a health check which was a combination of uh counts of non2 200 events on the load balancer which they W with some deep deeper health checks that they that they invented for a specific uh Health end point and theyve surfaced they wrapped it into a cloud watch alarm and uh created a r 53 health check based on that alarm it's quite simple quite elegant and it works and speaking of works that's the outcome uh which was over 5 NES of availability they were able to achieve as measured over over a 4-we period um and the calculation is quite simple how many events we've had minus how many events we've dropped divided by total number of events now let's move on to Amazon music so Amazon music uh had a more complex requirement for routing remember I mentioned earlier they were dealing with various device kinds and various event types so they had no choice but to cut some code and they did and they put it in ECS fargate and to make that code as resilient as possible they even codified the logic in the code itself so as to not have any additional extra dependencies uh sitting next to fargate now the routing logic was twofold the first router the first route was going to the supercell based on device type iOS Android Etc and then the second was based on event type some events are more critical than the others and that ultimately solved for that Noisy Neighbor situation now the third challenge that they had to solve for is how to manage that scale in and scale out cuz sometimes there's Peaks sometimes there troughs there's two approaches broadly speaking you allow cells to breathe in and breathe out or you stamp out new cells uh identical in size they chose to go down the latter path so you stamp out new cells they're all identical and you take them away when you don't need them so this is overall architecture as it looks like um and what's interesting to summarize my part of the talk is the two businesses of Amazon both used s-based architectures for their Sol in their in their architecture both were able to achieve increased availability and resilience at scale and also solve for a deeper business problem through cell-based architectures on that note I'm going to hand over to ran to talk about some amazing stuff that ring did to you aome thanks Eagle so cool that's our Legend isn't he all right so now we'll learn about Amazon website also Prime video and music it's time to talk about ring and ring has a very interesting use case so they built an event driven scalable architecture that able to serve over 130,000 request per second while maintain an avability of over six nights so before we get into the resiliency pattern that they implemented we will just get a set the scene first what is ring and what is the key element of their architecture so if you haven't came across it before ring is uh a set of cameras and doorbells you install it in your uh house and if someone ringing your bell or at your door you will get a notification on your phone so you will be able to check what has happened on your driveway for example and ring sorry I didn't realize I actually so ring uh architecture one of the key things I want to point out is the transcoder service so the transcoder service over here it's it's designed to transcode these uh videos so the image that you've seen in my previous slide with the uh red car it is actually a screenshot of a video so there is a camera there that takes this row footage video put it in an A3 bucket and once it lands on est3 bucket it kicks a notification that put request an sqsq and we have a fleet of ec2 over there that runs the transcoder service pulls the que gets a video transcoded and then put it into the last three S3 Bucket over there and this is what I will see on my screen so ring as any other product or service need to scale up and if you look into uh other Amazon product like for example Amazon website they all have large event days primarily around Prime Day or Christmas Cale and whatso on ring is a bit different right because it does video transcoding so what do you think is the largest event for uh ring got it it's a Halloween funny enough so as kids go around and uh try to get uh Trick and Treat and knock at the doors you will be getting notification and alerts from your phone as they trigger theem motion detection so ring team needs to be able to scale during these events like Halloween or Christmas if you getting packages and whatso on while transcoding all of these videos and send it to their C their consumers as soon as possible so let's see how they scale so here is the architecture and ring team uses cloudwatch to monitor the que the SNS q and they monitor a metric called empty receivers and the thing about empty receivers if we have a lot of empty receivers that probably give us the indication we over provisioned and maybe it's time to scale down however if there is a lot of work and there's not enough empty receivers around that may be because we backing up the queue and we had to scale up so ring take this metric and other Priory metrics and put it into a step function and a step function is a state machine so based on all of these metrics it decide when to scale up or scale down another thing ring team was very uh interested in is latency so as these Services talk to each other and the component they wanted to make sure they can reduce latency to the minimum so they built an event driven architecture and what I mean by event-driven architecture is if there is a camera recorded an event let's say um a motion detection event a doorbell or someone moving in front of your house so it send this event to a notification service and then the notification service will go and push a notification to my phone so it tells me someone ringing your door so as a consumer I want to know this as soon as it happens almost in real time so latency is very important to reduce over here all right so from an architecture perspective ring team build uh streaming event bus or what I will be referring to as sip it's a multi-tier architecture don't worry it's uh a little bit busy on the diagram I understand that we will break it down to different tiers so let's start with tier number one and this is the API layer and at the API layer what's happening there is authentication and the second primar thing this uh layer does is also it does some logic around routing so it checks the events and decide which cell to send this topic to the second tier which we have the processing layer and the processing layer over there you could see there is Apache Kafka in each of the cells and Apache capka has a harly stripit and a harly streaming event capabilities the third tier is the consumer proxy so team wanted to scale their architecture on board as many consumers as possible without really giving them access to pull CFA directly so what they've done is they build this consumer proxy layer so it pulls Kafka on the consumer behalf and then serves them the event in a direct API call or into an ssq so back to the full diagram uh SE is a multi cell multi-tier architecture and you can see that the uh sep team so the ring team over here has divided all of the cells and te into different accounts an account is these pink boxes on the screen and the reason they done this division is for manageability and for blast radius now now you got your head around the diagram and how they build it let's talk about what is the resiliency principle they apply to maintain this highly avability of six nines while serving over 130 requests per second so the first one we have the cell-based architecture so ring team build sip as a cell-based architecture and the thing about cell-based architecture is blast radius there is plenties and thousand of events coming all the time and it get routed to these cells that you have cell one and cell two and if there is a problem and one of the cells is down that mean half of the topics will be impacted the other half will continue to be served by the remaining healthy cell correct what tring team did as well on the top of the cellbase architecture is if a cell went down they actually go and scale the second cell up so it's able to accommodate all of the events and serve these requests so they gain the benefit of the scalable cell-based architecture and they able to scale the remaining cell if there's a problem in one of the cells to serve all of the requests the second principle is full back back cell and over here you can see cell one and cell two are bows down and the question I can see in your face is like how this is possible e have been talking about the cell-based architecture it's fault isolation boundary if something happen in one cell it shouldn't impact the other cell so how come we have two cells down it's actually happens and that's what we call correlated failure so what is correlated failure if there is a problem or an issue with a service that that across multiple cells what an example of that let's say Kafka so we have Kafka in two cells over there and maybe AWS team went and and done an update or released a new stack for Kafka and that caused an issue for the service in one of the regions so what you would see most of the customers and team do there is they're just going fall over to the other region so if there's a problem in Us East they go to us West or's a problem in in Sydney region they go to Melbourne region and this is absolutely right but ring team didn't really follow this rout what they've done there is they built this third cell called a fallback cell and the differ between this cell and the other two is the stack the stack doesn't have Kafka in cell number three it has SNS and sqs so ring team use SNS and sqs to do the streaming processing instead of Kafka and yes they know it's not as efficient as Kafka but by doing so they able to avoid colorated failure and maintain AB ability the thirt is circuit breaker so circuit breaker is if the circuit is closed everything is good think about it as a light switch if the light is on the circuit is closed the traffic flows everything is good in this context if the circuit is is closed the uh sill is healthy and it's able to serve requests but let's say you started to receive errors above of the threshold in one of your cells so it started to become unhealthy so we opened the circuit and all of the traffic and request get reacted to the remaining healthy cell right and if occasionally you started to get also errors in the second uh cell and it becomes unhealthy we go and open the circuit and the cell number two is also now unhealthy so what's happened there is the topics and events all get reacted to the fall back cell and that give you an additional layer of resiliency and once the circuit is open it goes into a half State sorry once the ciruit is open and the cell is unhealthy it goes to this half open uh status and in a half open status what happens here is we TR try to send occasional request to the cell and once it starts to serve these request it reassesses status and then closes the circuit and it becomes a healthy cell to serve requests so ring stack is deployed across eight region globally and aggregated number of request over these eight regions is over 300,000 requests per second and if we break down these eight regions over three day day period you will be able to see here some of these regions have not just achieved the 69s required it was 100% avability And that's because ring team have implemented all of the resiliency P practices is from cell based architecture circuit breaker and fall back and I know we've referen couple of times Us East and US West and these are the two largest regions ring is deployed at out of these eight regions globally here in Australia we are placed with two regions Sydney region and mbour region so you can get these principle and Implement them across these two regions and what I want point out as well is Oakland region is on away so from the examples that we shared from you across Amazon products we hope we manage to inspire you to go and build scalable resilient architecture regardless what size of your company all of these principle apply and we hope that you take them and go Implement them today another thing I want to point out is resiliency services and framework you probably will come across throughout your resiliency Journey on AWS things like Ark and resilienc AWS resiliency Hub fault Injection Service technology make Innovation possible but is people the one who actually go and build better tomorrow if you uh go and sign up for AWS skill Builder you will be able to redeem a 7day free trial igra and I hope this session been informative for you would like to thank you for your time and we will stti outside there to take any questions thank you [Music]