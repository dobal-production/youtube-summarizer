- 오늘 함께 해주셔서 감사합니다.다음에 대해 알게 되실 거예요
대규모로 복원력이 뛰어난 아키텍처, 그리고 앞으로 나아갈 방식
실제 사례를 통해 이에 대해 알아보도록 하겠습니다.
오늘은 amazon.com에서 보내셨습니다.제 이름은 세스예요.저는
여기 오게 돼서 정말 행복해요.여기서 만나서 정말 반가워요
이 멋진 극장에서, 오늘 제 공동 발표자들과 함께하게 되어 정말 기쁩니다.아비나쉬랑 튤립그들이 소개할 거예요
조금 후에 설명해 드리지만 저는 개발자입니다.
아키텍트, 개발자 아키텍트, 개발자 옹호자.좋아요.제 직함은 알아요.저는 솔루션스 아키텍트였습니다.
그래서 그냥 정리해봤어요.그리고 제가 솔루션 아키텍트였을 때 저는 신뢰성 책임자였습니다.
AWS의 경우 설계가 잘 되어 있습니다.그래서 저는 많은 사람들, 그리고 많은 고객들과 함께 일해왔습니다.
레질리언스 문제가 있었는데, 저는 12번 정도 겪어봤어요.
아마존에서 보낸 총 4년이고, AWS에서 근무한 지난 4년이지만, 이전 8년은 amazon.com에서 근무했습니다.이것이 바로 그 예시입니다.
오늘 얘기 들으실 거예요.복원력이 뛰어나고 확장 가능한 아키텍처의 예시입니다.
닷컴 측에서요.첫 시작을 위해 말씀드리자면, 제 말은 제목에 관한 것입니다.
복원력이 뛰어난 아키텍처.레질리언스란 무엇일까요?레질리언스는 다음과 같은 경우
애플리케이션은 다음을 견디거나 완화 또는 복구할 수 있습니다.
장애 및 부하 스파이크의 종류에 따라
프로덕션에서 볼 수 있을 거예요여러분 중에 달리고 있는 사람이 있다면
데이터 센터, 클라우드 또는 모든 종류의 애플리케이션
프로덕션 환경이 혼란스럽다는 거 아시죠?이상한 사용자 패턴, 네트워크 문제 등 다양한 일들이 항상 일어납니다.따라서 애플리케이션을 계속 사용할 수 있도록 복원력을 구축해야 합니다.이를 위해 약 한 달 전, 아마도 두 달 전에 이 제품을 출시했습니다.이것이 바로 라이프 사이클입니다.
회복탄력성을 위한 프레임워크. 왜냐하면 회복탄력성은
지속적인 프로세스죠.그냥 끝나버리는 게 아니에요. 그냥 안 하면 끝이죠.그리고 우리는 이 프로세스를 마치 소프트웨어 개발 라이프 사이클과 비슷하게 만들었죠.그래서 여러분도 이 사실을 알고 계실 수 있습니다. 그리고 여러분의 소프트웨어 개발 과정에서도
물론, 라이프 사이클, 레질리언스가 그 일부분인지 확인하세요. 그러면 실제로 배울 수 있습니다.
이에 대해서는 오늘 오후에 더 자세히 설명하겠습니다.브레이크아웃이 있을 거예요
그것에 관한 세션이요하지만 이걸 놓치셨다면 거기에 링크가 있습니다.
관련 내용을 읽어보실 수 있습니다.오늘 우리의 목적을 위해,
앞서 말씀드린 것처럼 Amazon의 복원력 있는 아키텍처의 실제 사례를 여러 개 소개해 드리겠습니다.세 가지로 나눠서 설명하겠습니다.
이 프레임워크의 카테고리.첫 번째는 설계, 설계 및 구현에 관한 것입니다. 그렇죠?최고를 위한 디자인
레질리언스 프랙티스.세포 같은 것을 이용한 결함 격리 같은 것을 보여주는 예시를 보여드릴게요.
오토 스케일링을 보여드리죠. 분리된 아키텍처를 보여드리죠.그리고 다음은
라이프사이클은 테스트와 평가에 관한 것입니다.혼란을 야기한 팀의 예를 보여드리겠습니다.
엔지니어링 및 부하 테스트.그리고 마지막으로, 알다시피, 가끔은
사람들은 이걸 잊어버리죠.여러분이 직접 디자인할 수 있는 것은
애플리케이션을 만들고 복원력을 높이려면 탄력적으로 운영해야 합니다.이제 팀이 어떻게 메트릭과 옵저버빌리티를 전반에서 사용하고 있는지 예시를 보여드리겠습니다.
계정, 서비스 전반에서 레질리언스 보장
자체 워크로드, 애플리케이션.제목의 두 번째 부분은 복원력이 뛰어나다는 것입니다.
대규모 아키텍처.스케일이 무엇인지 정의하는 게 좋을 것 같아요. 우리 모두 알고 있을 것 같지만
기본적으로 언제, 언제, 언제, 언제 도착하는지에 관한 것입니다.
추가 부하, 추가 범위, 그리고 사용자, 시스템,
애플리케이션은 이를 수용할 수 있으며 부하나 범위가 아무리 크더라도 계속 사용할 수 있습니다.예를 들어보자면, amazon.com으로 돌아가면, 아마존은 1995년에 설립되었습니다.
두 대의 서버에서 실행됩니다.그래서 처음에는 소규모로 시작했는데
빨리 커지라는 모토가 있었어요.저 티셔츠를 볼 수 있었죠.
1997년에 피크닉 갔을 때 얼른 몸집을 부리고 핫도그 한 개 더 먹어하지만 모토는 빨리 커지세요.그래서 두 명부터 시작했죠.
서버 맞죠?뭐 어찌 됐든, 처음에는 두 개부터 시작했죠.
이 웹사이트를 실행하는 서버들.한 서버에서는 실행 파일을 실행하고 다른 서버에서는 데이터베이스를 실행하고 있었습니다.하지만 순식간에 규모가 커졌죠.프라임 데이를 보면
올해부터 판매된 품목의 수, 매출액이 상당히 인상적입니다.하지만 이제 우리는 기술 분야 종사자이기 때문에 슬라이드는 다음과 같습니다.
저는 정말 보여주고 싶어요.이것은 AWS 서비스와 리소스를 보여주는 것입니다. 그 중 일부는
Amazon 팀에서 사용하고 있는 것은 그 중 몇 가지에 불과합니다.
탄력성과 확장성을 갖추세요.다음과 같이 DynamoDB를 볼 수 있습니다.
초당 수백만 건의 요청.다음과 같이 Aurora를 볼 수 있습니다.
수십억 건의 트랜잭션과 테라바이트의 데이터.여러분께 모든 통계를 읽어드리지는 않겠지만, 여기서 요점은
레질리언스, 해상도, 확장성, 대규모 레질리언스를 원한다면 클라우드를 사용하고 AWS 서비스를 사용하는 것이 그 목표를 달성하는 데 어떻게 도움이 될 수 있는지 보여드리겠습니다.그리고 제가 말씀드리지 못한 요점은
앞서 Amazon에 대해 말씀드렸듯이 소규모로 시작하고 확장하는 것은 애플리케이션 규모나 기업, 모든 것, 거의 모든 것에 상관 없습니다.
오늘 보여드릴 내용은 여러분에게 해당됩니다.저걸 넣고 싶으시잖아요
레질리언스 모범 사례, 그리고 필요할 때 확장할 수 있도록 적용해야 합니다.간단히 말씀드리자면, 이것이
아키텍처는 아까도 두 대의 서버라고 말했지만, 지금은 이게 서비스입니다.여기 있는 각 점은
서비스, 마이크로서비스 또는 솔루션의 일부
Amazon의 지향 아키텍처.그리고 줄이 몇 개 있습니다.
그 사이는 서비스 간의 종속성을 보여줍니다.오늘날 Amazon에는 수만 개의 서비스가 운영되고 있습니다.아주 작은 회사죠. 아마존
SOA 및 마이크로서비스 유형 아키텍처를 사용하는 것을 좋아합니다.보여드릴게요
지금 그 예를 들어보죠.사실 첫 번째 예가 있습니다.이건 그냥 아마존 웹페이지입니다.이를 상세 페이지라고 합니다.물건을 사는 페이지입니다.이 경우에는
킨들 파이어 태블릿을 사세요.태블릿 맞죠?이 페이지를 보면 필요한 모든 것이 있습니다.
리뷰, 사진, 제목, 가격 등이 있잖아요, 그렇죠?하지만 이건 사실
프레임워크는 지원되고 소유되는 내부 프레임워크입니다.
아마존 내부 팀이 담당했습니다.그리고 프레임워크는 위젯이라는 백엔드 서비스를 수백 번 호출합니다.그리고 이러한 위젯은
기본적으로 마이크로서비스죠.그리고 각 위젯에는 약간씩 있습니다.
비즈니스 로직의 일부, 그리고 무엇이 필요한지
페이지에 표시되며 이러한 호출을 병렬로 수행하고 매우 빠르게 렌더링합니다.그래서 이 페이지를 가져와서 아마존의 내부 도구를 통해 실행해 보면 다음과 같습니다.이제 이미지를 제공하는 마이크로서비스가 있다는 것을 알 수 있습니다.마이크로서비스가 있습니다.
타이틀 서비스에는 평균적인 고객 리뷰를 제공하는 마이크로서비스가 있습니다.이 모든 것이 병렬로 호출되고 렌더링되고 있습니다.그리고 이것은 두 가지 모두를 가능하게 합니다.
복원력과 확장성: 왜냐하면 이러한 서비스 중 하나가
장애 또는 장애가 발생하여 제대로 작동하지 않는 경우
제목이나 이미지 또는 가격이 아닌 한 고객은 여전히 가지고 있습니다.
유용한 경험.그래도 받을 수 있겠죠
필요한 것 대부분을 구매하세요.그래서 우리는 이렇게 부릅니다.
우아한 디그라데이션.우리는 내려가지 않고 그대로 남습니다.
고객이 이용할 수 있지만, 알다시피, 그렇지 않을 수도 있습니다.
몇 가지 기능이 있습니다.이게 바로 회복력 부분이죠.확장성 부분은 이러한 각 백엔드 마이크로서비스를 독립적으로 배포할 수 있다는 것입니다.따라서 다음과 같은 이점을 얻을 수 있습니다.
팀은 필요할 때 플레이할 수 있고
필요할 때 혁신하고 필요할 때 기능을 도입하세요.마지막으로, 세 번째 요점은 말씀드렸듯이 이 프레임워크는 가 소유한 프레임워크라는 것입니다.
프레임워크를 관리하는 중앙 집중식 팀이죠.비즈니스 로직은 위젯을 소유한 팀이 소유하며,
각 위젯을 소유하고 있는 팀이 서로 다르죠?즉, 위젯을 소유한 팀이 할 수 있다는 뜻이죠.
비즈니스 로직에 집중하세요. 어떤 것에만 초점을 맞출 필요는 없습니다.
프레임워크가 바로 그 역할을 합니다.이것 저것, 기본적으로
그들의 부담을 덜어줘서 더 나은 혁신을 이룰 수 있도록 말이죠.자, 그럼 이제 튤립을 사용한 다음 예시로 넘어가겠습니다. - 감사합니다.그럼 잠깐 설명해 보죠.
손을 들어서 얼마나 많은 사람들이 알고 있는지 확인해 보세요.
셀 기반 아키텍처.저기 손이 몇 개 보이네요.그래서 이번 세션의 이 부분에서는
셀 기반 아키텍처의 기본 사항과 Prime의 몇 가지 사용 사례에 대해 설명하겠습니다.
비디오와 Amazon Music, 그리고 이를 사용하여 가용성 및 장애 격리를 개선할 수 있었던 방법
셀 기반 아키텍처.우선 저는 튤립 굽타입니다. 저는 AWS의 선임 솔루션 아키텍트입니다.저는 AWS에서 다음과 같은 일을 해왔습니다.
지난 2년 반 동안 프라임 비디오, 아마존 게임 스튜디오, 아마존과 같은 아마존 고객을 도왔습니다.
음악, 트위치, 오디오블.아마 익숙하실 겁니다.
기존 스케일링과 기존 스케일링의 경우
일반적으로 작업자 노드 (이 경우에는 작업자 8개) 가 있습니다.
노드는 모든 고객의 요구 사항을 충족하며 현재 8명의 고객을 보유하고 있습니다.하지만 그 중 하나를 예로 들어 보겠습니다.
고객이 의도적으로 또는 실수로 전송함
잘못된 요청의 경우 이제 작업자 노드 중 하나에
장애가 발생해서 그는 다시 재시도하고, 천천히 당신의 모든 것을 재시도합니다.
작업자 노드가 손상되어 모든 고객이 영향을 받습니다.따라서 폭발 반경은 모든 고객입니다.그리고 셀 기반 스케일링에서는
이전 슬라이드에서 보셨던 독약 사태를 어떻게 피할 수 있는지 살펴보겠습니다.그래서 같은 고객이
잘못된 요청을 보냈지만, 이 경우 저희가 한 일은
그것을 셀로 나누면 각 셀은
두 개의 작업자 노드로 구성되어 있습니다.그럼 이제 고객은
잘못된 요청을 보내면 작업자 노드가 두 개뿐입니다.
영향을 받고 있습니다. 셀이 하나뿐입니다.그리고 어떤 고객이든
해당 셀에서 서비스를 받고 있는 것도 영향을 받습니다.보시다시피 단 두 개뿐입니다.
이 시나리오에서는 고객 8명 중 8명이 영향을 받습니다.따라서 폭발 반경은
CO가 상당히 감소했고 4배 개선되었습니다.
전체 시스템에 미치는 영향에서 말이죠.이것이 바로 세포를 기반으로 하는 것입니다.
아키텍처는 어떻게 생겼죠.그러니까 세포와 세포가 있잖아요.
서비스가 분할된 디자인 패턴입니다.
셀이라고 하는 여러 배포 스택으로 구성됩니다.이들은 독립적인 인스턴스이며 고객의 전체 워크로드에 독립적으로 서비스를 제공할 수 있습니다.한 가지 중요한 사실을 알아두어야 합니다.
세포는 아무 것도 공유하지 않는다는 거죠.세포가 세 개 정도 있다면
이 경우에는 셀 0, 셀 1, 셀이라고 해봅시다.
둘째, 셀 0과 셀 1이 매우 중요하죠.
어떤 데이터도 공유하거나 하지 마세요.그리고 그 이유는
만약 세포 하나에 필요한 데이터가 있다면, 그리고 세포 0이 손상되면 세포 1도 손상될 수 있다는 거죠.또 다른 열쇠는 중요합니다.
문제는 셀 라우터입니다.이제 셀 라우터 라우팅
일부 구성 로직을 기반으로 한 요청입니다.그래서 요청이 들어옵니다. 셀
라우터는 고객 ID와 같은 파티션 키를 기반으로 라우팅합니다.라운드 로빈을 할 수도 있겠죠.
셀 0에서 셀 1, 셀 2까지, 다른 셀로 셀을 이동하라는 라우터 요청일 뿐이죠.한 가지 중요한 점은
셀 라우터는 하, 가능한 한 가장 얇아야 합니다. 그 이유는 마치 망가지면
그러면 그러면 요청을 라우팅할 수 없게 됩니다.
다른 셀로 이동하면 고객도 영향을 받을 수 있습니다.그럼 살펴보도록 하겠습니다.
Prime의 몇 가지 사용 사례를 자세히 살펴보겠습니다.
비디오와 아마존 뮤직, 그리고 저는 그들이 셀을 채택하도록 도왔습니다.
올해는 아키텍처를 기반으로 했죠.그래서 셀을 채택한 팀은
프라임 비디오의 기반 아키텍처는 프라임 비디오 분석 팀으로 내부 고객이 외부 고객의 외부 경험을 심층적으로 분석할 수 있도록 합니다.
고객이 프라임 비디오를 시청하고 있기 때문에 더 나은 서비스를 제공합니다.
동영상 전송 품질.셀 기반 아키텍처를 채택하려는 주요 이유 중 하나는 글로벌 설정을 단순화하기 위해서였습니다.그들은 워크로드를 없애고 싶었고, 빠르게 옮기길 원했습니다.
실적이 저조한 지역에서 건강한 지역으로또한 특정 지역이 그렇지 않은 경우에도 마찬가지입니다.
수용 인원이 충분해서 옮기고 싶었던 거죠.
다른 지역으로그럼 그들이 가지고 있다고 가정해 봅시다.
미국 동부 지역의 워크로드인데 특정 인스턴스 유형을 위한 용량이 충분하지 않았습니다.
그들은 미국 동부 2곳으로 빠르게 이전할 수 있기를 원했습니다.Amazon Music의 경우 팀 지표 수집 서비스를 위한 것이었는데, 이 서비스가 하는 일을 알아서 수집하죠.
다양한 클라이언트의 지표로 개선에 도움이 됩니다.
음악 전달 품질.그리고 이들이 셀 기반 아키텍처를 채택하고자 했던 주요 이유는 무엇일까요?
장애 격리였죠.가장 심각한 사건부터 가장 심각하지 않은 사건까지 다양한 종류의 사건이 발생했습니다.
다양한 기기 유형에서 오더라도 원했습니다.
결함 격리, 그러니까 소음이 심할 때
운영 이벤트와 같은 트래픽은 그렇지 않았습니다.
가장 중요한 고객 영향 이벤트가 영향을 받길 바래요.그럼 핵심 내용을 살펴보겠습니다.
프라임 비디오가 내린 결정.주요 결정 중 하나는
그들은 세포를 어떻게 설계하고 싶었는지였습니다.그래서 예전에는 이걸 가지고 있었죠.
모든 고객의 요구 사항을 충족하는 단일 워크로드를 모든 고객에게 제공할 수 있었습니다.
영역을 한 지역에 두고 여러 셀로 나누었습니다. 그리고 지역당 세 개의 셀을 보유했습니다.그리고 그 이유는
한 지역의 AZ에 걸쳐서 말이죠. 지역이 있었기 때문이죠.
Lambda와 같은 서비스, 이것이 바로 Lambda가 지역 셀이었던 이유입니다.그리고 핵심 중 하나는
셀을 도입할 때마다 알 수 있는 결정
기반 아키텍처는 사용 중인 서비스를 살펴보는 것입니다.따라서 AZ 기반의 EC2 또는 이와 같은 서비스를 사용하는 경우 다음과 같은 셀이 있을 수 있습니다.
AZ 기반일 수 있습니다.그리고 그들이 내린 두 번째 결정은 셀룰러 트래픽 정책에 관한 것이었습니다.그래서 요청이 들어왔을 때
디바이스에서 53번 Route 53까지 트래픽 정책이 있었습니다.
53번 국도에 건설되었는데, 우회해서 라우팅을 할 수 있습니다.
라운드 로빈을 기반으로 한 트래픽.그러면 요청이 전달될 것입니다.
셀 1, 셀 2, 셀 3 등등.그럼 요청을 해봅시다.
두 번째 셀에 들어오는데 53번 라우트 DNS가 있었습니다.
지리적 근접성 라우팅과 지리적 근접성 라우팅을 수행하는 정책도 나와 있습니다.
요청의 출처와 가장 가까운 지역으로 요청을 라우팅한다는 의미입니다.요청이 왔다고 가정해 봅시다.
뉴욕에서 가장 가까운 지역으로 라우팅해 줄 거예요.
이 경우 미국 동부 지역이고 어떤 지역으로 들어가면 애플리케이션 로드 밸런서에 도달한 다음
그 뒤에 있는 해당 셀.그리고 그들이 내린 세 번째 결정은
계산된 건강 검진이었죠.이제 한 가지 예를 들자면
참고로 성능이 떨어지거나 상태가 좋지 않은 셀로 요청을 라우팅하고 싶지는 않을 것입니다.그래서 그들이 확인한 방식은
셀이 정상이면 Route 53 상태 검사를 설정할까요?부트스트랩 API를 핑합니다.
개별 셀용.그리고 400이나 500의 오류가 난다면 알아차릴 것입니다.
그 셀이 정상적이지 않아서 요청을 라우팅하지 않을 거라는 거죠.두 번째로 그들이 한 가지는
클라우드워치 알람이 있었죠.그들은 ELB 500 오류를 조사했고 그 이상이 있는지 살펴보았습니다.
1분 동안 100개의 오류가 발생하면 해당 지역의 로드 밸런서가 비정상임을 알게 되고 요청을 라우팅하지도 않을 것입니다.그 결과,
99.9996% 의 가용성 결과를 볼 수 있었습니다.
4주 동안 말이죠.그리고 이것은 성공적으로 처리된 이벤트의 비율입니다.그리고 그들이 계산한 방식도
가용성은 총 요청에서 오류를 뺀 값을 다음과 같이 나눈 값입니다.
총 요청을 100개로 나눕니다.그리고 모든 실패는 서버 규모 실패로 분류되었습니다.
ELB 500 오류처럼요.이 모든 것이 함께 제공되었죠.
가용성이 향상되고 페일오버 기능이 향상되어
셀룰라이제이션과 함께 말이죠.그러면 아마존 뮤직이 떠오릅니다.이전 슬라이드 중 하나에서 셀 라우터에 대해 말씀드렸는데, 이것이 바로 그들이 한 일입니다.여기선 가능한 가장 얇은 셀 라우팅 로직이 있었죠.따라서 요청이 들어와서 애플리케이션 로드 밸런서로 전달되면 셀 라우팅 로직이 만들어집니다.
AWS Fargate에서 관리하는 ECS 클러스터에서 말이죠.응답을 승인했습니다.
세포 라우팅도 해줬죠.셀 라우팅 구성은 정적 구성으로 견인됩니다.
구성을 코드로 하고, 앱 구성으로 옮길 계획입니다.
좀 더 동적이고 이들, 그리고 요청이 서로 다른 슈퍼셀로 라우팅되는데, 슈퍼셀은 셀 모음입니다.그리고 이들이 내린 두 번째 주요 결정은 셀프 매핑 전략이었습니다.그래서 요청이 들어오면
디바이스 유형을 기반으로 파티션의 위치를 기준으로 라우팅되었습니다.
키는 디바이스 유형입니다.따라서 iOS인 경우
슈퍼셀 1로 가는 길.안드로이드부터 슈퍼셀 2까지 말이죠.그리고 이 슈퍼셀은 성장할 수 있습니다.
그리고 TPS를 기반으로 축소되죠.예를 들어 iOS에 7만 건 정도의 요청이 많이 들어온다고 가정해 봅시다.
안드로이드는 80,000개 정도로 그 이상의 숫자를 보유하고 있지만 슈퍼셀 2는
SU가 클수록 슈퍼셀 1보다 셀 컬렉션이 더 많을 것입니다.그들이 한 또 다른 일은
세포로 가는 경로를 기반으로 세포를 라우팅하는 것이었죠.
이벤트 티어를 기반으로 합니다.그랬다고 가정해 봅시다.
아시다시피, 고객 이벤트와 운영 이벤트는
고객 이벤트는 가장 중요한 이벤트로 분류되었습니다.
이 경우에는 계층 1 이벤트와 계층 2, 계층 3과 같은 이 이벤트의 중요도가 가장 낮았는데, 이는 운영 이벤트입니다.이런 식으로 보면
예를 들어 티어 3에 시끄러운 트래픽이 많다고 가정해 봅시다.
운영을 위한 이벤트는 영향을 미치지 않을 것입니다.
티어 원 이벤트.그리고 이 모든 티어 원 이벤트는
계층 2, 계층 3은 다른 셀 집합으로 라우팅됩니다.
그 슈퍼셀 안에세 번째 핵심 결정
그들이 택한 것은 세포 크기 관리 모델이었습니다.이제 셀을 디자인하는 두 가지 방법이 있습니다.원자일 수도 있고 비원자일 수도 있습니다.비원자 셀 크기에서
관리, 세포만 있으면 개별 세포도 성장할 수 있습니다.
요청이 많이 들어오는데, 그게 바로 당신이 좋아하는 방식이죠.
규모를 늘리세요.하지만 내부적으로,
원자 셀은 같은 크기의 셀을 추가해서 10% 확대할 수 있습니다.그러니까 모든 세포의 크기가 같다는 거죠.그리고 아마존 뮤직은 이렇게 선택했습니다.
원자 세포의 길을 가는 것은 그들이 할 수 있기 때문이죠. 그들은 같은 것을 배포하는 것을 좋아하기 때문이죠.
모든 세포 안에 쌓여 있어요.이것이 개요입니다.
전체적으로 어떻게 보이는지.요청은 제품에서 들어오고 생산자는
애플리케이션 로드 밸런서는 ECS 클러스터와 AWS Fargate가 있는 라우팅 레이어로 이동하여 응답을 승인합니다.
디바이스 유형에 따라 트래픽 라우팅이 이루어지나요?
파티션 키가 개인을 통과할 때
슈퍼셀: 티어 1, 티어 2, 티어 3 이벤트 진행 상황에 따라 매핑됩니다.
슈퍼셀 안에 있는 다른 셀 세트로.그 결과 결과는
복원력이 높았죠.티어를 기반으로 제어할 수 있었고 정전 폭발 반경도 92% 감소했습니다.그 이유는 이제 그들의
티어 1 이벤트는 시끄러운 트래픽이 많더라도 영향을 받지 않았습니다.
운영 이벤트는 채택되기 전에도 그랬습니다.
셀 기반 아키텍처.또한 디바이스 유형별로 이벤트가 분리되므로 가용성이 더 높고 처리 지연도 줄어들었습니다.따라서 잘못된 배포나 문제가 발생한 경우
저희 기기 유형, 안드로이드, 알렉사 등은 영향을 받지 않습니다.요약하면 프라임 비디오와 아마존 뮤직의 경우 둘 다 셀이 스테이트리스 시스템에 있었습니다.그래서 그들은 세포를 가지고 있었죠.
프라임 비디오의 경우 ALB 람다와 SQS 같은 것이 들어 있고 아마존 뮤직의 경우 ALB, 람다, 키네시스가 들어 있습니다.스테이트리스 시스템이란, 아무 것도 저장하지 않았다는 뜻입니다.
셀에 있는 정보, 그리고 라우팅
프라임 비디오의 정책은 라운드 로빈이었고 근접성이었고 아마존 뮤직의 경우
디바이스 유형 및 이벤트 기반.그 결과,
둘 다 비슷한 결과를 보였습니다.증가했습니다.
가용성 및 복원력.그럼 세스한테 넘겨줄게요. - 좋아요, 튤립 고마워요.좋아요, 그래서 알게 됐어요
웹사이트에 대해 말이에요.실례해요, 물 한 잔 마실래요.아마존 음악에 대해 배웠고 프라임 비디오에 대해 배웠으니 이제 링에 대해 알아보겠습니다.그리고 Ring은 대규모로 만들었습니다.
초당 약 130,000개의 요청을 처리하면서 99.99개의 가용성을 달성할 수 있는 확장 가능한 이벤트 기반 아키텍처입니다.그럼 먼저 어떤 내용인지 알아보도록 하겠습니다.
아무래도 반지가 뭔지 모두가 알고 있는지 확인해야 할 것 같아요.저는 링 고객이고 링 팬이에요.그래서 Ring은 초인종, 카메라, 알람 장비 세트입니다.
집에 걸 수 있어요.그리고 알다시피, 무슨 일이 생기면
차도에서 움직임 경보를 받게 되죠.
핸드폰을 보시면 제 진입로가 보이시죠.
오, 저기 제 미니밴이 있고, 저기 제 토끼가 있어요.오, 진짜 제 토끼는 아니에요. 하지만 제 차도를 가로지르는 토끼였는데 그래도 보는 게 재밌었어요.그게 바로 링의 목적입니다.그리고 제가 들어가기 전에
초당 129,000건의 요청이 들어왔는데, 다른 서비스를 소개하고 싶어요.이게 그들의 비디오 인코더 서비스예요.이것은 제가 이전 슬라이드에서 보여드린 곳입니다.
당신은 비디오의 스냅샷이었어요.제 안에 카메라가 하나 있어요.
진입로, 원본 비디오 영상을 찍어서 S3에 넣기
버킷, 오브젝트 스토리지.하지만 그런 반지는 아니에요.
제 핸드폰으로 보여드리고 싶어서요.그들은 어떤 일을 해야 할 필요가 있습니다.
포스트 프로세싱 트랜스코딩.그래서 비디오를 버킷에 넣으면 이벤트가 시작됩니다. 요청을 대기열에, SQS 대기열에 넣죠. 여기서 플릿은
세 개의 작은 상자, EC2 인스턴스 플릿
트랜스코더 서비스를 실행하면 대기열이 줄어듭니다.그리고 그들이 그걸 얻으면, 오
네, 해야 할 일이 있어요. 그들이 그걸 가져갈 거예요.
비디오를 트랜스코딩하고 다른 버킷에 넣으면 카메라에서 볼 수 있어요.이것이 트랜스코더가 작동하는 방식입니다.하지만 다른 많은 서비스와 마찬가지로
Ring은 규모를 늘리고 축소할 수 있어야 합니다.이제 아마존에 있는 대부분의 서비스를 보면 웹사이트나 동영상, 음악을 보면 알아서 할 수 있겠죠.
프라임 데이를 전후한 대규모 이벤트.하지만 링은 달라요, 그렇죠?링 (Ring) 이 비디오 트랜스코딩을 하고 있는데 어떻게 생각하세요?
Ring의 가장 큰 이벤트는 비디오 트랜스코딩을 하는 곳이죠?네, 알겠어요. 할로윈이에요.아이들이 집집마다 돌아다니며 움직임 감지 기능을 끄고 있네요.저는 개인적으로 좋아해요. 왜냐면
난 애들을 데리고 속임수를 쓰거나 치료하는데 아내가 남아요
집에 캔디 볼이 있는데 작은 알림을 받을 수 있어요.
애들한테 우리 집 문 앞으로 와서 사탕을 사느라 돈을 낭비하지 않았다는 걸 보여줬어요정말 좋아요.하지만 그들은, 링에겐 필요해요
규모를 확장할 수 있으려면 말이죠.아마 어마어마한 규모일 거예요.
거기서 그 모든 비디오를 트랜스코딩할 수 있게 된 거죠.그럼 어떻게 하는 거죠?자, 그 아키텍처를 다시 보여드릴게요. 이 아키텍처는 다음을 사용하여 모니터링합니다.
CloudWatch는 대기열과 지표를 모니터링합니다.
빈 수신이라고 합니다.빈 리시브는 흥미롭습니다. 왜냐하면 빈 리시브가 있다면 말이죠.
빈 리시브가 많아요. 극지방 사람들이 일자리를 요청하는데 거긴 아무것도 없어요.
규모가 너무 커진 것 같아서 규모를 축소할 수 있다는 뜻이죠.하지만 그들이 일자리를 요구하는데 빈 수취가 없다면, 항상 일거리가 있는 거죠.우리가 아마 그럴 수도 있다는 뜻이죠.
대기열을 백업하려면 규모를 늘려야 합니다.그래서 그들은 그 데이터를 공급합니다.
상태 머신인 단계 함수에
그 데이터와 다른 데이터를 가져갈 수 있는 곳이죠.
독점 지표 및 확장할지 또는 축소할지 결정
동영상을 최대한 빨리 제공할 수 있습니다.그 예를 들자면
이건 이전 예시인데요, 줄이는 것에 관한 겁니다.
동영상 시청 지연.그래서 다음 예시는
지연 시간은 모든 것 사이의 지연 시간을 줄이는 데 초점을 맞춥니다.기본적으로 Ring은 다음과 같이 제작되었습니다.
디바이스 간의 이벤트 기반 아키텍처와
해당 백엔드 서비스 및 서비스에 대한 서비스.모든 것이 이벤트 중심입니다.그래서 그들은 비용을 줄일 수 있는 시스템을 만들고자 했습니다.
그런 것들이 서로 최대한 많이 대화하고 복원력을 높이는 데 걸리는 시간을 최대한 줄여야 합니다.
그리고 가능한 한 확장 가능하죠.이벤트 주도형이란 무슨 뜻인가요?예를 들어, 카메라는
Stream start와 같은 이벤트를 녹화하세요. 이름이 바로
내부적으로 볼 때 이 이벤트는
장치가 움직임을 감지했습니다.그러면 해당 이벤트를 알림 서비스로 라우팅해야 합니다. 그 알림은
서비스에서 푸시 알림을 보내서 알려줄 거예요.
제 초인종 앞에 누가 있는데 이 경우에는 저,
저 혼자서 초인종을 맞았어요.하지만 여전히 사용자로서 알고 싶을 것입니다.
최대한 빨리.예를 들어보죠.그래서 그들은 스트리밍 이벤트 버스, 즉 SEB, SEB를 만들었습니다.그래서 저는 아키텍처 다이어그램을 만드는 걸 좋아해요.이건 좀 복잡해 보이지만 제가 안내해 드릴게요. 그럼 좀 더 자세히 설명해 드릴게요.
한 조각씩 내려가죠.우선, 여러 개의 아키텍처가 있습니다. 바로 다중 계층 아키텍처입니다.회색으로 표시된 모든 것이
SEB의 범위를 벗어납니다.그래서 이벤트 프로듀서가 있습니다.
카메라나 기타 다양한 서비스, 이벤트 소비자들이 이벤트를 좋아하죠. 이벤트 서버, 알림 서비스 같은 말이죠.
앞서 보여드렸는데요.그러니까, 저건 회색으로 되어 있네요.
흰색으로 표시된 모든 것은 SEB입니다.첫 번째 티어는 API 레이어이고, API 레이어에서는
인증을 좀 하면 로직이 좀 되긴 하지만
라우팅도 하고 있어요. 튤립이 보여줬던 것처럼 말이죠.어떤 세포를 선택할지를 결정하는 거지
이벤트 주제를 기반으로 특정 이벤트를 보낼 수 있습니다.처리 레이어에서는
여기 Kafka, Apache를 실행하는 셀이 여러 개 있는 것을 볼 수 있습니다.
Kafka는 처리량이 많고 확장성이 뛰어난 이벤트입니다.
스트림 처리 시스템.그리고 이 레이어에서는
소비자 프록시입니다.그들은 영리한 일을 했어요.
그들은 많은 소비자를 수용할 수 있기를 원했지만 모든 소비자를 원하지는 않았습니다.
그 소비자들은 카프카에 투표해야 합니다.그래서 그들이 한 일은
Kafka를 대상으로 설문조사를 실시한 다음 이벤트를 제공하는 소비자 프록시를 만들었습니다.
직접 API를 호출하거나 SQS 대기열에 넣을 수 있습니다.이것이 바로 SEB입니다.앞서 말씀드렸듯이 멀티 셀입니다. 이 핑크색 박스는 각각
별도의 AWS 계정.그들은 이 모든 티어와 셀을 서로 다른 티어로 나누었습니다.
AWS는 폭발 반경과 관리 용이성 모두를 고려합니다.앞서 말했듯이 Kafka는
이벤트 스트리밍 시스템.확장성이 뛰어나고 처리량이 높습니다.그렇다면 매니지드 Kafka 또는 Kafka, MSK의 매니지드 스트리밍이란 무엇일까요?매니지드 카프카는 한 가지 방법입니다.
AWS에서 Kafka를 실행할 수 있으며 AWS에서 설정을 처리합니다.
클러스터를 대신 설치해 주세요.걱정하지 않으셔도 됩니다.
알고 계신 서버 설정에 대해 말씀해 주실 수 있습니다.
원하는 서버, 원하는 대로 말하면 알아서 해줘요
암호화는 사용자가 공유 스토리지를 원한다고 지시하면 알아서 처리해 줍니다.그래서 관리되는 거죠.이것이 바로 관리라는 의미입니다.그래서 그들은 SEB, 스트리밍 이벤트 버스를 셀룰러 아키텍처로 만들었죠?그래서 셀룰러 아키텍처는
도구에 따르면 상단에 있는 작은 분홍색 상자나 다양한 이벤트가 예정되어 있다고 합니다.
수천 개가 들어왔어요이벤트 주제를 바탕으로
첫 번째 방으로 가거나 두 번째 방으로 갈 거예요그리고 세포에 관한 건
아키텍처는 폭발 반경과 같습니다. 튤립은 이미 여러분께 알려드렸죠.셀이 다운되면 토픽의 절반만 영향을 받고 나머지 절반은 여전히 작동합니다.그러니까 꽤 괜찮은 거죠.하지만
팀은 이 기능을 구현한 후 정말 흥미로운 것을 발견했습니다.연구진은 세포가 고장나도 실제로 고장날 수 있다는 걸 알아냈습니다.
다른 칸을 확장해서 거기에 있는 모든 주제를 수용하세요.셀룰러 방식이긴 하지만 필요하면 실제로 확장해서 사용할 수 있습니다.
모든 세포를 수용할 수 있죠.그래서 그들은 모든 것을 얻게 되죠.
이점, 셀룰러 아키텍처의 모든 확장성.이제 폭발 반경은 아무 소용이 없습니다. 모든 주제에 대해 남은 건강한 세포가 처리하기 때문이죠.그리고 이건 뭔가요?
제가 정말 좋아하는 일을 해냈다는 게 정말 영리해요.이 경우 하나의 세포는 볼 수 있지만 두 번째 세포는 그렇지 않습니다.
건강해요, 쓰러졌어요왜 그럴까요?왜냐하면 튤립이 세포에는 결함이 있다고 했으니까요.
격리 경계, 그게 요점이에요. 그 결함이 세포를 가로질러 퍼져서는 안 됩니다.하지만 일어날 수 있는 일이죠. 상관관계가 있는 실패라고 부르세요.몇 가지 실패가 있습니다.
이는 여러 세포에 어느 정도 상관관계가 있는 영향을 미칩니다.Kafka에 문제가 있거나 버그를 배포했다고 가정해 봅시다.
그들의 카프카 구현.이 경우에는 많은 서비스, 많은 구현에서 선택할 수 있습니다.
여러 지역으로 진출하려는 거죠?오, 카프카와 미국 동부에 문제가 생겼어요.
미국 서부 두 군데로 넘어갔지만 그들은 그 길로 가지 않았어요.그들은 이런 것들을 만들었죠.
폴백 셀이라고 합니다.저는 그걸 셀 3이라고 부릅니다.
하지만 저는 셀을 따옴표로 묶었습니다. 왜냐하면 세포는 정말
어디에나 같은 스택이 있어야 하죠.하지만 세 번째 셀은 같은 스택이 아닙니다. 세 번째 셀은 실행되고 있지 않습니다.
카프카, MSK를 실행하는 게 아니라 SNS를 실행하고 있어요. 심플
알림 서비스.간단한 알림 서비스와 간단한 대기열 서비스를 사용하여
스트림 처리를 꼭 Kafka처럼 효율적으로 할 필요는 없습니다.
셀 1과 셀 2를 사용하지만 가용성을 유지하기 위한 것이므로 상관 관계가 있는 오류를 방지할 수 있습니다.상관 관계가 있는 실패는 다음과 같습니다.
MSK 또는 Kafka와 관련된 문제인데, 그럴 가능성은 거의 없습니다.
SNS와 SQS에 영향을 미칠 수 있기 때문에
가용성 유지를 위해서죠.다른 패턴은
회로 차단기가 구현되었습니다.많은 사람들이 가지고 있는 것 같아요.
서킷 브레이커에 대해 들어봤어요.어차피 다루어서 모두 같은 이해를 바탕으로 어떻게 해냈는지 보여드리도록 할게요.회로 차단기를 사용하면 회로가 닫히기 시작합니다.
그리고 닫힌 것도 좋습니다.전등 스위치를 생각해보세요.
닫을 때, 뒤집을 때, 전등 스위치가 켜져 있을 때
회로가 닫혀 있다는 뜻이에요.
전기가 흐르죠?그리고 그건 좋은 일이에요.이 경우 회로가 닫혔다는 것은
셀이 요청을 수락하고 있습니다.하지만 특정 결과를 얻으면
임계값을 초과하거나 초과한 오류의 수는
세포가 건강에 해롭다고 생각한다는 것을 알려주죠.
회로를 열면 첫 번째 세포에서 회로가 열린 것을 볼 수 있습니다.이렇게 하면 요청은 가능하지 않습니다.
병든 감방에 갈 거야. 안 갈 감방엔 안 갈 거야
서빙을 받으면 건강한 감방으로 갈 거예요그리고 기억하세요, 만약 당신이 만약 진단을 받으면
두 번째 칸에 가끔 오류가 몇 번 생기면, 따옴표를 붙이지 않은 세포로 되돌아오게 되죠.
세 개는 다른 기술을 사용해서 거기에서 서비스를 제공하죠.그러면 추가 혜택을 받을 수 있습니다.
한 층 더 높은 탄력성.이제 회로가 열리면 반쯤 열린 상태가 됩니다.반쯤 열린 상태에서는 요청을 보내죠. 가끔씩 요청을 보내죠.그리고 충분하다면
정상 요청은 세포가 건강한 상태인지 평가하고
회로를 다시 닫으면 세포는
요청을 다시 수신합니다.그래서 방금 보여드린 게
이러한 이벤트와 알림이 예전에 어떻게 생겼는지 보여주는 예시인데, 다시 가져가고 싶어요.저는 이런 것에서 배운 게 있어요.
서비스가 실제로 어떤 역할을 하는지 이해해서 예를 들어보죠.이 사례를 하나 더 보여드릴게요.이전 스트리밍 시작 예시를 기억하세요. 카메라가 움직임을 감지하면 푸시 알림이 전송되었죠.자, 여기 또 하나 있어요.
예를 들어, 실제로 같은 이벤트를 이벤트 매니저에게 보내면 이벤트 매니저는
제가 볼 수 있도록 멋진 타임라인을 만들어 주세요
저기 제 문 앞에서 사람이 감지되거나 소포가 감지됐을 때, 이거
아마존 배송 기사를 예로 들어보죠.재밌는 예를 하나 보여드릴게요.
링 링 이벤트.링 링 이벤트는
누군가 초인종을 울렸어요. 이미 설정했다면
해당 이벤트를 봇 서비스에 전송하면 봇 서비스는
자동 응답을 실행하세요.자동 응답은
메시지를 남기는 것처럼 평범한 것이거나 할로윈의 경우 트릭 오어 트릿 같은 것일 수 있습니다.어떤 내용인지 읽을 수 있을 겁니다.
저기, 재밌어요.그리고 어마어마한 규모를 약속할게요.오른쪽에 보시는 것처럼 분홍색 상자나 메시지가 수천 개에 달합니다.
메시지가 많이 들어옵니다.그럼 한 개당 메시지 수는 얼마나 될까요?
SEB가 받는 시간은 몇 시간인가요?왼쪽에 있는 그래프를 보면 실제로 그래프를 보여주고 있습니다.
8개 지역의 시간당 메시지 수.따라서 Ring은 SEB를 8개 지역에 배포했습니다.미국 동부에서 가장 큰 지역인 곳은
배포된 데이터는 약속한 초당 129,000개까지 올라가지만 실제로는 그보다 더 높습니다.그냥 최고치일 뿐이죠.
제가 받은 스크린 샷 때문이었어요.하지만 그게 얼마나 높이 올라가는지를 잘 표현한 거죠.하지만 보시면 아시겠지만 실제로는
여러 지역이 모두 별도의 하위 스택을 실행합니다.
그리고 이 모든 이벤트를 제공합니다.여덟 개의 서로 다른 지역으로 나뉘어져 있기 때문에 보시면 이렇게 적혀 있습니다.
8개 지역 전체에서 초당 총 299,000건의 요청을 처리했으며, 약속한 99.99건의 가용성으로 처리되고 있습니다.
이 8개 지역 모두요.그리고 우리는 각 지역을 살펴봅니다.
각 지역을 분석해 보세요.미국 동부 지역의 경우 사흘간의 기간만 봐도 알 수 있습니다.
여기 보고 있었는데 100개를 달성할 수 있었어요.
SEB에서의 가용성 퍼센트그 이유는 그들이
셀룰러 아키텍처를 구현했고
페일오버와 페일백을 구현했고
회로 차단기를 구현하고 이러한 조치를 취했으며 이러한 조치를 가장 잘 적용했습니다.
99.999%, 심지어 100% 의 가용성까지 구현할 수 있는 방법이 있습니다.그리고 그걸로 제가 할 수 있는 건
아비나쉬한테 넘겨줘요. - 고마워요 세스좋아요, 잠깐 손을 들어서 몇 명인지 봅시다.
여기서는 Alexa 모바일 앱을 사용하고 있습니다.손 몇 개 예쁘네요좋아요, 오늘은 Alexa가 어떻게 복원력을 개선하고 개발자 속도를 향상시켰는지에 대해 알아보겠습니다.특히 Alexa 모바일 개인화의 예를 들어보겠습니다.저는 아비나쉬 콜루리예요.
AWS의 시니어 솔루션스 아키텍트.저는 아마존을 지지하고 있어요.
AWS 고객으로서 말이죠.저는 주로 일을 하고 있습니다.
Alexa와 디바이스를 사용하고 있어요.좋아요, 알렉사 모바일
개인화는 기본적으로 모두를 위한 랜딩 존 앱입니다.
이런 종류의 스마트 디바이스는 Alexa와 통합되어 있습니다.이를 통해 다음과 같은 것들을 정리할 수도 있습니다.
빠르게 정리하고 싶었던 작업들
즐겨 사용하는 기기 또는 다른 기기를 사용할 수도 있습니다.
다음과 같은 특정 작업을 수행하기 위한 특정 단축키
새 온도 조절기 온도 제어 또는 전환
거실 조명 켜요.동시에 날씨 업데이트나 교통 정보 업데이트와 같은 일상 생활에 주로 집중할 수 있도록 도와줍니다.세스가 앞서 지적했듯이
우리가 지원하는 마이크로서비스의 광대한 에코시스템에서 Alexa 모바일 개인화는
다른 많은 다운스트림의 트리거링 포인트 역할을 하는 것도 그 중 하나입니다.
Alexa 전반의 서비스.우리가 집중하고자 했던 레질리언스 목표 중 일부는 다음과 같습니다. 이는 마치 전 세계 공통의 목표를 요약해 놓은 것과 같습니다.
우리에겐 다양한 조직들이 있어요.우리는 고객으로부터 출발합니다
집착에 대한 배경, 그리고 고객 경험 개선은 우리의 주요 우선 순위 중 하나입니다.그리고 동시에, 언제
앞서 논의한 바와 같이 특정 피크 이벤트가 있습니다.
프라임 데이일 수도 있고 어떤 종류의 이벤트일 수도 있습니다.새로운 디바이스가 많이 추가되고 있고, 새 디바이스가 추가되는 것을 볼 수 있지만, 그와 동시에
또한 해당 다운스트림 서비스를 위해 확장하고 투명성을 지원해야 할 것입니다.따라서 이러한 모든 피크 이벤트와 다운스트림 서비스를 확장하고 지원해야 합니다.다음은 내결함성입니다.우리는 고객이 결함과 문제를 발견하기 전에, 그리고 동시에 사전 식별할 수 있도록 하고 싶었습니다.
비상 조치를 취하세요.따라서 내결함성은
우리에게 있어 또 다른 중요한 핵심 측면이죠.이 모든 작업을 수행하려면 개발자의 많은 노력이 필요함과 동시에 많은 복원력 노력이 필요합니다.
아직 개발자 내에서는 말이죠.개발자들이 항상 집중할 수 있도록 하고 싶었습니다.
주로 혁신과 개발 활동에 중점을 두었습니다.
하지만 운영 또는 탄력성 활동에 대해서는 더 자세히 설명하지 않겠습니다.에 대해 간단히 복습해 보겠습니다.
엔지니어링을 정말 혼란스럽게 만들었네요.깊이 들어가기 전에
어떻게, 무엇을, 어떤 실험 템플릿으로, 어떤 오류 행동을 하는지 이해하세요.
이 전체 프레젠테이션을 살펴보려고 합니다.그래서 카오스 엔지니어링은
숨겨진 문제를 발견하는 데 도움이 되는 학문, 그리고 다음을 할 수 있도록 도와줍니다.
시스템 복원력을 한 번에 개선하세요.
통제된 환경.다음과 같은 경우에 도움이 되는 경우가 많습니다.
옵저버빌리티 및 모니터링 표준을 평가해 보세요.
이를 통해 숨겨진 문제를 발견하고 개선할 수 있는 여지를 남길 수 있습니다.
옵저버빌리티 포지션.일반적으로 카오스 엔지니어링 또는 레질리언스 엔지니어링입니다.
안정된 상태에서 시작합니다.그리고 정상 상태는
기본적으로 최적의 조건과 작동 방식, 그리고 가설이 뒤따르는 시스템의 이상적인 상태입니다.여기가 실제로 여러분이 있는 곳입니다.
결점을 소개하고 그 방법을 알아보고 싶었습니다.
당신의 시스템이 어떻게 동작하는지, 어떻게 도입할 건지
그 결함들, 이 실험 덕분이죠.따라서 실험은 기본적으로 여러분이 할 수 있는 행동과 변수라고 할 수 있습니다.
자신의 잘못을 고려해 본 다음
이를 AWS 리소스에 소개하십시오.그러니까 한 번, 한 번
이 실험을 시작하면 계속 진행하면서 어떻게 하는지 검증하게 됩니다.
실험은 순조롭게 진행되고 있으며, 이 검증은 보통 정상 상태를 기준으로 이루어집니다.그래야 안정된 상태를 항상 최적의 상태로 유지할 수 있습니다.그렇지 않다면,
확실히 개선의 여지가 있습니다.전반적인 레질리언스 여정에서 많은 어려움을 겪었지만, 다시 한 번 말씀드리지만, 몇 가지 사항을 다시 한 번 말씀드리겠습니다.
이러한 문제들을 저희가 해결하고자 했던 것들이죠.그래서 처음에는 작업을 시도해봤어요.
우리는 다양한 도구와 기술을 사용하여 구축했습니다.
저희가 직접 만든 스크립트를 만들었기 때문에 이 모든 스크립트는
도구를 사용하려면 많은 운영 기능이 필요합니다. 왜냐하면 직접 가봐야 하기 때문입니다.
유지 관리, 패치 적용, 운영 및 운영상의 특정 부담도 해결하세요.그래서 그렇게 하는 동안
우리가 관찰한 것은 우리가 한 곳에서 온 것과 같습니다.
기술 스택이 다양해졌기 때문에 호환성을 확보하는 것도 마찬가지로 중요합니다.그리고 많은 도구, 에이전트, 라이브러리가 참석할 때
이 모든 기술 스택에서의 호환성
또 다른 과제입니다.동시에 보안을 더욱 강화하고 싶었죠.우리는 방을 나가는 게 아니에요.
생산 시스템 내에서 다른 도구나 에이전트를 사용할 때 발생하는 모든 종류의 침입자나 누수에 대비할 수 있습니다.마지막으로는 실제 이벤트, 실제 시나리오를 모방하는 것입니다.그러기 위해서는 우리가 해야 할 일이 있을 것입니다.
일련의 이벤트를 모방하거나 시뮬레이션하기 위해 서로 다른 팀과 인력을 한데 모아 모두가 특정 표준을 준수하는지 확인하세요.안타깝게도 이 모든 것을 말씀드리자면, 저희는 어벤져스가 아닙니다.
그리고 그냥 개발자일 뿐이죠.자, 이제 AWS 장애 주입 서비스에 의존하기 시작했습니다.이것은 관리형 혼돈입니다.
장애를 직접 해결할 수 있도록 도와주는 실험 서비스
AWS 리소스에 대한 격리 실험 또는 작업.많은 것을 지원하지만
다양한 AWS 리소스에서 작업을 수행하지만 오늘은
이 두 가지에 대해 소개하겠습니다.하나는 종료, 중지 또는 부팅을 어떻게 종료할 수 있는지에 대한 Amazon EC2 인스턴스에 관한 것입니다.다른 하나는 시스템을 실행할 수 있는 방법에 관한 것입니다.
관리자가 명령을 실행하고 명시적 제어를 할 수 있습니다.
리소스에 대한 조치.그리고 한 가지 좋은 점은
장애 주입 서비스는 CloudWatch와 함께 사용되므로 다음과 같은 설정을 할 수 있습니다.
자체 모니터링 및 경보를 확인하고 가드레일에 정지 또는 종료 조건이 있는지 확인하여 이런 종류의 실험을 수행하는 동안 언제 종료해야 하는지 알 수 있습니다.간략히 살펴보자면 다음과 같습니다.
우리의 안정 상태 개요.우리는 우리의 것을 확실히 하고 싶습니다.
CPU는 항상 50% 미만이고 메모리는 20% 미만이지만 동시에 최소 3% 이상을 지원하고 싶었습니다.
100밀리초 미만의 P90 지연 시간 내에 있는 사용자 수는 백만 명입니다.첫 번째 예는 다음과 같습니다.
CPU 및 메모리 스트레스 실험에 대해 이야기하겠습니다.왼쪽이 보이시죠.
가설을 보고 있네요.그럼 내가 여기서 뭘 하는 거지?여기 Alexa 안에 있어요
모바일 개인화: CPU 및 메모리 부하가 40% 에 달합니다.그리고 동시에 우리는
또한 로드 제너레이터를 통해 트래픽을 확장합니다.
30% 더 늘렸죠.그렇게 했을 때, 우리가 기대했던 것과 같은 결과를 얻을 수 있습니다.
어떤 사건도 보고되지 않았고, 동시에 P99 지연 시간도 100밀리초 이내로 유지됩니다. 단, 가끔 130밀리초까지 급증하는 경우는 예외입니다.여기서 사용하고 있는 완화 방법은 자동 크기 조정입니다.실험 템플릿은 기본적으로 장애 주입 서비스 내에서 직접 사용할 수 있는 JSON 또는 YAML 템플릿입니다.그리고 이름 설명과 RoLearn으로 시작합니다.RoLearn은 기본적으로 다시 한 번 더 명확하게 설명하기 위해 사용됩니다.
대상 또는 AWS 리소스에 대한 제어, 즉
이러한 작업을 바로 실행하고 싶었죠.앞서 말씀드렸듯이, 저희는
여기서 조건을 멈추고 우리가 꼭 가져갈 수 있도록
우리의 실험 내에서는 통제된 방식으로 말이죠.그리고 중지 조건
클라우드워치 알람입니다.그리고 대상은 EC2 인스턴스이고, 우리는 ResourceArns를 사용하여 모든 인스턴스를 분류하고 이 실험을 활용하고 있는지 확인하고 있습니다.
이러한 대상에 대해서요.그리고 이것들은 행동들입니다.
액션에는 시스템 관리자 파라 명령이 포함됩니다.
이로 인해 CPU와 메모리 부하가 가중되고 있습니다.그리고 이러한 동작은
EC2 인스턴스의 해당 대상에 대해 실행됩니다.전체 템플릿이 바로 여기에 있습니다.이 템플릿을 실행해 보았을 때, 몇 가지를 간단히 설명해 드리겠습니다.
전체 인프라 스택에서 캡처한 많은 이벤트의 스냅샷입니다.그리고 우리는 CloudWatch를 사용하여
이 모든 이벤트를 캡처하세요.그리고 이건 특별한 건데요,
그 사건들을 예로 들어보죠.저희가 관찰한 바로는,
도입했기 때문에 CPU 사용률이 있습니다.
40% 의 추가 CPU, 그리고 동시에
메모리가 커졌습니다.그리고 우리가 관찰한 것도
이러한 EC2 인스턴스에서 네트워크 트래픽이 급증했나요? 인스턴스도 생성 중이기 때문입니다.
추가 TPS 부하.그렇게 해보니 P 99의 지연 시간을 알 수 있었습니다.
결과 내에서도 여전히 시간 내에 머물러 있습니다.130, 133밀리초인데
한 인스턴스일 뿐이지만 P 90의 지연 시간은 여전히 100밀리초 미만으로 유지됩니다.따라서 인프라 스택에 대한 확신을 가질 수 있습니다.
CPU와 메모리가 제한되어 있더라도 추가 부하와 추가 트래픽을 감당할 준비가 되었습니다.다음은 두 번째 실험입니다.따라서 이 실험에서 살펴보려는 것은 가용 영역 중단을 구현하려는 것입니다.그러니까 한 가지 예를 들자면
실시간 실패의 경우, 어떤 일이 일어날까요?
가용 영역이 다운되었는데 어떻게 대처할 생각이세요?다시 말씀드리지만, 가설은 끝났습니다.
여기에 가용 영역 손상 요인이 적용되고 있는데, 예상했던 완화 효과를 적용하고 있습니다.
외부 스케일링이 시작되어야 할까요?
가용 영역.결과는 트래픽입니다.
적절하게 처리해야 합니다. 왜냐하면 지금은 삭감하고 있으니까요.
가용 영역을 좁히기 위해 다음 사항을 확인하고 싶었습니다.
트래픽은 정상적으로 전달되며, 동시에 P90도
지연 시간이 다시 100밀리초 미만으로 단축되었습니다.앞서 설명했듯이 실험은 다음과 같이 시작됩니다.
역할 설명 및 Arn.그리고 다시 중지 조건이 생겼습니다.
여기 CloudWatch 경보가 있고, 대상은 EC2 인스턴스이고, 여기 있는 작업은
다음과 관련된 EC2 인스턴스 세트를 중지하려고 합니다.
특정 가용 영역.AWS 사용의 장점
FIS 실험은 직접 진행하면 전체를 파악할 수 있습니다.
LA에서 연말에 연속적으로 또는 병렬로 실시된 실험에서는 CPU 스트레스와 메모리 스트레스를 살펴보게 됩니다.
실험은 이후에 시작될 예정입니다. 죄송합니다. 이 혼란스러운 가용 영역 운영 중단 실험은 CPU와 메모리 스트레스 이후에 시작될 것입니다.다음은 CloudWatch 대시보드를 기반으로 한 또 다른 관찰 결과입니다.실행하면서 본 것은
이 실험을 통해 TPS가 지속적으로 증가하는 것을 확인할 수 있었습니다.가용 영역 중 하나를 다운시켰는데 나머지 두 개는 즉시 CPU가 급증하기 때문입니다.
가용 영역은 이 트래픽을 받기 시작해야 하며, 동시에
평균 지연 시간은 여전히 100밀리초 미만입니다.이 점은 주목할 만한 사실입니다.
우리가 나서서 인프라를 확보할 수 있는 공간을 마련해 주는 것과 같습니다.
항상 준비된 상태이므로 서비스를 제공할 수 있습니다.
정전이 발생했는데도 트래픽이
가용 영역 하나.그래서 저희는 이렇게 시작했습니다.우리는 우리의 것으로 시작했습니다.
수동 테스트, 부하 테스트, 그리고 게임데이즈로 옮겼습니다.이런 것들은 언제나 변치 않습니다.네.그리고 나서 소개했습니다.
모든 기능 및 기존 테스트를 파이프라인에 통합하고 파이프라인에 통합했습니다.
매번 개발 배포 주기에 맞는지 확인하세요.그리고 동시에 저희는
저희 회사의 디자인 작업에도 참여하기 시작했습니다.
자체 개발 도구들과 기술들이 한데 어우러져 있습니다.
이런 종류의 복원력 테스트를 하기 위한 스크립트 측면에서요.하지만 우리는 모든 것을 이해했습니다.
여기에는 상당한 운영 과부하가 포함됩니다.
저희한테는 비용도 마찬가지고요.그리고 나서 우리는 Fault Injection Service를 이용한 완전 자동화된 카오스 테스트로 완전히 전환했습니다.이제 또 다른 이점이 있습니다.
AWS FIS를 사용하면 이러한 실험 템플릿을 여러 곳에서 공유할 수 있습니다.
개발자 커뮤니티.따라서 다른 팀에서는 처음부터 이 작업을 시작하지 않을 수 있지만 이를 추상화 계층으로 직접 사용할 수는 있습니다.
그들의 서비스 외에도 말이죠.몇 가지 주요 시사점은
제가, 우리가 이 모든 실험을 통해 관찰한 바로는
3백만에서 4백만까지 규모를 확장할 수 있었죠.
인프라 측면에서는 아무 것도 변경하지 않고도 사용자를 확보할 수 있습니다.현재 상황이 많이 개선되고 있습니다.
우리의 운영 탄력성 덕분에 이제 개발자들은 혁신과 새로운 개발 활동에 더 많은 시간을 할애할 수 있게 되었습니다.
우리는 계산에 따라 그 사실을 알게 되었습니다.
개발자 시간은 거의 640시간에 달합니다.
분기당 절감된 금액으로 개발자의 업무가 크게 개선되고 있습니다.
제품 생산성도 마찬가지죠.또한 다음 사항도 확인했습니다.
우리가 운영하던 인프라 중 일부를 철거했습니다.
40% 의 CPU와 메모리를 대상으로 한 실험을 기반으로 프로비저닝했으며, 이를 통해 전체 인프라 비용의 60% 를 절감하고 다음 작업을 수행할 수 있었습니다.
30% 의 탄소 절감.그럼 다음으로 넘어가겠습니다.
다음 사용 사례로 튤립을 살펴보겠습니다.고마워요. - 아비나쉬 고마워요.물 한 모금 마실게요. - 여기 너무 말랐어요.(웃음) - 카오스 엔지니어링에 관한 이야기를 들으셨을 텐데요.
셀 기반 아키텍처, 링 아키텍처도 마찬가지죠.그래서 이걸 가지고 있으면
AWS에서 실행되는 대규모 아키텍처 또는 대규모 워크로드, 관찰 가능성,
정말 중요해지고 있습니다.할 수 있으면 좋겠어요.
인프라를 모니터링하세요.그래서 이 부분에서는
세션에서는 Audible이 CloudWatch 통합을 사용하여 옵저버빌리티를 확장하는 방법에 대해 알아보겠습니다.
옵저버빌리티 솔루션.따라서 Audible이 최대 생산업체 중 하나라는 것을 알고 계실 것입니다.
전 세계 오디오북.그래서 그들은 많은 서비스를 가지고 있고, 각 서비스는 자체 로그와 지표를 생성합니다.따라서 이전에는 부족했습니다.
근본 원인을 정확히 찾아낼 수 있는 총체적인 관점 말이에요.그들은 할 수 없었어요.
심각도 문제가 발생한 이유에 대해 자세히 알아보세요.
매우 빠르게 발생했고 시간도 오래 걸렸습니다.그래서 CloudWatch가 등장했을 때
크로스 계정 옵저버빌리티는 작년에 다음 중 하나로 출시되었습니다.
기능 면에서는 얼리 어답터 중 하나였고 그 이점을 빠르게 실현할 수 있었습니다.이것이 바로 CloudWatch의 상호 작용입니다.
옵저버빌리티는 어떤 모습일까요?AWS 계정이 세 개 있고 ECS를 실행한다고 가정해 보겠습니다.
EC2와 람다가 시중에 나와 있습니다.따라서 AWS X-ray를 설정했을 수도 있습니다.이제 AWS X Ray가 하는 일은 하나의 요청을 추적할 수 있다는 것입니다.
다른 서비스에 서비스를 제공하고 이 추적 맵을 생성하십시오.그리고 이 모든 트레이스를 수집하고, CloudWatch가 있습니다.
수집하는 이 모든 계정에 설정해 놓았습니다.
로그와 지표.이제 크로스 클라우드워치와 함께
크로스 계정 옵저버빌리티를 통해 이러한 추적, 로그 및 지표를 하나로 전송할 수 있습니다.
AWS 모니터링 계정을 사용하면 중앙 집중식 AWS OB 옵저버빌리티 계정이 됩니다.그리고 로그만 하면 됩니다.
모니터링 계정에 접속해서 상관 관계를 파악할 수 있어야 합니다.
로그, 트레이스, 메트릭의 상관 관계를 전체적으로 파악하세요.
소스 계정.이제 데모를 해볼게요
심각도 문제를 추적하는 과정은 기본적으로 단계별로 진행됩니다.
Audible의 엔지니어에게 전화를 걸어 어떻게 처리하는지 확인해 보세요.자, 이 흔적을 보여드리죠.
지도, 어떻게 생겼는지그렇지 않은 분들을 위해
트레이스 맵이 뭔지 알다시피 트레이스 맵은 기본적으로 요청이 한 서비스에서 다른 서비스로 어떻게 전달되는지를 보여줍니다.자, 그럼 본격적으로 살펴보죠.
대기 중인 엔지니어의 입장에서 말하자면, 클라이언트 중 하나가
요청을 볼 수 없습니다.그럼 어떻게 하시겠어요?예전에는 Audible의 온콜 엔지니어가 별도의 계정에 로그인해서 로그와 지표를 살펴봐야 했죠.하지만 이제는 로그인만 하면 됩니다.
여기 모니터링 계정이 하나 있는데 추적할 수 있어요.
그 요청은 여기 있어요.그래서 트레이스 맵에서는
화살표를 통해 요청이 한 서비스 노드에서 다른 서비스 노드로 어떻게 전달되는지 확인할 수 있습니다.여기 동그라미가 있는 동안
서비스 노드는 다음과 같습니다. 맨 위에는
작은 빨간 점이 보이네요.그리고 저 빨간 점들은 뭐지?
오류가 발생했는지 아니면 오류가 발생했는지를 나타냅니다.
특정 서비스 노드 1개그러니까 이게 정말 현실이 되는 거죠.
그냥 하나의 계정으로 가서 서비스 맵을 보고 어디서 모든 오류가 발생했는지 확인하면 됩니다.또한 필터링할 수도 있습니다.그래서 이것은 모든 것을 모으는 것입니다.
여기에 나와 있는 네 개의 AWS 계정과 비슷한 모든 서비스를 제공합니다. 필터링하여 계정 중 하나를 선택할 수 있습니다.
이 경우에는 세 개, 두 개, 여기에 연결된 모든 서비스를 볼 수 있습니다.원하신다면 다른 것을 선택해 주세요.
세븐 제로 같은 계정은 두 개처럼 보일 수 있습니다.
세븐 제로에 연결되어 있고 좋아요도 할 수 있는 서비스
예를 들어, 별도의 AWS 계정을 살펴보면 다음과 같은 혜택도 받을 수 있습니다.
전체론적 관점을 얻을 수 있죠.다시 돌아가 봅시다.이제 서비스 노드를 볼 수 있게 되었습니다.
그 오류가 있네요.그래서 해당 서비스 노드를 클릭합니다.그러면 이런 관점이 떠오릅니다.이게 여러분에게 도움이 되는 것은
메트릭과 트레이스의 상관관계를 분석하세요.그리고 트레이스 맵을 볼 수 있고, 가능할까요? 하단에서 메트릭을 볼 수 있습니다.이제 지연 시간과 같은 측정항목도 볼 수 있고 결함과 같은 측정항목도 볼 수 있습니다. 이는 기본적으로 다음을 나타냅니다.
여기에도 오류가 있습니다.그리고 더 깊이 들어가고 싶다면
더 깊이 파고들어 보세요. 여러분이 해야 할 일은
트레이스 보기를 클릭하면 해당 장애가 발생한 시점의 트레이스 세그먼트를 확인할 수 있습니다.이를 클릭하면 다음과 같은 화면이 나타납니다. 여기서 다음을 볼 수 있습니다.
맨 위에 있는 트레이스 맵이 보이시죠?
이 트레이스가 수집될 당시와 관련된 결함입니다.그리고 무슨 일이 있었는지, 정확히 어떤 원인이 있었는지도 보여줍니다.원인은 고객이었습니다.
ID가 전파되지 않았거나 한 서비스 노드에서 다른 서비스로 전송되지 않았습니다.따라서 빠르게 심층 분석하여 오류가 발생한 이유를 확인할 수 있습니다.또 다른 방법으로는 뷰 트레이스를 클릭한 페이지에서 보기를 클릭할 수도 있습니다.
컨테이너 로그 인사이트이제 이렇게 하실 수 있습니다.
로그인하세요. 내부 화면 밖으로 나가세요.그러면 자동으로 여기가 시작됩니다.
오류가 발생한 기간을 선택하면 해당 시간대와 관련된 모든 로그를 볼 수 있습니다.또한 이 오류가 발생하면 추적 ID를 가져옵니다.그래서 우리는 다음을 얻을 수 있습니다.
이러한 오류가 발생한 이유에 대한 추가 정보그 결과, 이후
Audible은 이러한 교차 계정 관찰 기능을 구현하여 로그, 추적 및 메트릭의 상관 관계를 쉽게 파악할 수 있었습니다.그들은 겨우 해낼 수 있었습니다.
하나의 모니터링 계정을 사용하여 전체를 살펴보세요.
소스 계정.그 결과 디버깅 시간이 60% 이상 단축되었습니다.따라서 이전에는 평균적으로 약 한 번에 소비할 정도였죠.
모든 스레드 문제를 디버깅하는 데 평균 2시간이 걸립니다.그리고 이제 그들은 소비하고 있습니다.
거의 20~30분 정도요.그리고 이게 인용문 중 하나입니다.
한 개발자의 말에 따르면 예전처럼 여러 창에 로그인해야 했지만 이제는 하나의 창에만 로그인하면 된다고 합니다.그래서 그는 아래에서 자신의 모든 서비스를 보고 쿼리할 수 있게 되었습니다.
하나의 창으로 볼 수 있고, 덕분에 시간이 많이 절약됩니다.그리고 이걸로 저는
세스한테 넘겨줘요. - 맞아요.고마워요 튤립.저는 개발자들이 남긴 명언과 우리가 가지고 있던 사진을 보는 게 정말 좋아요.
라이선스가 부여된 사진만 있고
제가 과거에 다른 개발자 견적서에 사용했던 것과 똑같습니다.그래서 누군가는 생각하겠죠.
세상에서 가장 행복한 개발자예요.모든 것을 갖추고 있습니다.
아마존을 위한 가장 행복한 명언.오늘의 결론은
아주 간단합니다.복원력이 뛰어나고 확장 가능한 아키텍처를 구축할 수 있는 방법을 보여드리고자 합니다.예를 들어 설명해 보고 싶었습니다.그리고 예시를 보여드렸죠.
이 모든 팀으로부터, 여기 있는 모든 팀으로부터
여러분에게 영감을 줄 수 있도록 실제 사례를 보여드리고
할 수 있다는 걸 보여주세요.제가 말씀드렸듯이, 무슨 일이 있어도
여러분의 사이즈는 작든 크든 간에 이러한 원칙이 많이 적용됩니다.이러한 모범 사례 중 상당수가 적용됩니다.나가서 해보시길 바래요.이제 더 자세히 알아볼게요. 다른 세션도 있어요.
확인해 보실 수 있습니다.이 슬라이드는 에서 가져온 것입니다.
이 강연의 이전 버전입니다.그래서 이 세션들 중 일부는
이미 일어난 일일지도 몰라요브레이크아웃이면 녹화될 거예요
그러니 걱정하지 마세요.제가 시작한 라이프사이클은
얘기가 나와서 말인데, 거기에 링크가 있어요.그 외에도 몇 가지가 더 있습니다.
셀 기반 아키텍처에 대해 배울 수 있는 것들
카오스 엔지니어링, 엑스레이 등 여기 있는 모든 링크에서 보실 수 있습니다.다시 말씀드리지만, 이 글은 결국 유튜브에 게시될 것입니다.
거기서 링크를 얻으실 수 있습니다.Avinash가 FIS에 대해 이야기했지만, 몇 가지 다른 점이 있습니다.
AWS에서 복원력을 위해 특별히 구축한 서비스는
이러한 사항을 숙지하고 있어야 합니다.레질리언스 허브는 또 다른 허브입니다.
좋은 것, 그리고 AWS 백업과 엘라스틱 디재스터
복구 및 루트 53 ARC.따라서 이 서비스들은 모두 좋은 서비스입니다.
레질리언스 여정에 사용하고 싶을 수도 있습니다.그리고 이를 통해 우리는
질문 좀 할게요.설문조사를 꼭 작성해 주셨으면 좋겠지만 정말 감사합니다.
감사합니다.