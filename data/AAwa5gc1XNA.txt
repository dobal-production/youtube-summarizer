- All right. Welcome, my name is Roland Barcia. I'm the worldwide director for our specialist technology team. I have the honor to be presenting
with much smarter people, Christina and Victor. They will introduce themselves
when they start speaking for the interest of time. So let's get going, 'cause we have a lot of
material to get through. This is a 400-level session,
so we wanna go deep very fast. I'm gonna kick it off, and
kind of set some context before I hand it off to Christina. And we're gonna talk a little
bit about data to start. Obviously, in a conference,
where we're talking a lot about generative AI, LLMs, Bedrock, et cetera, data has become probably
one of the key commodities that we have. And there's lots of it, lots
of different data formats, lots of different users
or personas of data. It's no longer just analysts anymore. Its developers use data,
end users use data, internal folks use data, and people need to make
decisions fast and quickly. And so when we talk about data, we wanna think about personas, because we're at AWS,
we're customer-obsessed, and we gotta think about data from a perspective of who uses it. And so we're gonna get a little bit deeper into things like data processing
with Spark as an example. But we want to think about
things like your external users. So you have data that you expose as part of your business to your users. Oftentimes you have
historical data, batch data, things that people look
at monthly, weekly, that have a known pattern. And more and more, we have
a need to make decisions real-time with data. This is probably where we see a lot of generative AI use, right? Where people are using prompt engineering and things to do ad hoc queries, ad hoc answers to questions,
and things like that. Internally, we have personas
that wanna make use of data to make decisions about
anything and everything from how do I run a
system more efficiently? How do I scale it better? What are some security
audits that I'm seeing? And observability tools
kind of help out there. And then we have to process
lots and lots of data, right? So we're gonna get into things like Spark, and it's cluster technology, and things like Hadoop in the past. And then emerging technologies like Flink that have kind of brought
together, you know, how do I process streaming data, as well as batch data together
to make these decisions. And so let's talk a little bit
about platform engineering. A lot of our customers have
standardized on Kubernetes. They use EKS, Elastic Kubernetes Service. Some great announcements this week in other sessions in the track, like Auto Mode and things like that. But oftentimes when you are a startup, maybe you have just a development team and you do platform engineering as part of that development team. But over time, as you grow, the emergence of platform engineering has come about with Kubernetes. And a lot of those concerns really focused in the first generation
around web applications, microservices, event-driven architectures. And this emergence, I always call it a
little bit of a collision between personas and goals, right? Where developers wanted freedom, they want to use their
favorite open-source tools, their technologies, they want autonomy. I wanna build a new
app, I wanna deploy it, I wanna get it out quickly. And then you have platform teams that are worried about things
like the security systems, scalability, cost, performance. And so the first generation
of these, you know, platform teams are really
thinking about web applications, microservices, transactional systems. And platform engineering
has really emerged as a way to kind of meet in the middle between providing this autonomy and still automating things like standards in a developer-like way. There's lots of talks on
overall platform engineering. I have one on Thursday
in the serverless track from the developer persona point of view, the users of the platform. And so you can come to that
session when I get into it. But when we're talking
about platform engineering, one of the things that
customers often struggles, where is that line between the developers and the people that use the
platform, and the platform team? Do I vend out clusters? Do I vend out code as a service? Do I make it easy for developers
to just check in code? And so these are the
first generation problems. It's all been solved, right? But probably not. There's still probably lots of debate here on where the line stands, and that's because of the workloads. And so we get now into the
age of data and lots of data, and data scientists and engineers have started building
things with Notebooks, using SQL data processing. They're building stateful applications. They're building
applications with streaming and events workflows. They're using Notebooks
with Python or R to build applications and Notebooks
that are doing generative AI, or ML, or any type of data processing. They're building data
lakes and data meshes, and applications that sit in front of them to kind of federate the data. And the platform team all of a sudden has to worry about things like, what's the optimal storage to use? Do I give them GPUs? We have cool Inferentia2 and Trainium2 announced this morning that we kind of heard
about as well, right? And so, and we have new
types of DevOps, MLOps, data pipelines, the lifecycle of an LLM, the lifecycle of MyData. So these are now new concerns
in this next generation. And so I'm not gonna go into
all of these in exhaust, but there's a lot of goals
for platform engineering and data workloads, right? So how do I get the right
resources, developers? I wanna get the right
type of machine type, the right type of GPUs,
the right storage type for their workloads, for their Notebook, for their data processing. They want the right security,
the right isolation. They don't want a data
job running over here to interfere with a
transactional system over there. And I wanna do it at the
best price point possible. And of course, just like
developers, data scientists, data engineers are very
opinionated about their favorite, you know, PyTorch framework, TensorFlow, all the different types
of frameworks out there for doing data applications. And so how do I support that autonomy with this new generation of applications that are very data intensive? Lots of different challenges
with stateful workloads, right? We're talking about
things like how do I use thousands of nodes or
even the opposite problem, really large images that
take a while to start up to warm up the data. And some of the things in Kubernetes, we're really optimizing
the first generation to spin up compute very fast and detect if there's a
failure of every pass, and kill the pod, bring up a new pod. And sometimes these data workloads need to do things like check mark or checkpoint where they
are in data processing. They have these new concerns
with stateful applications. Sometimes the scalers are too fast, there's different network configurations. Do models have gravity? Does data have gravity? Even people are finding in some cases that data is cheaper to
move closer to the compute. And so you know, there is all
fluctuation of these patterns, and platform teams have to adjust to that. And so these are some of
the challenges we'll see, Christina and Victor are gonna
go into real world examples of how they implemented this. Just very quickly. You know, there's different
options for folks. By all means you can do Spark, EMR in a serverless way
using serverless paradigm if you are doing things
more native to AWS, and you get some default
services, lots of advantages. A lot of our customers choose
Kubernetes to standardize. We have one of the best
Kubernetes services out there in EKS, and there's
different ways to run Spark, because of the ecosystem you have. You can use an operator
run Spark on your own. We of course have Amazon
EMR that runs on EKS as well as an option to
give you kind of the best of both world between
management and flexibility, and the best open-source frameworks. So Kubernetes and data, right? When we talk about it, there's lots of different
patterns early on, started with storage and
custom resource definitions, moved on to event-based
applications with Kafka that gives you events,
event streaming, messaging. Then we get to spark with
the architecture moving from this type Hadoop workloads in memory intensive parallel processing. Then we get into Apache
Flink, which is like, I need both real-time data
and I need batch data, and I need to do processing
across them both, we get into I need GPUs and other types of specialized
hardware to run intensive, maybe training models et cetera. And specialized schedulers
in supporting things like Notebook as a service. So we have an evolution
of Kubernetes for data. We have a project, you take
a picture really quickly, data on EKS. If you can come to us afterwards, lots of different blueprints
and best practices to help you provision different patterns, like running Spark on EKS,
or running Flink on EKS, or running Kafka on EKS. So we have different types of scripts that kind of help you get started. And on the last point,
and we talk about data, there's really a life cycle. So we talked about the support that a data platform needs to have. So for example, how does the
data get into your environment, into your system ingest? And you need to support
different types of tools, Kafka-based things. You can do managed
services like Amazon MSK or run Kafka and Kubernetes. We have native options
like Kinesis, et cetera. And then we have workflow tools that kind of help stage the data, whether it be things like
Step Function, or Airflow, or Argo Workflows,
different batch schedulers to have some instances come
up and more stateful patterns, and we already talked
about data processing. Then we get into the data processing tier, and we get into things
like batch scheduling, doing kind of all the
processing you need overnight. And then you wanna get your
data out and consume it. You might do some transformations, might do some SQL against it,
different catalog the data, and make the data available as an API. So this is the lifecycle of the data. And so with that I'm gonna
now, we're gonna go deeper, I'm gonna hand it to Christina. Christina, gave you a minute back. - Thank you, Roland. Hi, everyone. Next I'm gonna cover three
things that you can do. So you can optimize your
analytics platform on Kubernetes for growth. So you're set to bring in
analytics workloads fast into AWS, as well as grow into different regions. My name is Christina Andonov,
I am a solutions architect, and I help you, our customers, building analytics
platforms on Kubernetes. And as Roland mentioned, one reason that you might
be here in this room is that you're already running
your business applications on Kubernetes. You feel comfortable using it, scaling it, and now the analytics
workloads are coming in. So when we engage with
customers like that, this is where we usually find them, having two separate teams,
building two separate platforms centered around the same compute. And the reason for that is not
that much the compute itself as it is the traffic
pattern of the workload that lends on that compute. See for business application, the traffic pattern is much
like the weather in California. For the most part, you get
highs of 80s, lows of 60s, and with minor exceptions,
it is very predictable. The traffic pattern for
the analytics workloads, well, that's more like the
weather in the Caribbean during the hurricane season. And when that flood of jobs
arrives at your clusters, that data platform team's
responsibility is to make sure that these clusters can scale
in a very short period of time so they can ingest the jobs are optimized, so they can process the jobs fast and can do all that cost efficiently. So next, let's go over some best practices of what you can do to
optimize these clusters. And when I mentioned best practices, because I'm assuming
you're already running your business applications on Kubernetes, I'll just go over the
delta of best practices between your business platform
and your data platform. We're gonna build a cluster with best practices from the ground up. We're gonna split the build
in three logical layers where layer one will be building a vanilla yet production-ready cluster
optimized for analytics. Layer two will be installing
various open-source tools to make that cluster
purpose-built cluster. And layer three will be
onboarding your tenants on that cluster. And actually before you
even create the cluster, you have to set up the networking to make sure the network will
scale and you have enough IPs, because you might have
heard that with IPv4, IP exhaustion is a thing,
and your natural inclination might be to use an overlay network. But overlay network will inevitably add a little bit of latency, which is counterproductive to
what we wanna achieve here. So instead of an overlay network, what you can do is use this special range of non-routable IPs. And in this case, in this example, two CIDR blocks that will give you 130 IPs to use for the nodes and the pods. Here is where I usually get the question, but how about IPv6? And yes, yes, IPv6 will make
all of our exhaustion problems go away eventually. Kubernetes supports IPv6
since two years ago. Spark supports IPv6 since last
year, version 3.4 or later. But chances are something,
somewhere along your stack, or that your cluster talks to, might not yet be supporting IPv6. So it's definitely an option, but if you choose to use
it, please test it truly. Either way you go about here, I'll consider you have enough IPs, and then we can proceed
with creating the cluster, and start to put the
add-ons onto the cluster. The first add-on that
goes in is the VPC CNI. And the VPC CNI comes
with this default setting, WARM_ENI_TARGET=1. What does that even mean? Well what it means is that
when instances come up, they have an ENI, and that ENI holds a set of pre-warmed IPs. One of those IPs goes to the worker node, and then a spot comes up, they get assigned IPs
from those pre-warmed IPs. Now if that instance
were to be a c5.2xlarge, that ENI will hold 15 IPs. But for analytics, we tend
to use much larger instances, such as c5.18xlarge, and that
instance comes with two ENIs, and they hold 50 IPs each. That is 100 IPs. Well for analytics, our pods
tend to be larger as well, which means you are not gonna schedule nearly as 100 pods on this instance, and all these IPs, most
of them will go wasted. And that pool of 130 we just
said, that might not be enough. So what you can do here is set
MAX_ENI to 1 for the VPC CNI. And then the second
option here, maxPods: 30. That is setting on the cubelet, you can set it to a static number like 30, or you can check our data
and it has blueprints. We have different patterns
where we set that option automatically based on
how large the instances for different flavors of Linux. Last thing for the VPC CNI, VPC
CNI tends to be very chatty, and who it likes to talk to,
its best friend, the EC2 API. See for each and every of those IPs, usually the VPC CNI makes
a call to the EC2 API. And if you're trying to
bring up one instance, that's not a problem. But if you're trying
to bring 100 instances at a time, well, guess what? You're gonna throttle the API. So what you can do here is set up this enable prefix delegation. What enable prefix delegation does, it packages 16s of those IPs in one query, and it queries the EC2 API, that will reduce the queries
to the EC2 API fold 16. All right, VPC CNI set. Next I have a riddle for you. Do you know why CoreDNS gets
invited to all the parties? 'Cause it resolves everything. Well most of the time. For analytics, usually, CoreDNS
has to scale up proactively as a lot of instances are coming up. And in the past, you had to
use this open-source project called Cluster Proportional Autoscaler in order to scale CoreDNS. And I have good news for you. In May, we announced
many scaling for CoreDNS. That means if you are
on version 1.25 of EKS and CoreDNS 1.9, and you have upgraded one time to those versions
or later since May, this feature is already available for you, and all you have to do is
just set, just enable it, and then CoreDNS will
proactively scale for you. You should also install
open-source project called NodeLocal DNS. NodeLocal DNS is a
DaemonSet that you install, it runs on every single node,
and it caches the queries, so it alleviates the
queries that go to CoreDNS. And one last thing here,
let's talk a little bit about how CoreDNS resolves external domains. If we give it a domain
like s3.amazonaws.com, it will go to its configuration, and there's this setting called ndots:5, it's gonna see ndots:5. And what that means is CoreDNS
will look at that domain, say, oh, this domain has two dots, and two is less than five. So this must be internal
to my cluster domain. So what I'm gonna do is I'm gonna go here and append every single
option in the search line, and that will resolve of
in five unnecessary queries before I go externally and
check where that domain is. One thing you can do here
is, first and foremost, please do not change
ndots of 5 to CoreDNS, but you can set ndots
to 2 on the pods only. What that will do is it's gonna
take a look at the domain, it's gonna say, "Oh, it has two dots, and I'm gonna resolve it right away." It's gonna resolve in one query. And that will alleviate the
pressure on the CoreDNS pods, and that's how CoreDNS can keep
its party celebrity status. Next, for your business applications, we're gonna scale some compute here. So for business applications, you tend to use this open-source project where a lot of our customers tend to use this open-source project
called Cluster Autoscaler. Cluster Autoscaler scales up instances based on CPU and memory. And then what it does,
how it scales instances, it usually takes on average like between two and three minutes
to bring an instance up. We see data platform teams
are the first to switch to another open-source
project called Karpenter. Karpenter came out from
AWS, we open-sourced it, and we donated it to seek
auto-scaling last year. And this year, we graduated it to GA. So now it's V1, it is stable,
you can go ahead and use it. And the difference here
with Karpenter is that one Karpenter is fully
aware of AWS pricing. So not only it's gonna pick an instance that is gonna fit the pod's
memory and CPU speaking, but it's gonna pick the most
cost efficient instance. Also, Karpenter is very
quick on how it scales those instances up much, much quicker. We usually see scaling the instance up in less than a minute. Before we dive into the
Karpenter configuration, let's talk a little bit
about how Spark jobs work. Usually when a data engineer
submits a Spark job, a driver pod comes up, and that driver pod is
responsible for managing the job. That driver pod is responsible
for scheduling the executors, and if any of the
executors get interrupted, the driver pod knows about
it, it schedules another one, and it finishes the job. If the driver pod gets interrupted, the whole job has to start over. What that means for Karpenter is that we can install the driver pod to on-demand instances,
and executors are ideal for our deeply discounted spot capacity. As it comes to Karpenter, you can actually set one NodePool, and provide both capacity
types in the NodePool, and then use either annotations
or labels, or taints, and toleration to land
each workload respectively. Okay, great. Now we set the compute to scale, next on the menu, we have storage. And when I say storage, I
mean Spark shuffle storage. See we have these instance
types with d in the name before the dot. What that d means is that that instance comes with a SSD drive
built-in into the instance. That will be the fastest
storage you get, hands down. So we recommend utilizing that, those instances and that storage. Now some instances come with one, some instances come with
more than one SSD drives. And to take full advantage
of the whole capacity, you should set these drives
to RAID0 using Karpenter. It's as simple instance
source policy RAID0. I don't know if you've used this before, but this used to be a user-data script that does some calculations. So this is awesome. Not only this is awesome, but you don't have to set the host pod for the executors anymore. Executors will have direct
access to that storage. Well you still might wanna
limit how much of that storage they'll use in the Spark
configuration, but that's about it. So for short running
jobs, this is perfect, but for long running jobs, if that instance gets interrupted, well the driver pods know about it, and it's gonna bring other executors up, but now we lost the checkpoints. So these two executors have to start over. So if you have longer running jobs, maybe a better option would
be to use EBS volumes. And in Spark configuration,
we can do that, we can make the driver pod
aware of the EBS volumes with this configuration. And that way if the
instance gets interrupted, another one comes up, the driver pod knows where the EBS volumes are, it attaches them to the new executors, and the new executors take off from where the interrupted ones left off, because they have the
checkpoint data in EBS. Last thing about storage here is that these two options
are not mutually exclusive. You can use, you can provide
both of them to your end users, and they can pick and
choose which one to use depending on the job they're running. As I mentioned, Karpenter
brings instances up very quick. The next thing that needs to happen is that instance has
to download the images and to run the pods. And good rule of thumb here is to have from the
moment I want an instance to the moment I have
my pods up and running is to have that process under a minute. So a few optimizations you can do here. First and foremost, get
those images the closest to your instances as possible. If you are storing them in ECR, make sure they're in the region with cross-regional replication. If you're storing them
in a different repo, we support pull through cache for ECR, use pull through cache,
put them in the region. Set a VPC endpoint, so those images get downloaded
through the network, and we don't have to go out and back in. And if that's not enough, and that's might not be enough
to get it under a minute, if your images are pretty
large, meaning 5 to 10 gigs, what you can do is you
can take large files out of your images such as JAR
files and other libraries. You can store them in S3 express zone 1. And then have the images lean, and when they download, use S3-Mountpoint to mount this S3 bucket as
a drive to your executors. Okay, so now we get it under a minute. And last one, in this first layer, please monitor, set up your monitoring before you put this in production. I know most of you monitor, Spark and the open-source
softwares that you're using, they come with metrics. I want you to monitor three
additional things here. The first thing is please monitor the Kubernetes control plane. We do expose the
Kubernetes control metrics. Check out the EKS best practices guide, and make sure you monitor that. Make sure you monitor the AWS APIs. We do provide in CloudWatch metrics. And as I mentioned earlier,
if you're using VPC CNI, you might throttle the EC2 API. Make sure you have access
to those throttle metrics, out CloudWatch as a data
source to your Grafana, or to your monitoring vendor, and watch those throttle metrics. Same thing for the EBS API. And last one, make sure
you monitor the network. See for example, CoreDNS
is using UDP protocol, which means if your
contract table on the Linux, the connection tracking
table gets filled up, those CoreDNS packages
are gonna get dropped. And you wanna know about it
from the CoreDNS metrics, you need to monitor the network again, EKS best practices guide
for monitoring the network, check it out. And that concludes the first layer. So now we have a cluster that can scale, will be performant enough,
and cost efficient. Next we're gonna add
various open-source software on this cluster to make
it purpose-built cluster. The most common open-source projects that we see customer use are Apache Flink and Apache Spark. Usually the question is, do I put them together
in the same cluster, or do I split them in their own cluster? And the answer to that
is, well, it depends. If you have smaller clusters, or clusters up to let's say 2-300 nodes, yeah, you can run them
on the same cluster. But if your clusters start
growing beyond that point, it will be good to start
thinking about separating them. The rest of the talk will be focused mostly on Apache Spark here. For Apache Spark, we recommend, if you're running the open-source version to use the Spark Operator. Earlier this year, the
maintainers of data on EKS worked with the community and
got the Spark Operator donated to the Kubeflow community. And since then, we have
seen tremendous bug fixes, active maintainers. So use Apache Spark. Another one here that is optional, like see the Kubernetes
scheduler, you can schedule jobs on first in, first out basis. But if you're coming from
like a Hadoop and YARN world, you might want your priority
cues or gang scheduling. And to use that, we recommend using this other open-source project
called Apache YuniKorn that works really well with Spark. And last but not least, you will need some workflow
engine for your jobs. Most common one we see is Apache Airflow. You can install it, and self-manage it, or you can use our managed offerings. We also see a fair
amount of Argo Workflows, even lately, Step Functions
to do the workflow here. And that concludes the second layer. Next, let's talk a little bit about your tenant isolation
strategy before we move on to onboarding your tenants. See, for some customers
use one team per cluster, but we see the majority
of the customers actually using multi-tenant cluster and
using namespace as a service. And if you're using a
namespace as a service, in that third layer for your users, you have to create some construct
like namespace, Spark API, even Jupyter Notebook, and
a few other things in there. So let's take a look at this in a little bit of a
different perspective, and the perspective is the following. Who creates the automation for each layer, and who consumes the
automation for each layer? So for layer one, usually
we see data platform teams. They're tool of choices,
generally Terraform. so they will create the Terraform, and they will also consume that Terraform. Layer two is very similar, they'll create a Terraform
and some other tooling, but they will install this
open-source softwares, and they'll manage them. Layer three however becomes things look a
little bit different. And what looks different is
that the data engineers usually have to go through a ticketing system, or if they have access to
use that Terraform platform, they're not very well-versed, so they have to open the tickets for help. And one of the reasons for those tickets is actually these AWS resources. 'Cause the rest, we have some tooling, you can put them on the helm chart, you can use like some other tooling. But if you have to create
AWS roles, policies, S3 buckets, RDS instances,
any resources in AWS with permissions, the data
engineers have to open a ticket, a security has to approve this ticket, somebody from the data
platform team will pick it up, copy-paste some Terraform,
create this construct, and the compliance team has to audit it. Instead of using this process, you can optimize it by
providing your users with a proper interface, which
we think is should be an API, and you can use the
Kubernetes API to do so. And you can install this
open-source project on Kubernetes to extend its API in order
to do, create IM policies, S3 buckets, and so on. So ACK is an open-source project that AWS open-source a while back. Recently, this year, the EKS
service team took ownership of all of the controllers for ACK. And currently, we're at 50 GA controllers. That means we have coverage
for 50 AWS services. And what GA means for you is that this is covered by your
enterprise support. So using ACK, you can
extend the Kubernetes API, and you can create APIs
to onboard your tenants, and you can do the full
namespace vending for them. If you're using one team per cluster, we see that those customers
are trying to expand this to the whole three layers. But in general, this is where the majority of the customers are, and they're really happy with this. You know, in the beginning when I started, and I said, here's where
we find the customers having two separate teams,
supporting two separate platform, centers around the same compute. And if you are an executive
in the room, you are probably, well, is this a good thing? Is it a bad thing? What should I do about it? Should I put them together? Should I, what do I do? I'll tell you what to do. So you don't wanna limit
the organizational growth, you wanna be able to, the number
of platform teams you have is somewhat irrelevant. What you want to do is you
want to standardize on tools and practices so you
can foster that growth as it comes to your organization. A word on Terraform here. So when you start splitting
your platform teams, let's say you end up
with 10 platform teams and you standardize on Terraform, you're gonna end up with 10
different code bases guaranteed. So you wanna standardize
not only on the tool, but how the tool is used. Again, as I mentioned, we wanna
utilize the Kubernetes API as one of those toolings, so we can provide APIs to our engineers. So the three things that you should do in order to set your
analytics platform to scale are use the best practices
that we just talked about, so you can optimize, optimize,
optimize those clusters for scalability, performance, and cost. Foster organizational growth, not by limiting the number
of platform teams you have, but by standardizing on
tools, best practices, and processes in your organization. And delight your end
users with the interface that they can use to
onboard onto the platform. Now all these are great in theory, and it would be just theory
if it wasn't for you, our customers, to put them in practice. So I'm delighted to say we have our customer here, AppsFlyer, and Victor will tell us how
they not only put all of these into practice and a lot more,
but they did it at scale. - Thank you. Thank you, Christina. Super excited to be here. Hello, everybody. Roland spoke about different
options to run Spark workloads. What if I can tell you
that only one decision can change your data organization. It can boost your performance,
enrich your observability, save costs, and empower your developers. Sounds fictional, right? Well I'm here today to tell you about the decision we made one year ago. My name is Victor, I'm
working for AppsFlyer for the past six years, I'm
leading the data platform group responsible for the real-time
and analytics data platforms. I have vast experience in AWS
ecosystem and data processing, and one of the biggest
benefits at AppsFlyer is that I can do it at scale. Did you know there are more
than 7 billion smartphones? On average, there are
more than 60 app installed on each phone. If you will show me your
phone and application, I will definitely recognize an app which has AppsFlyer SDK on it. AppsFlyer is helping app owners to optimize their advertising campaign and find the right audience for that app. We expose digital analytics
and handle a massive amount of data each day to make it happen. Now let's jump right into
the analytics workloads, and see how we run it at AppsFlyer. First, let's talk a bit about Spark. Spark is an open-source analytic engine for large scale data processing. You will find it almost
in every organization that handles data at scale. Let's review some of
its key characteristics. Spark is usually being
used as batch processing for consuming and
producing downstream data. Depending on the job, it will
require different CPU, memory, and IO performance. Processing time is very important, and the right compute
and scaling strategies are a key factor. And Spark jobs are usually
stateless in nature. However, if interrupted, they will require data reprocessing, which might affect your SLA. Now let's talk about the challenges at AppsFlyer with Spark jobs, we process more than 100 petabyte daily. We run thousands of very different
jobs at any given moment. Our compute is widely distributed
from Intel to Graviton, CPU, memory, and
storage-optimized instances. Basically you name it, we have it. We're very dynamic according
to our data trends. We process millions of events per second, and our traffic can scale
up and down very rapidly, and we must be very efficient
with our scaling strategies. And finally, we're very
strict with our SLA. As a data company, we must make sure we process our data in time, otherwise it will affect our business. So we decided that the
best course of action in handling those challenges
is moving our workloads from our original EC2 instances managed by configuration management and internal tools to EKS. Why EKS? Let's deep dive. There are several areas
where EKS ecosystem excels when it comes to analytics processing. I'm going to mention three
and show you the value. We're going to talk about
Karpenter for scaling and compute optimizations. Observability, which can be enriched and give you valuable
insights about your data. And enablement, how we
empower our data engineers. Remember this slide, it
perfectly shows the ingress, consume, and downstream. Let's focus on this area, the data lake. And let me show you as an example how our main data processing
compaction runs on EKS. Compacted data is the result of merging small fragmented data files into larger, more organized files. To optimize storage
efficiency and performance, this is usually the first
entry point for your data lake. We run it hourly, have around 150 jobs with different runtimes, and we process around
60 tera each iteration. I will start with Karpenter, which help us tackle our challenges and achieve our goals
for compute, scaling, and cost performance. Take a look how our old
EC2 base spark cluster looked like before we
switched to EKS and Karpenter. It was managed by configuration
management tools and EnOS. The yellow line represents utilization, and the purple represents
the cluster size. You can see that the scaling
mechanism is not effective. When our utilization drops, the node scale down cannot keep up. Karpenter solves it. it picks the right compute
per our configuration and scales up and down efficiently according to our utilization. In each iteration we scale
up approximately to 60 node, and back down when we end the processing. If we look at the utilization of the node, we will see that we peak at around 80%, which is our sweet spot and
stay stable in old capacity. We start to scale down only
when utilization drops. Data processing is very expensive
in term of compute power. By doing this, we minimize our idle time, and eliminate resource
waste and unnecessary costs. In a 24-hour cycle, we're
doing it more than 1,600 times, both for creation and termination. And remember this is only
our compaction workload. We have hundreds of other
different processing workloads, so we can only imagine how our
termination and creation node looks like on a daily basis. Let's look at the instant spread. Karpenter's strategy is
speaking the most stable and cost effective node
at any given moment. We use Graviton Gen 3
and fall back to Gen 2 when there is no capacity. We use spot instances only. And take a look at the bare metal node. Full control of the machine resources, direct hardware access,
no hypervisor overhead, very high performance. It is 15% of our NodePool. When was the last time anyone manually picked bare metal nodes for stateless or dynamic applications like Spark? It also gives us the option
to use new gen instances, and we can provision them
as soon as they get out. We also distribute a node across multiple availability zones. Karpenter's make sure that we provision on the most cost effective zone. Zone A was the winner that day. It had 20% more nodes compared to zone B. We also ensure that our job pods run within the same availability zone, eliminating cross AZ data
transfer and its associated costs. But not all was well that day. We had many spot interruption
during those 24 hours. In some cases, dozens in one hour, which should be devastating
to our SLA, right? However, due to the effective
termination handling by Karpenter, we can hook spot
termination signal to Spark, which gives us the option to
migrate the intermediate stage of data within two minutes. This approach eliminates the need for reprocessing in the
event of node failure. It makes spot termination nearly
seamless to our workloads, keeping us well within our SLA, and the longest job during those 24 hours was only 33 minutes. I've shown you some nice results. Now let's see how you can
achieve near the same performance with a few simple configuration changes. For scaling, we use Amazon Linux 2024, optimized for fast boot
time and kernel performance. This allows us to provision
nodes in under 10 seconds. For decommission, we adjust
our node distribution budget according to our processing trends. We set times when we can
decommission more aggressively to reduce the idle time, and less aggressive when we
are usually at peak utilization to maintain high performance. This configuration is being used for Spark to be aware of no termination
and enable Spark shuffling. And to ensure we can
migrate within two minutes, we utilize local storage
for optimal performance. To enhance resiliency and cost efficiency, we configure each Spark job to run within the same availability zone. Again, eliminating cross AZ traffic and its associated costs. And this section defines the
use of Graviton instances with local storage for
optimal performance. Now, I must say that we
try placing specific nodes, but no matter how many variations we try, Karpenter automatic selection
consistently outperform in terms of cost efficiency. We're done with tuning. Now let's talk about what we
can do with observability. We saw how Karpenter helps us
manage and scale our compute. Now let's see how combining
the metrics from Karpenter, Kubernetes, and Spark helps
us get some valuable insights from our platform. How many of you know
what is the percentage of each data processing flow? In compaction, we know exactly what is the ratio of every processing, and when it starts and ends
in each processing cycle. Here you can see that
clicks take around 28% of the workload, and start
at the middle of each cycle. This allows us to see the distribution and weight of each dataset
at any given moment. Moreover, we can see daily,
weekly, and monthly trends, and understand our system
and business implications. Look at this metric. We can see that click
processing was reduced by 8% compared to last week. And a different dataset
was increased by 35, which might be a bit concerning, right? By examining our Datadog query, we can see that this information
can be easily obtained by combining metrics from
Kubernetes and Spark. But what happens when
we add Karpenter metrics for price estimation? Karpenter sends us cost
per instance and AZ, and by adding to Spark and Kubernetes, we can calculate how much
each data processing cost. So we get the price for
processing our data per minute in near real-time. This is huge, both for
engineering and business purposes. You can tell how your
co-deployment affects the cost, and take decision according to the trends. You can even set a better pricing for the service you
provide for your customers. You can even take it one step further by integrating this
into your data lineage. Allowing you to determine the total cost of your entire data processing pipeline, you can decide what can be
optimized and what is redundant. Combining your observability metrics gives you a comprehensive
view of your data flows, and allows you to take
data-driven decisions about your data. Well, all those metrics and
performance I showed you worth nothing if only platform engineers know how to use it and tune it. And I showed you only
a few of the options. We encourage and give our
data engineers full autonomy and control over their
applications and platforms. We of course provide best
practice defaults and policy, but they can change in overall
depending on their needs. Everything is being done
through our source code. We created a detailed Git structure, which defines the action of the repo. Deployment units, both for
infrastructure application and third-party integrations. And environments, which
segregates between development, staging, and production. Each has its own Git or flow. This approach allows us to
manage the infrastructure of Kubernetes and application components from a single interface
enabling automated workflows and validation for
executions and deployment. It protects us from configuration drift, ensure we maintain the current state, and allows us to spin up
and modify application and the infrastructure within minutes. And finally, this removes the dependencies of our platform engineers. It raises the velocity and
autonomy of the data engineers, and contributes to the knowledge of both. So at the beginning, I presented that one decision
change our data organization. As you notice, this decision
was moving Spark workloads from EC2 to EKS. Now let me show you the value. This is a young version of me. Compared to our old EC2
Intel by Spark clusters, by using EKS, Karpenter, and Graviton, we reduced our cost by 60%. We improved our SLA by 35%, and significantly enriched
our observability. And the cherry on top, we
reduced the operational overhead of our platform engineers, and raise their overall
happiness along the way. Thank you. (audience applauding) - All right. So excellent presentation, thank you both. If you take anything away, right? Optimize and monitor EKS for
analytics and best practices. Align tools and practices to
foster organizational growth. So we saw the layers that
Christina talked about, so you can be happy like Victor. And provide APIs. Remember, developers, data engineers, data scientists are your
customers of the platform. If they can't build, if you
can't provide them APIs, all you're gonna have is
a platform not being used. So enable APIs to empower data engineers to work independently. Autonomy, while you're still providing the automations for control. We have a happy union together. So thank you. Thank you, everyone, for joining. Don't forget for session
resources like data on EKS, you can click and scan the QR code there. Thank you for coming, everyone, and enjoy the rest of the conference. (audience applauding)