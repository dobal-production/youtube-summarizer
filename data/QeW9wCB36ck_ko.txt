- 안녕하세요.어서 오세요, 여러분.제 세션을 선택해 주셔서 정말 감사합니다.와줘서 정말 고마워요.물론 당신 여기 있잖아요
안정적인 확장성을 위해 amazon.com이 AWS에서 실행되는 방식과 AWS의 클라우드에서 확장하는 방법에 대해 자세히 설명하겠습니다.
규모가 크고 정교한 고객인 amazon.com이 어떻게 AWS를 사용하는지 보여주는 예시.제 이름은 세스 엘리엇입니다.저는 현재 개발자 옹호자이자 수석 개발자 옹호자입니다.
개발자 관계에 있어서는 제가 최근에 바꾼 것이 바로 이것입니다.그 전에는 AWS Well-Architected와 Well-Architected의 안정성 책임자로 근무했습니다.
오늘 강연에서 큰 역할을 하겠지만, 그 전에도 사실 저는 amazon.com에서 일했습니다.그래서 저는 2005년에 아마존에 입사해서 아마존닷컴에서 일하고 있었습니다.
AWS로 옮기기 전에는 그랬죠.그래서 저는 항상 약간의 역사 수업으로 시작하는 것을 좋아합니다.저는 1995년에 그곳에 없었지만, 이게 바로
웹사이트는 1995년에 만들어졌습니다.그 모든 영광을 만끽하세요.정말 놀랍습니다.
사실 이 기간 동안은 방금 보신 웹사이트를 운영하는데 사용된 아키텍처입니다.그래서 저는 오비도스라고 쓰여진 상자에 여러분의 관심을 집중시키고 싶습니다.오비도스는 브라질의 아마존 강이 있는 곳입니다.
가장 좁고 가장 빠른 곳이죠. 그 당시에는
그들은 브라질에 있는 장소의 이름을 따서 많은 것들에 이름을 붙였는데
아마존 강에 있는 물건들, 그게 실행 가능한데요.C++가 아니라 단일 서버에서 실행되는 단일 C 바이너리입니다. amazon.com 서적의 경우 ACB라는 다른 서버에서 실행되는 단일 Oracle 데이터베이스와 통신합니다.
그 안에 모든 데이터가 들어 있었고, 그게 바로 아키텍처였죠.CC 모텔이 있는 걸 볼 수 있었죠.
그게 신용카드 시스템이에요.그건 별개였기 때문에
액세스 권한을 제한할 수 있었으므로 크레딧을 받을 수 있습니다.
카드 번호가 안전할 수도 있고, 물류 센터가 있는데 나중에 주문 처리 센터로 이름을 바꿨습니다. 이 곳에서 패키지를 보관할 수 있습니다.
배송되어 보여드립니다.이것이 원래 아키텍처입니다.자, 아마존의 모토는
특히 그 당시에는 빠르게 커졌어요.
보니까 피크닉 중에 나온 티셔츠가 있잖아
빨리 커지세요, 그리고 빨리 커지려면
확장성이 필요하겠죠.그럼 확장성이란 무엇일까요?음, 확장성은
합의된 작업을 수행할 수 있는 워크로드의 능력
범위가 변경되거나 부하 또는 범위가 변경됨에 따라 작동합니다.그래서 그들은 거기에 도달하기 위해
아키텍처를 발전시키기 위해서요.첫 번째로, 그들은
데이터베이스를 살펴봤어요.철수된 걸 볼 수 있었죠.
저기 이 웹 데이터베이스를 꺼내세요.그래서 그 웹 데이터베이스는
고객과 상호 작용하고, 주문을 처리하고,
그런 다음 주기적으로 ACB 데이터베이스에 비동기적으로 다시 동기화됩니다.마찬가지로, 다음을 추가했습니다.
새로운 유통 센터가 생겼고 각 센터에도 자체 데이터베이스가 생겼습니다.따라서 이 방법도 한 가지 방법으로 제거할 수 있습니다.
가장 큰 병목 중 하나는 데이터베이스였습니다.
하지만 그것만으로는 충분하지 않았어요.그럼 2000년으로 빠르게 넘어가서 한 가지에 대해 얘기해 보죠.
서비스 지향 아키텍처.단일 바이너리를 가지고 있다가 결국에는
C++는 원작과 마찬가지로 최초의 엔지니어이기도 합니다.
아마존은 C로 유지하자고 주장했지만 시간이 지나도 C++를 제어할 수 없게 되어 결국 C++ 라이브러리가 들어왔지만 여전히 단일 바이너리였습니다.변경을 원하신다면, 변경에 참여했다고 가정해 봅시다.
원클릭 주문, 원클릭 구매를 구현하려면 다음을 수행해야 합니다.
해당 바이너리를 변경하면 다른 모든 사람들이 변경 내용을 적용하고 있습니다.
해당 바이너리를 변경하고 빌드 중입니다.
다른 모든 분들과 함께 여러분도 배포하고 있습니다.
다른 모든 사람들과 마찬가지로 이 시스템은 그다지 민첩하지 않습니다.다른 누군가가 빌드를 망가뜨린다면 여러분은 배포할 수 없습니다.
지금은 별로 좋지 않아요. 그럼 뭘 할 수 있을까요?글쎄요, 게다가
데이터베이스를 나누면 고객을 볼 수 있습니다.
ACB에서 데이터가 빠져나갔는데 그렇게 되고 싶지 않을 거예요
데이터베이스를 직접 호출하기 때문에 다음을 입력할 수 있습니다.
그 앞에 있는 서비스, 고객 서비스,
원래는 해당 고객 서비스를 선택하여 해당 데이터베이스에 삽입하는 용도로만 제공되었는데, 이제 이곳이 데이터베이스를 위한 장소가 되었습니다.
고객에 대한 비즈니스 로직마찬가지로 주문이 들어왔습니다.
서비스와 아이템 서비스, 그리고 이번이 처음입니다.
Amazon의 서비스 지향 아키텍처.이제 빨리 크게 성장하세요.이제 시작해 볼까요?
현재로 빠르게 넘어가겠습니다.이전 프라임 데이,
아마존은 크게 성장했어요.아마존이 크다는 건 우리 모두 알고 있죠.
분당 10만 개의 아이템, 120억 개의 매출을 올렸습니다.
지난 프라임 데이에 대해서요. 하지만 여러분은 그것에 대해 배우러 온 것이 아닙니다.자세한 내용을 알아보러 오셨습니다.
AWS를 어떻게 사용하고 있죠?그래서 저는 읽지 않겠습니다.
여기서 숫자를 빼면 되죠.분명히 수십억 달러가 있어요
그리고 수조, 수백만.어서 읽어보세요.아마존이 빠르게 성장했고 이를 이용해 사업을 진행하고 있다는 것을 보여주는 것일 뿐입니다.
AWS는 허용하고 있으며 AWS를 사용하여 안정적으로 확장하고 확장하고 있습니다.따라서 오늘날의 아키텍처가 어떤 모습인지 빠르게 알고 싶다면 상당히 비슷해 보입니다.
2000년으로 거슬러 올라가죠.제 말 믿으시는 분 계세요?아니요.주의를 기울이고 있는지 확인해 보세요.아니요.자, 이제 실제 현재 아키텍처에 더 가깝습니다.저기 있는 각 점은 다음을 나타냅니다.
서비스 또는 마이크로서비스 또는 수만 개
amazon.com을 운영하고 있으며 모두 다양한 종속성을 통해 서로 연결되어 있습니다.다음 중 하나를 확대했습니다.
실제로 존재한다는 것을 보여드리기 위해 여기에 있는 것들입니다.
그 도표에 있는 선들.제 생각에 다이어그램은
정말 아름답죠?하지만 이것이 바로 수만 개의 서비스와 수천 개의 서비스를 갖춘 현재의 아키텍처입니다.
해당 서비스를 소유한 팀.좋아요, 그럼 이제
안정적인 확장성을 갖추게 되었습니다.따라서 신뢰성은
워크로드가 필요한 기능을 수행할 수 있는 능력
정확하고 일관되게.지금 생각해보면 아마존이 확장성을 필요로 했던 이유가 바로 그것입니다.규모가 커져야 했습니다.
빠르고 안정적이기 때문에 확장성이 필요했습니다. 그리고 오늘 우리는
amazon.com 팀의 사례를 자세히 살펴보겠습니다.
그리고 AWS를 기반으로 구축하고, 우리는 다음을 사용할 것입니다.
잘 설계된 프레임워크는 이를 제시하기 위한 프레임워크입니다.따라서 잘 설계된 것은
프레임워크는 여섯 개의 기둥으로 이루어져 있고 모두 중요하지만 솔직히 오늘날 우리는 신뢰성에 초점을 맞추고 있습니다.신뢰성의 핵심 요소에는 Well-Architected의 모범 사례가 있습니다.Well-Architected는 모든 모범 사례를 기록한 문서일 뿐입니다.
클라우드에서 빌드하는 데 적합합니다.여기에는 다른 것들도 포함됩니다.
그것도 마찬가지죠.실습이 가능한 실험실도 있습니다.자체 워크로드를 검토할 수 있는 Well-Architected 도구가 있습니다. 하지만 솔직히 이 경우에는 가장 좋은 방법을 살펴보도록 하겠습니다.
사례: 안정성의 기둥.그 중 66개가 있습니다.우린 보지 않을게요
모두 66개지만, 오늘 보여드린 것처럼
제가 보여드릴 예시를 말씀드리자면, 어떤 모범 사례가 아래에 설명되어 있는지 말씀드리겠습니다.
현재 살펴보고 있는 아키텍처에 대해 자세히 알아보겠습니다.
첫 번째 예를 들어볼게요.아, IMDb는 다음과 같이 재구성되었습니다.
서버리스 마이크로서비스.그래서 IMDb는 인터넷 영화 데이터베이스입니다.IMDb에 대해 들어 보신 분 계신가요?좋아요, 그리고 나머지 분들은
손 들고 싶지 않으니까 손 들고 싶지 않아
손 들어보세요.(웃음) 인터넷 영화 데이터베이스는
1998년 아마존에 인수되었습니다.최고의 위치입니다.
영화, 드라마, 배우, 프로듀서에 대해 알아보러 가려면
이 모든 것들이 좋았고, 재설계하기 전에는 획일적인 작업을 하고 있었습니다.
수백 개의 EC2 서버에 REST API를 사용하여 빌드하세요.따라서 AWS에 있지만 수백 개의 EC2 인스턴스, 서버에서 실행되고 있습니다. 아키텍처를 변경했을 때 페더레이션된 서버로 이전했습니다.
마이크로서비스를 사용한 스키마.이제 마이크로서비스는
특정 비즈니스 도메인에 초점을 맞춘 소규모 분리형 서비스.페더레이션된 스키마가 무엇인지 모르신다면
이미 말씀드리자면, 이 문제에 대해서는 Lambda를 사용했습니다.그래서 그들은 서버 없이 코드를 실행할 수 있는 AWS의 서버리스 컴퓨팅인 Lambda를 사용하고 있습니다. 이제 모범 사례를 살펴보겠습니다. 강연 내내 이 슬라이드를 몇 개 보게 되실 겁니다.그들은 가지고 있습니다.
잘 디자인된 로고가 있는데 형식이 좀 이상할 수도 있어요.이게 뭐냐면, 스냅샷이에요.
AWS 콘솔에 있는 Well-Architected 도구와 모범 사례
프레임워크에 표시된 것처럼 모범 사례 집합을 나타내는 질문이 있습니다.그런 다음 각 항목을 검사합니다.
확인란은 모범 사례입니다.그래서 이 경우에는 최고입니다.
저희가 관심을 갖고 있는 사례는 워크로드를 어떻게 세분화하고, 그 세그먼트를 특정 비즈니스 사용 사례에 어떻게 집중시키는가입니다.
특정 비즈니스 요구 사항에 대해?제가 동그라미를 친 게 보이시죠?
마이크로서비스가 저기 있어요.이러한 모범 사례를 달성하기 위해 마이크로서비스를 사용할 필요는 없습니다. 하지만 IMDb가 해낸 것이 바로 이것입니다.
따라서 동그라미로 표시됩니다.이 두 개가 처음 두 개입니다.
한 번 살펴보아야 할 모범 사례를 살펴보자면
질문을 하나 할게요.IMDb에서 Jackie Chan을 입력하면 검색어가 실행됩니다. 상위 4개 프로그램은 무엇인가요?
성룡이 유명하다고?자, 재키는 신분증을 가지고 있어요. 모든
IMDb의 엔티티에는 id가 있습니다. 이 nm은 이름 엔티티이고
저건 그의 엔티티인 329이고, 사용자인 당신도
신경 쓰지 마세요. 하지만 방금 뭘 물었냐고 하셨잖아요
성룡이 유명하냐고요? 그리고 이게 바로 그 질문인데요
클라이언트가 만드는 것이죠.GraphQL입니다.GraphQL은 쿼리를 설정할 수 있는 쿼리 언어입니다.
이와 같이 스키마를 사용하여 정보를 요청하고 모든 정보를 한 번에 다시 가져올 수 있습니다.REST와 마찬가지로 아마도 여러분도 그럴 것입니다.
네 번 전화를 걸어야 해요.여기서는 한꺼번에 처리하면 되는데 여기서 요청한 내용은 무엇인가요?에서 이름 ID를 볼 수 있습니다.
맨 위, 저 성룡이에요. 처음 네 명도 알고 싶어요
그가 유명했던 것들, 그리고 그 중에서도 당신이
네 가지만 내놔, 제목 텍스트를 알고 싶은데
출시일을 알고 싶고, 집계된 평점도 알고 싶고, 이미지 URL도 알고 싶어요
이미지를 보여드릴 수 있도록 말이죠.자, 그럼 질의는 이겁니다.
프런트엔드가 만들고 있고, 여기서 마이크로서비스와 페더레이션된 스키마가 작동합니다.이 요청은 실제로 네 개의 서로 다른 마이크로서비스로 전송되며, 각 마이크로서비스에는 AWS가 있습니다.
이 경우에는 Lambda가 있습니다.첫 번째는 '찾기'입니다.
저는 성룡이 가장 잘 아는 네 가지 물건이고
이 네 가지 항목의 ID를 반환할 거예요.
tt로 시작하죠.자, 첫 번째 서비스는 출시일이나 등급에 대해 알지 못합니다.상위 4개에 대해서만 알고 있습니다.다음은 제목입니다.
텍스트와 출시일.그게 메타데이터예요.
다른 서비스로 이동하면 해당 서비스는 메타데이터만 알고 있으므로 메타데이터를 반환합니다.세 번째는 평점이므로 등급만 알 수 있습니다.집계 평점을 반환할 것이고, 마지막 것은 이미지 URL만 알고 있기 때문에 이미지 URL을 반환할 것입니다. 페더레이션된 스키마인 이유는
요청은 하나의 큰 스키마인데, 각 스키마는 작은 스키마입니다.
마이크로서비스는 스키마의 고유한 부분만 알고 있습니다.따라서 프론트엔드가 등장하면
해당 요청에 대한 응답은 사용자에게 다음과 같이 표시됩니다.성룡과 그가 잘 아는 네 가지를 볼 수 있었죠.출시일을 볼 수 있을 거예요.집계된 평점을 볼 수 있는데 여기서 한 가지 잘못된 점이 있습니다.“쿵푸팬더”는
실종됐어요.어떻게 그럴 수 있죠?몰라요. 정말
IMDb 팀과 함께 골라야 할 게 있어요.제가 알려드릴게요
얘기 나눈 다음에.좋아요, 그럼 이제
건축에 대해 알아봅시다.좋아, 그럼 이게 뭐야
IMDb 아키텍처는 다음과 같습니다.게이트웨이 기반 아키텍처입니다.그래서 그들은 게이트웨이를 서버리스 아키텍처로 재설계하여 모든 것을 호출할 수 있도록 했습니다.
각각 고유한 기능을 알고 있는 이러한 백엔드 마이크로서비스
코끼리의 작은 조각.여기 백엔드 마이크로서비스가 있습니다.그냥 앉아만 있을 뿐이죠.
거기 앞에 람다가 있어요.일부는 완전히 서버리스입니다.최신 버전 중 상당수가 그렇습니다.그 중 일부는, 만약 있다면
마치 레거시 서비스 같은 것이었는데
업데이트를 원했는데 앞에 Lambda를 붙일 것입니다.
호출할 수 있도록 말이죠. 그리고 람다의
GraphQL 쿼리가 가능하도록 데이터 쉐이핑을 담당합니다.
응답이 올바른 형식으로 되어 있습니다.좋아, 이쪽으로, 알았어, 그럼 이제
각 마이크로서비스는 자사의 일부만 알고 있습니다.
스키마와 게이트웨이, 즉 게이트웨이는
클라이언트가 호출하는 프런트 엔드에서 게이트웨이는 다음을 수행해야 합니다.
전체 스키마를 알아야 합니다.스키마 매니저가 필요하니 바로 여기 있습니다.새로 만들 때
서비스를 제공하거나 서비스를 업데이트하면 그 내용이 조금씩 게시됩니다.
스키마의 일부를 스키마 관리자에게 보냅니다.
이를 S3 버킷에 게시합니다.따라서 게이트웨이에는
스키마의 전체 모습, 그리고 여기에 필요한 API가 있습니다.
API의 프론트엔드입니다.애플리케이션 로드가 있습니다.
밸런서.방화벽이 있어요.콘텐츠 전송 네트워크 관련 부분이 있는데 이에 대해서는 나중에 더 이야기할 예정이니 잠시 보류하겠습니다. 이때 조금 다른 이야기를 나눠서 아마존의 문화에 대해 이야기해 보겠습니다.이미 많은 분들이 계실 것 같은데요.
피자 두 판이라는 팀 얘기 들었어요피자 두 판 팀은
피자를 두 개 정도 먹을 수 있는 팀이죠. 너무 크지도, 너무 작지도 않죠.다양한 기능을 갖춘 팀이고,
하지만 모든 것은 소유권에 관한 것이죠.두 피자 팀은 디자인부터 디자인까지 자신이 담당하는 서비스를 소유합니다.
구현, 배포, 운영, 그리고 이를 둘러싼 비즈니스에 이르기까지따라서 다음과 같은 문제가 발생할 수 있습니다.
팀의 제품 매니저, 즉 비즈니스 전문가
거기서 개발자들과 함께 일하고 있어요.좋은 점은
이 모델은 이 창작 모델입니다.
이러한 페더레이션된 마이크로서비스가 비즈니스를 움직였을까요?
해당 서비스의 로직은 팀이 할 수 있도록 하기 위한 것입니다.
비즈니스 로직을 소유하세요.그러니까 소유한 팀이
메타데이터는 메타데이터에 관한 전문가입니다.를 소유한 팀
평점은 평점에 관한 전문가인데, 이는 조직적으로
이 단체에 긍정적인 점은, 그리고 실제로 일어난 일이 있었다는 것입니다. 그래서 온콜 (on-call) 이라는 것이 생겼습니다.로테이션 온콜 로테이션이 있는데, 만약 있다면
생산에 문제가 생겼고, 자체 생산 현장에 있고
그들은 이에 대응해야 합니다. 선임 개발자가 말했죠.
그 후로 온콜이 말도 안 되게 순조롭게 진행됐는데 그 이유는
기술 변화에 따른 조직 변경으로 인해 팀은
비즈니스 도메인을 소유했고 서비스를 이용할 수 있었습니다.
문제가 발생하여 문제가 발생할 때마다
집계 서비스, 즉 평점 집계 서비스에서는 그 팀이 부름을 받아 무슨 일이 벌어지고 있는지 이해할 수 있었고, 그것도 도움이 되었습니다.
서버리스로 전환한 것이 확장성에 도움이 되었습니다.좋아요, 그럼 차선책입니다.
우리가 살펴볼 방법은 다음과 같은 경우에 자동화를 사용하는 것입니다.
리소스 확보 또는 확장, 리소스 확보
리소스가 필요한지 감지하여 새 리소스가 필요한지 감지하여 자동으로 확보합니다.그러려면 제가 할게요.
조금 다른 이야기를 해보죠. 람다에 대해 얘기해 주세요.이것은 IMDb 아키텍처가 아닙니다.이것은 일반적인 예일 뿐입니다.
서버리스 아키텍처예요. Lambda에 대해 얘기하고 싶거든요.말씀드렸듯이 람다는 한 가지 방법입니다.
서버 없이 코드를 실행할 수 있지만 작동 방식은 여러분입니다.
일부 코드와 함께 Lambda 인스턴스를 배포한 다음
요청을 받을 때마다 실행되어 Lambda 인스턴스를 호출합니다.여기서 6개의 요청을 볼 수 있습니다.
6개의 람다가 가동됩니다.요청을 처리하고
더 이상 요청이 없으면 스핀다운됩니다.그래서 자동으로 스케일링이 되죠.수신한 요청 수에 따라 규모가 커지거나 줄어들게 되는데, 이것이 실제 지표입니다.
Lambda 호출의 경우이것이 숫자입니다.
IMDb가 분당 호출하는 람다의 수와 이것은
분당 요청 수로 변환할 수도 있습니다.
왜냐하면 각 요청, 각 Lambda 호출
단일 요청을 나타냅니다.최대 80만 건이라는 점을 참고하세요.
분당 요청 수, 이 값도 전환했습니다.
알고 싶으면 초당 요청 수로 환산하면 꽤 오르락 내리락할 수 있습니다.상당히 주기적입니다.몰라요, 누가 제 걸 봤는지 모르겠어요.
트위터에 이런 내용이 올라왔나요?이게 제가 하는 것 중 하나예요.
트위터에 글을 올리면서 “이게 무슨 서비스야?” 라고 물었습니다.음, IMDb입니다.이제 당신은
알다시피, 여기 두 가지가 있습니다.하나는 Lambda가 그냥 확장된다는 거죠?예를 들어, 모든 요청이
알겠네요, 람다를 작동시키죠.오토 스케일링이긴 하지만 이게 끝이 아닙니다.Lambda의 특징은
특정 런타임이 있으면 새 제품을 가동할 때
Lambda는 시간이 좀 걸릴 수 있습니다. 약간의 지연 시간이 걸린다는 거죠.이를 콜드 스타트라고 합니다.IMDb는 콜드 스타트를 원하지 않았기 때문에 다음과 같은 것을 사용했습니다.
프로비저닝된 동시성.프로비전된 동시성을 사용하면 다음과 같은 수를 지정할 수 있습니다.
따뜻하게 유지하고 싶은 람다와 따뜻한 람다는
콜드 스타트는 필요 없습니다. 비용을 지불하면 되죠.
하지만 비용의 일부분만 지불하면 됩니다.
실제로 Lambda를 실행해 보세요.따라서 a를 지정하면
80만 같은 고정된 숫자라면 정말 아까울 거예요, 그렇죠?왜냐하면 항상 80만 달러를 운영하는 건 아니니까요.여기 보이는 것은 회색 선입니다. 이것은 IMDb가 아닙니다. 이것은 회로도이지만 회색 선은
여러 Lambda 호출과 오렌지색 단계별 줄은 프로비저닝된 동시성이 확장되었다가 축소되는 것을 나타냅니다.따라서 개수 뿐만이 아닙니다.
Lambda는 확장 및 축소되지만 프로비저닝은
동시성은 확장 및 축소되므로 다음 모범 사례로 넘어갈 수 있습니다.이제 가용성이 높은 퍼블릭 엔드포인트를 사용하는 방법에 대해 알아보겠습니다.이것이 바로 그 부분입니다.
끝으로, 제가 말씀드린 실제 API 엔드포인트입니다. 그래서 여기서 자세히 설명하겠습니다.그럼 확대해서 보죠.
저 게이트웨이의 프런트 엔드에서 몇 가지를 볼 수 있습니다.좋아요, 그들은 웹을 사용하고 있어요.
애플리케이션 방화벽, 즉 WAF좋아요, 그럼 WAF는
AWS에서 제공하는 방화벽 제품인데 정말 마음에 들었습니다.이니셜이 그랬다고 하더군요.
전원을 켜는 것은 매우 간단했고, 바로 사용할 수 있었습니다.
구현하고, 제거했는데, 더 이상 고사양 문제는 없다고 하더군요.그냥 크게 줄었다고 말씀드리죠.
사용량이 많은 문제였기 때문에 수동 네트워크 차단을 따로 설정할 필요가 없었죠.그렇다면 이러한 고사양 문제의 원인은 무엇일까요?로봇, 둘 다 악성이야
또는 악의가 없는 로봇.계속 싸우고 있잖아
로봇에 맞서 싸웠고, 그래서 WAF가 해결책이었죠.
그들에게는 정말 효과가 있었죠.여기에 CDN도 있어요.
CloudFront라고 하는 콘텐츠 전송 네트워크와
CloudFront가 하는 일은 AWS가 30개 리전에 있지만 410개가 넘는 엣지 로케이션을 보유하고 있다는 것을 알고 계실 수도 있습니다.따라서 사용자는 CloudFront를 사용할 때
IMDb를 사용하는 사람에게 요청을 보내면 요청이 라우팅됩니다.
한 지역보다 더 가까운 엣지 로케이션 중 한 곳으로이렇게 하면 바로 근처에 도착할 수 있습니다.
AWS 백본은 즉시 성능이 향상될 뿐만 아니라 콘텐츠 전송 네트워크이기도 하며 캐싱을 제공하므로
해당 엣지 로케이션에서의 캐싱, 그러니까 그게 가능하다면
캐시에서, 그리고 마지막으로 ALB를 통해
애플리케이션 로드 밸런서.이것이 실제 프런트 엔드입니다.
게이트웨이를 실행하는 Lambda에 연결되어 있습니다.좋아요, 이것이 첫 번째 예였습니다.재밌게 보셨길 바라요.
그리고 몇 개 더 있어요.그럼 글로벌 옵스 로보틱스에 대해 알아보고 셀 기반 아키텍처로 워크로드를 보호하는 방법에 대해 알아보겠습니다.자, 이제 무엇을 이해해야 하는지 알아봅시다
글로벌 옵스 로보틱스는 아마존의 공급망에 대해 조금은 이해해야 합니다.사용자 입장에서 볼 수 있는 주문 레이어가 있는데, 제가 무언가를 보고 주문하면 문 앞에 나타나는데 그 아래에는 공급망 레이어가 있습니다.창고 관리 부분이 있는데, 이 부분이 잘 진행되고 있습니다.
창고나 주문 처리 센터 안에서 저희처럼
아마존에서 말하는 미들 마일 (middle mile) 이라고 부릅니다. 미들 마일은 물건을 창고로 옮기거나 그 사이에 물건을 옮기는 것을 말합니다.
그리고 라스트 마일은 물건을 집 앞까지 옮기는 것입니다.뭐, 옵스 로보틱스는
창고 관리 부분.사람들이 그렇게 부르죠.
옵스 로보틱스에서는 이 모든 것이 규모에 관한 것이죠.아마존의 창고 관리 규모에 대해 말씀드리고 싶습니다.이 중 500개가 넘습니다.
창고, 주문 처리 센터.최대 1개까지 가능합니다.
크기는 백만 평방피트이고 수백만 평방피트는
주문 처리 센터당 품목 수.이제 옵스 로보틱스 팀은
창고 관리를 운영하는 이 서비스에는 여러 서비스가 있습니다.그럼 어떤 종류의 서비스일까요?글쎄요, 자료가 언제 들어오는지 이해하는 서비스가 필요하죠.
필요한 곳에 보관하고, 누군가 주문할 때 픽업하고, 포장하고, 배송하는 등 이 모든 것이 글로벌 옵스 로보틱스의 일부이고 이러한 서비스의 기반이 되는 서비스입니다.
여러 마이크로서비스가 있습니다.수백 개, 어쩌면 1,000개가 될 수도 있습니다.
여기서 운영되는 마이크로서비스와 안정성의 핵심 모범 사례에 대해 말씀드리려고 합니다.
벌크헤드 아키텍처 사용.벌크헤드 아키텍처란
구획을 여러 개 설정할 수 있도록 설정하기
이 구획들 중 한 구획에서 장애가 발생해도 다른 구획에는 영향을 미치지 않습니다. 우리는 이 작업을 다음과 같이 하고 있습니다.
셀 기반 아키텍처.다시 말씀드리지만, 이것은 글로벌 옵스 로보틱스가 아닙니다.이 사람은 창고 관리자가 아닙니다.다음은 일반적인 슬라이드입니다.
셀 기반 아키텍처.셀 기반 아키텍처 사용
우리가 하고 있는 일은 완벽한 제품을 만드는 것입니다.
스택이 서로 여러 번 분리되어 있기 때문에 서로 데이터를 공유하지 않기 때문에 그 위에 얇은 라우팅 레이어가 생깁니다.그 라우팅 레이어
클라이언트를 결정적으로 셀에 할당합니다. 저는 클라이언트를 따옴표로 묶었습니다.그래서 특정 클라이언트는, 그리고 언제
클라이언트를 따옴표로 묶어 말하자면, 실제로 사용자 ID일 수도 있습니다.무엇이든 될 수 있습니다.여러 가지가 있을 수 있습니다. 각 셀에 파티션 키를 지정하면
각 셀로 이동하는 특정 수의 클라이언트, 그리고 만약
한 셀에 장애가 생겼습니다. 네, 그 안에 있는 클라이언트들이죠.
셀은 영향을 받을 수 있지만 다른 셀의 클라이언트는 실패로부터 격리됩니다.자, 그들의 경우에는
주문 처리 센터.창고이고 고객 ID는 주문 처리 센터 ID가 됩니다.각 주문 처리 센터
다른 주문 처리 센터와 데이터를 공유하지 않습니다.해당 고객만을 위한 신중한 데이터 세트입니다.그러니까 말이 되는 얘기죠.
라우팅 레이어를 사용하고 셀에 요청을 할당하는 방법을 결정할 때는 주문 처리 센터에서 결정합니다.각 주문 처리 센터에는 셀이 할당되고 모든 요청은 해당 셀로 이동합니다.다른 사람과 공유하고 있을 수도 있습니다.
다른 주문 처리 센터 (단, 해당 센터의 요청은 항상)
같은 셀로 이동하세요.그러니까 이 경우에는
세 개의 주문 처리 센터가 서로 가까이 있는 것을 볼 수 있습니다.
각 셀은 서로 다른 셀에 할당되었는데, 그들이 원했던 것은 바로 지리적 중복성이었습니다.이 세 가지가 보이시죠?
서로 모여서 이런 서비스를 제공하고 있습니다.
미국 오하이오주 인디애나주 지역이 그런 것 같아요.그럼 실패하면 어떻게 될까요?그 세포, Cell2에 들어 있어요.그 FC는 오프라인 상태일 수도 있지만 아직 두 개가 더 있습니다.
그 지역의 FC와 트럭은 계속 굴러가지만 사람들은 여전히 제품을 구매하고 있습니다.이제 이들이 셀을 배치할 때 실제로 사용하고 있는 것은
각 셀마다 별도의 AWS 계정이 있고 이들은 파이프라인, 인프라를 배포하기 위한 파이프라인, 코드를 배포하는 데 파이프라인을 사용하고 있습니다.그래서 첫 번째 배포는
프로덕션에서는 실제로 사용되지는 않지만 테스트에 사용되는 사전 제작 셀로 이동합니다.
출시되기 전에, 이후에 각각 별도의 AWS 계정에 있는 세 개의 다른 셀에 각각 배포됩니다. 중앙 집중식 리포지토리라는 또 다른 계정도 있습니다.
다른 셀이 내보내는 모든 로그와 트레이스 중에서 전체 결과를 얻을 수 있습니다.
시스템을 보는 거야 왜냐면 안 되니까
세포 단위로 살펴보세요. 아니면 꼭 보고 싶으실 겁니다.
가끔은 셀 단위로 볼 수도 있지만, 모든 것을 다 찾아보고 싶을 수도 있습니다. 그래서 그곳이 모든 로그와
트레이스를 집계할 수 있습니다.이제 어떻게 생겼는지 보여드릴게요.초록색 상자는 각각 셀입니다.노란색 하나하나가
글자가 그려진 동그라미는 주문 처리 센터이며
각 센터마다 고유한 ID가 있습니다.이 케이스에는 글자가 있는데, 이것은 셀룰러 아키텍처입니다.서비스 1과 서비스 2를 보여드리겠습니다.서비스 2는 서비스 1에 따라 다릅니다.서비스 1은 업스트림입니다.
서비스 2의 종속성, 그리고 이는 셀룰러 측면에서도 마찬가지입니다.이것은 셀 기반 아키텍처입니다.여기서 그들이 발견한 문제는, 만약 고장이 난다면
서비스 1의 셀 1, 이것이 설계된 방식에는 부정적인 영향이 있습니다.
종속성으로 인해 서비스 2의 셀과 서비스에 미치는 영향, 각 주문 처리 센터의 영향
서비스를 기반으로 셀을 교환할 수 있습니다.그래서 그들이 원했던 것은
해야 할 일은 이걸 확립하는 것이었어요.각 주문 처리 센터는
특정 셀에 할당되며 해당 셀에만 할당됩니다.
스택에 있는 모든 서비스, 그리고 이제 만약 있다면
앞서 살펴본 것처럼 실패가 가장 큰 문제는 아닙니다.
세상에는 실패가 있겠지만, 실패는 제한되어 있고
다른 주문 처리 센터들은 계속 정상적으로 운영되고 있으며, 이러한 방식도
셀에 주문 처리 센터를 할당하는 시스템을 설계했나요?매우 빠른 NoSQL 데이터베이스인 DynamoDB를 사용하여 이 작업을 수행했는데, 이렇게 하면 두 가지 효과가 있었습니다. 주문 처리 센터를 셀에 맞게 정렬하는 동시에 다음 작업을 수행할 수 있게 해주기도 했습니다.
셀 간의 로드 밸런싱은 주문 처리 센터로 이어집니다.
크기가 다릅니다.그래서 제가 여기서 했던 것처럼 셀당 세 개씩 넣을 수는 없죠.그래서 이 시스템도 실행됩니다.
어떤 세포도 다른 세포보다 크지 않도록 세포의 균형을 맞추는 다양한 규칙과 휴리스틱이 있습니다. 이것이 바로 우리의 세포 구조입니다.이제 Amazon Relay와 Amazon Relay가 다중 지역을 어떻게 사용하는지에 대해 말씀드리겠습니다.
트럭이 계속 움직이도록 하기 위해서죠.여기 트럭들이 끼어들었어요.그래서 우리는 여전히 공급망의 세계에 있습니다.그래서 우리는 창고 관리에 대해 이야기했습니다.이제 다음에 대해 이야기해 볼게요.
미들 마일 매니지먼트.스포일러 경고, 없어요
라스트 마일의 예시죠.그래서 만약 여러분이 기대하고 계신다면
그거, 내년에 다시 오세요.내년에 하나 가져올게요.좋아요, 여긴 미들 마일 정도예요.그러니까 미들 마일은 세미입니다.
도로에서 아마존 프라임 심볼이 새겨진 트럭을 볼 수 있습니다.이것은 물건들을 창고로, 그리고 창고 간에 옮기는 것에 관한 것입니다. 그리고 수백만 개의 물건들이 모두 들어오는지 확인하는 거죠.
그리고 아마존 재고에 있는 수백만 개의 물품들도
고객에게 서비스를 제공할 수 있는 적절한 위치에 있습니다.이제 이 예제를 보여드릴게요. 북미에 초점을 맞추고 있지만 미들 마일은 전 세계에 존재합니다. 릴레이 앱에 대해 이야기하겠습니다.Relay 앱은
트럭 운전사가 사용하는 iOS 및 Android용 앱입니다.그러니까 미들 마일은 물건이 어디에 있어야 하고 언제 있어야 하는지를 결정하는 아주 정교한 모델을 가지고 있다고 생각하시면 됩니다.
미국 전역에서 이 모델이 이렇게 구현되었습니다.그 모델은 그냥
컴퓨터에 뭔가 있어요.그렇지 않으면 무의미하죠
트럭이 굴러다니고 물건을 옮길 수 있어요.이것이 바로 그 모델을 구현한 것입니다.이 모델은 트럭 운전사가 어디로 가야 하는지, 언제 어디로 갈지, 무엇을 픽업하고 어디로 가져가야 하는지를 알기 위해 사용하는 모델입니다. 다운로드 받을 수 있습니다.
지금 바로 휴대폰에 앱을 설치하세요.제가 해봤는데 트럭 운전사가 아니라면 거의 쓸모가 없어요.그러니까 관객 중에 있는 트럭 운전사들,
부담없이 다운로드하실 수 있지만, 다른 분들은 다 다운로드해 주세요.(웃음) 좋아요, 그럼 베스트 프랙티스
가용성이 높은 엔드포인트를 사용하는 것에 대해 이야기하겠습니다.좋아요. 그래서 이야기를 나눴습니다.
이미 얘기했잖아요, 그렇죠?가용성이 뛰어난 엔드포인트.아, 그 얘기 나눠봤어요.
IMDb 게이트웨이에 대해 이야기했을 때자, 여기도 마찬가지입니다.아마존 릴레이는
앱에는 게이트웨이도 있습니다. 다시 말하지만, 단일 지점입니다.
앱이 다음과 같이 입력하세요. iOS 앱과 안드로이드
앱 둘 다 호출 중이며 IMDb와 마찬가지로 게이트웨이가 있고 프론트엔딩에 있습니다.
여러 백엔드 서비스.이 경우에는 이를 모듈이라고 부릅니다.그래서 저는 그것들을 모듈이라고 부르기도 할게요.거기에서 모듈을 볼 수 있습니다.모듈은 대부분 서버리스 모듈로 Lambda와
DynamoDB, 그리고 게이트웨이가 있습니다.따라서 IMDb와 달리, 그렇지 않습니다.
애플리케이션 로드 밸런서 사용API 게이트웨이를 사용하고 있습니다.API 게이트웨이는 매우 유용합니다.
확장 가능한 관리형 API가 있다는 것을 알 수 있습니다.
이런 이유 때문에 API 게이트웨이가 여러 개 있습니다.
Route를 사용할 수 있다는 게 효과가 있다는 거죠.
거기 보이지 않는 53은 DNS 시스템인데
도메인 이름을 만든 다음 경로 기반 라우팅 (예: 슬래시 뒤의 내용) 을 기반으로 합니다.
도메인 이름 다음에는 다른 API Gateway로 이동한 다음 API Gateway 프론트로 이동합니다.
이 백엔드 모듈 중 하나인데, 거기 인증 로직도 있다는 것도 알 수 있습니다.이는 매우 중요하며, 이는 Amazon 인증을 호출하는 것입니다.
그걸 할 수 있는 시스템.자, 그들이 진짜로 뭘까요?
이 모델을 보러 갔을 때 마음에 들었던 것은
코드나 인프라의 공유 소유권은 없습니다.
게이트웨이와 백엔드 모듈 사이.따라서 독립적으로 배포할 수 있습니다.계약을 위반하지 않는 한 독립적으로 변경 작업을 수행할 수 있고 유연성이 크게 향상되었습니다.자, 다른 최고는
이 두 팀의 사례를 살펴보자면
워크로드를 여러 위치에 배포하고
해당 배포에 적합한 위치를 선택하고,
이에 대해 이야기하려면 2021년 12월로 돌아가야 합니다.많은 분들이 아시다시피
us-east-1에서 2021년 12월에 있었던 이벤트는
여러 서비스에서 서비스 장애가 발생했고, 영향을 받은 서비스 중 하나가 SNS 또는 단순 알림 서비스였으며 릴레이 앱은 이에 따라 달라지고 그 효과를 확인할 수 있습니다.좋아요, 그래서 일부 트럭 운전사는 적재 할당을 받지 못했습니다.그들은 짐을 받을 수 없었어요.
어디로 가야 하고 무엇을 챙겨야 하는지 배정,
100% 가 아니었음을 알 수 있습니다.최고점에 이르렀을 때는 약 30% 까지 올라갔고, 일부 사람들한테만 그랬어요.
기간은 한정되어 있지만 아직
고객과 아마존에 미치는 영향은 원치 않습니다.
그런 영향을 미치세요.그래서 당신은 무엇을 할 수 있을까요?SNS를 사용하지 않도록 다시 설계하거나 SNS를 소프트 의존성으로 만들 수도 있고, 이런 접근 방식을 취할 수도 있습니다.
그들은 그렇게 했고 예비 부품을 사용했습니다.예비 부품은 설치해야 하는 곳입니다.
리소스의 여러 인스턴스 (즉, 그 중 하나가 아닌 경우)
작동 중이면 다른 하나를 사용할 수 있습니다.자, 이 경우에는 SNS가
지역 서비스이므로 할 수 있으려면
다른 SNS 서비스를 사용하면 다른 지역으로 이동해야 했는데 어떻게 했는지 보여드릴게요. 먼저 아마존의 또 다른 문화적 요소인 COE, 즉 오류 수정 이벤트를 소개해 드릴게요.그러니까 트럭 운전사의 30% 가 도착하지 못하는 이런 일이 생기면
그들의 부하 배정은 고객에게 영향을 미칩니다.
팀에서 COE를 수행합니다.COE는 다음과 같은 심층 분석입니다.
문제의 원인과 이를 방지할 수 있는 방법아무 잘못도 없습니다.그건 아니에요.
손가락으로 가리키기 위해서요찾을 수 있는 게 아니에요
범인은 사람이야.바로 그 사람을 찾기 위해 거기 있는 거예요
문제의 실제 원인 및 해결책 마련, 이와 같은 문제, 이와 같은 문제 또는 이와 관련된 문제가 다시는 발생하지 않도록 조치를 취하십시오. 다음은 몇 가지
그들이 생각해 낸 것들과 제가 이야기할 문제들이죠.앱을 어떻게 만들었는지, 복원력에 대한 리뷰를 말씀드리겠습니다.
Relay 앱과 그 이후 배포 방법에 대해
해당 리뷰에서 찾은 내용을 적용하기 위해 여러 지역을 방문했습니다.따라서 해당 검토의 주요 목표는 경험이 저하되더라도 물리적 운영 연속성을 유지하는 것이었습니다.그렇다면 성능이 저하되었다는 것은 무엇을 의미할까요?
그 얘기 좀 해볼까요.그래서 그들이 한 세 가지 단계는
최소한의 중요 워크플로우를 명확히 설명해야 했을까요?그럼 이 중 어떤 부분이 필요할까요?
다른 부품들이 제대로 작동하지 않는다면 최선은 아니지만 그래도 계속 할 수 있습니다.
트럭이 굴러다니고 있어요.둘째, 설계 솔루션
중요한 부품은 계속 작동하도록 하고, 셋째, 중요하지 않은 부품이 작동을 멈추더라도 시스템이 계속 작동할 수 있도록 시스템을 조정해야 합니다.이것이 바로 우리가 의미하는 바입니다.
타락한 경험.여전히 효과가 있어요. 중요한 건
기능이 있고, 아까도 말씀드렸듯이 다지역 접근 방식을 사용했습니다.그래서 그들은 이미 존재하고 있었습니다.
us-east-1에 배포되었는데 재밌는 사실은 이전 버전이었던 것이 바닥났다는 것입니다.
AWS가 존재하기 전의 us-east-1이었죠.Amazon은 그곳에 데이터 센터를 보유하고 있었는데, 그것도 부족했지만, 배포하기로 결정했습니다.
오리건주에 있는 미국-서부-2에게아마존에는 30개가 있고, AWS에는 30개가 있습니다.
전 세계 지역.다음과 같은 것을 볼 수 있습니다.
북미에 있는 것들인데 해결책은 이렇게 생겼습니다.자, 이게 백엔드 모듈이에요, 알겠죠?그래서 백엔드 모듈은 그렇지 않았습니다.
반드시 Lambda와 DynamoDB처럼 간단하겠지만 중요한 것은
모두 Lambda를 앞세웠기 때문에 할 수 있었습니다.
API Gateway와 통합해도 모두 지속되었습니다.
그들의 중요한 데이터, 필요한 데이터
DynamoDB에서 공유해야 하므로 이 경우에는
us-east-1과 us-west-2에 배포된 것을 볼 수 있었습니다. DynamoDB의 좋은 점은 글로벌 테이블이라는 것이 있다는 것입니다.DynamoDB 글로벌 테이블을 사용하면 테이블을 여러 리전에 배포하고 해당 테이블 중 하나에 쓸 수 있습니다. 그러면 다음과 같은 쓰기가 가능합니다.
다른 리전에 복제됨.그래서 그들은 그걸 그냥 찾았어요.
바로 넣기만 하면 되는 쉬운 해결책이 되겠네요.자, 이 모듈들 각각은
피자 두 판 팀이 소유하거나 피자 두 판 팀이 소유할 수도 있습니다.
둘 이상을 소유하고 있지만, 모두 피자 두 판 팀이 소유하고 있고, 피자 두 팀을 기반으로 합니다.
중요도 분석을 바탕으로 다음과 같은지 여부를 결정했습니다.
여러 지역으로 갈 건지 말지전부 그랬던 건 아니었어요.
자원을 어디에 배치할지 선택해야 하기 때문이죠.
맞아요, 어디에 투자해야 할까요.이제 게이트웨이 부분은
저기 앞 부분인 그것도 두 지역에 배치되었죠.미국-이스트-1과 us-west-2로 가는 게이트웨이를 나타내는 API 게이트웨이를 볼 수 있었습니다. 이제 그 앞에 Route 53을 배치했습니다.따라서 루트 53은 우리의 DNS 시스템입니다.이것을 a라고 합니다.
액티브/액티브 아키텍처.이것이 의미하는 바는 각각
여기 있는 두 지역은 활발히 요청을 받고 있습니다.특정 요청은 그렇지 않습니다.
두 지역으로 가세요.둘 중 하나로 이동합니다.어느 쪽으로 갈지 어떻게 결정하죠?음, Route 53은 다음을 제공합니다.
몇 가지 라우팅 정책.이 경우 그들은 다음과 같이 결정했습니다.
레이턴시 라우팅을 사용하기로 했죠.따라서 Route 53은 과거 경험을 바탕으로 특정 요청에 대해 지연 시간을 가장 낮출 대상을 결정하고 요청을 해당 요청으로 라우팅합니다.다른 라우팅 정책도 있습니다.가중치 기반 라우팅이 있습니다.
지오로케이션 라우팅이 있습니다.따라서 다음을 기반으로 라우팅됩니다.
요청이 어디서 왔는지.그래서 다양한 옵션이 있습니다.이 팀인 Relay는
레이턴시 기반 라우팅, 그러니까 이건 us-east-1에 대한 요청인데, 다음을 볼 수 있습니다.
모듈 A라는 모듈은 다음과 같은 모듈입니다.
여러 지역으로 이동했습니다.따라서 요청은 다음과 같이 전달됩니다.
그들의 us-east-1 버전과 모듈 B가 있습니다.
다중 지역으로 이동하지 않았으므로 요청은 us-east-1에도 전달됩니다.하지만 us-west-2로 전달된 요청은 여기서 흥미로워집니다. 모듈 A의 경우 us-west-2로 전달되지만 덜 중요한 모듈인 모듈 B는 us-west-2에 아무것도 설정하지 않았으므로 여전히 수신할 수 있습니다.
us-east-1에서 요청이 들어왔는데 어떻게 되는지 살펴보도록 하겠습니다.
나중에 다양한 실패 시나리오에서 잘 나타나죠.네, 그게 작동하는 방식이에요.좋아요, 다음 모범 사례는 원활한 성능 저하를 구현하여 하드 종속성을 전환하는 것입니다.
소프트 디펜던시로 말이죠.자, 이제 그레이스풀 디그라데이션이 무엇인지에 대해 조금 말씀드렸습니다.유지보수에 관한 것이 관건입니다.
워크로드의 중요한 부분이지만 덜 중요한 부분은
실패할 수도 있지만 전반적으로 최종 사용자는 여전히 할 수 있습니다.
그들이 해야 할 일들.이것이 그들이 보고서를 작성할 때 했던 분석입니다.왼쪽에서 오른쪽으로 순서대로 보면 다음과 같은 것들이
트럭, 택배, 각종 사업
미들 마일이 거치는 분야별 문제,
빨간색 막대가 나타내는 것은 중요도입니다.따라서 이런 그래프를 만들면 식별할 수 있습니다.
어떤 모듈이 중요하고 어떤 모듈이 덜 중요한지 말이죠.예를 들어,
배송을 완료할 수 있는 능력이 중요합니다.여러분이 하는 것이 매우 중요합니다.
운전자를 배정해 짐을 받도록 할 수도 있습니다.그럼 중요하지 않은 게 뭐야?
없어도 뭘 할 수 있을까요?음, 앱이 제공하는
단계별 내비게이션.다시 말씀드리지만, 이 문제가 해결되지 않으면 최적이 아닙니다. 하지만 다른 GPS 시스템도 있습니다.앱에는 다음 주 화물을 예약할 수 있는 장기 예약 기능도 있습니다.뭐, 그게 중요하긴 하지만
지금 무슨 문제가 있는 동안엔 그게 중요하지 않을 수도 있고, 결국엔 뭐가 됐든
문제가 진행 중이야, 해결될 거야
그러면 다음 주의 부하를 할당할 수 있습니다.그래서 자막이 정말 마음에 들어요
여기, “트럭은 계속 움직이고, 부두에는 어떤 제품도 백업되지 않아요.”그들이 저한테 그렇게 말했어요.
“트럭은 계속 움직이고, 부두에는 어떤 제품도 백업되지 않습니다.” 이것이 바로 그들이 목표로 하는 것입니다. 그래서 다음으로 살펴볼 모범 사례는 페일오버입니다.좋아, 실패할 수 있다는 건
건강한 자원으로 넘어가세요.그럼 또 다른 이벤트가 있어서 us-east-1에서 탈락하고 순전히 us-west-2에 속하게 되면 어떻게 될까요?따라서 이 경우에는 라우팅 정책을 사용하여 모든 기능을 끌 수 있습니다.
us-east-1로 가는 트래픽, 모든 트래픽을 us-west-2로 보내고,
이런 일이 일어납니다.그래서, 미안해요.진짜로 다시 갈게요.모듈 A의 경우 다음 버전을 사용한다는 점에 유의하세요.
us-west-2에 있는 모듈 A는 중요한 모듈이고 계속 작동할 것입니다.모듈 B는 어떻게 되나요?모듈 B는 절대 안 된다는 걸 기억하세요.
us-west-2 버전을 설정하세요.그래서 두 가지 중 하나는
아마 일어날 거예요.어느 쪽이든, 요청은, 음, 요청은 다음으로 넘어갈 거예요
us-east-1에서는 실패했지만 거기서 실패했습니다
실패하고 싶다고 생각하는 문제가 생겼기 때문이죠
그렇긴 하지만 그렇다고 해서 이 지역은 절대 망하지 않아요.그런 일은 일어나지 않아요.그러니까 us-east-1의 서비스는
여전히 반응할 수 있습니다. 그게 최선의 시나리오이긴 하지만 최악의 시나리오는
반응하지 않습니다. 그리고 시스템이 설계한 방식과 점진적인 성능 저하 때문에 최적이 아닌 경험이긴 하지만 사용자가 중요한 기능, 즉 트럭을 계속 움직이게 하는, 도크에 아무것도 백업하지 않고 그대로 사용할 수 있게 해주는 경험이죠. 마지막으로 살펴보도록 할 것은 바로 테스트에 관한 것입니다.
재해 복구 전략은 다음과 같은 방법이 있을 수 있으니까요.
재해 복구 전략, 하지만 테스트해 보지 않으면
효과가 있는지는 모르겠지만 게임데이를 진행했죠.
게임데이는 기본적으로 이러한 재해 복구 전략을 실행하기 위한 것입니다.그들은 2022년 성수기에 대비하고 싶어하죠.피크 앳 아마존은
홀리데이 시즌.이미 다 된 것 같아요
블랙 프라이데이와 사이버 먼데이가 이미 진행 중이잖아요그래서 그들이 한 일은 프로덕션 단계에서 페일오버를 시작했다는 것입니다.그들은 마치 필요한 것처럼 행동했습니다.
우리 동쪽에서 나가기 위해서요그럴 필요는 없었지만
그들은 마치 그랬던 것처럼 행동했고, us-east-1에서 빠져나와 장애를 일으켜 모든 트래픽을 us-west-2로 보냈습니다.
그리고 이런 일이 일어났죠.그 증가를 눈치채셨을 겁니다.
us-west-2의 교통량은 100% 를 훨씬 넘습니다.균등하게 균형을 이루었다면 100% 증가할 것으로 예상할 수 있지만 100% 이상 증가했습니다.따라서 이것이 가장 큰 수치입니다.
트래픽의 대부분은 여전히 미국-동부-1로 향하고 있습니다.
아마도 인구 밀도의 특성 때문인 것 같습니다.
미국에서요.제가 잊고 있었던 또 다른 사실은 액티브/액티브 모델로 전환했을 때 서부 지역의 트럭 운전자들은 요청이 서부 지역으로 전송되기 때문에 지연 시간이 훨씬 짧아지기 시작했다는 것입니다.또한, 문제가 발생했을 때 실제로 가능했던 것은
고객에게 심각한 영향을 주거나 장애 등을 겪지 않고도 서비스를 성공적으로 운영할 수 있습니다.약 10분이 걸렸습니다.
페일오버를 실행하는 데.증가세를 보였습니다.
지연 시간도 짧았지만, 현재 작업 중이고
이 문제를 줄이기 위해 여전히 리엔지니어링 중이긴 하지만
지연 시간의 증가는 최적이 아닐 수도 있지만 모든 사람이 여전히 중요한 기능을 수행할 수 있게 해주었습니다.
트럭은 계속 굴러갔고 부두에는 아무것도 백업하지 않았죠.좋아요, 다음 예시는 분류 및 정책 플랫폼과 이들이 셔플을 사용하는 방법입니다.
블래스트 반경을 제한하기 위한 샤딩기본적으로 다음과 비슷합니다.
이전과 비슷하게 폭발 반경은 대략
특정 영역, 셀에 대한 고장을 포함해서
이 경우에는 샤드이므로 영향을 받지 않습니다.
시스템의 다른 부분들, 그리고 분류란 무엇일까요?
그리고 정책 플랫폼?음, 이들은 카탈로그 서비스의 일부분이고 아마존에 있는 카탈로그는 수백만, 수백만 개에 이르는 방대합니다.
아마존 카탈로그에 있는 물건들, 그리고 그 모든 물건들
아이템을 분류해야 합니다.그렇다면 분류란 무엇을 의미할까요?글쎄요, 50가지 종류가 있습니다.
분류 프로그램.어떤 유형인지 간단하게 알 수 있습니다.옷인가요?전자제품인가요?뭐 그럴 수도 있겠네요
세금이 적용되어야 합니다.이거 붙일 수 있을까요?
비행기?위험한가요?우리가 할 수 있는 일인가요?
특정 주에서 팔아요?뭔가 그런가요?
어린이도 사용할 수 있나요?뭐랄까, 온갖 종류가 있잖아요
분류가 진행 중인데, 50개의 프로그램이 사실 꼭 이 팀에 속할 필요는 없습니다.이 팀은 이 모든 분류 프로그램과 분류 적용을 호스트하기 위한 플랫폼을 운영합니다.
수백만, 수백만 개의 사물에
아마존 카탈로그에 있는데, 이게 왜 중요할까요?글쎄요, 이건 좀 버렸어요.
제가 말했거든요, 좋아요, 여기 워싱턴에 사는 제가 살 수 있는 물건이 있어요. 하지만 존이 캘리포니아에 살 때
사러 갔더니 “아니요, 가질 수 없어요.
캘리포니아에서 이 제품을 제한하거나 살 수 없다고 하기 때문이죠.” 그게 그 예입니다.
품목에 분류가 적용되었으며
주문 서비스에서 해당 분류를 읽고 다음과 같이 말할 수 있었습니다. “아니요, 판매할 수 없습니다.
캘리포니아에 사는 사람들 말인데요. 다시 말씀드리지만, 규모에 관한 것이죠.아마존 전역에 50개의 프로그램이 이 분류를 하고 있습니다.
이 플랫폼을 사용하는 사람들이죠.10,000대 이상의 기계가 있습니다.
적용 중인 학습 모델, 10만 개의 규칙, 그러니까
이거랑 저거 좀 덜 복잡하면
머신러닝보다 중요하지만 여전히 중요하죠. 매일 100개의 모델이 업데이트되죠.그러니까 이 중 10,000개
매일 100개의 머신러닝 모델이 업데이트되고 있습니다.수백만 개의 제품이
시간당 업데이트되고 있습니다.자, 이렇게 생겼습니다.좋아요, 그러니까, 만약 당신이 졸고 있다면
주의를 기울여야 할 시간이야, 여기가 바로 여기니까
조금 복잡해지네요.간단하게 만들고 싶어요, 알겠죠?그래서 분류해야 할 항목이 수백만개나 있습니다.
그리고 우리는 그것들을 각각 약 200개의 묶음으로 나누고 있습니다. 하지만 분명히 상당히 다를 수 있습니다.그러니까 그건 별로 중요하지 않아요.중요한 건 그게
10,000개의 머신러닝 모델 전부가 아니라 100개 정도의 머신러닝 모델을 모든 항목에 적용해야 합니다. 그리고 30개 모델로 묶어서 적용합니다.즉, 다음과 같은 결과가 나올 것입니다.
요청이 세 번 들어오면 30개씩 세 묶음으로 나눠요.왜 30개죠?잠시 후에 설명할게요.그러니까 이 항목들을 30개의 머신러닝으로 처리해 달라는 이런 요청들은
모델은 분류기로 이동합니다.이 경우에는 엘라스틱입니다.
머신 러닝을 실행하는 컨테이너 서비스 서비스
이러한 항목을 기준으로 모델을 만들고 그 역할을 하는 것은 가져오는 것입니다.
S3에서 내려온 모델들.그래서 이렇게 적혀있죠. “오, 이것들
아이템에는 이 30가지 모델이 필요합니다.이 30개를 뽑을 게요
모델을 내려서 실행해 보세요.” 그러면 캐싱할 수 있습니다.
모델들, 그게 중요해요. 왜냐하면 우리가 실제로 하고 싶거든요.
워커를 사용해 보세요. 워커는 모두 워커, 분류기로 이미 해당 모델이 캐싱되어 있습니다.따라서 모델을 축소하고 교체하는 것은 비효율적이며 분류를 수행한 후 DynamoDB에 기록합니다.이것이 논리적인 관점입니다.건축적 관점을 보여드릴게요.아, 잠깐만, 말하겠다고 약속했잖아
30이 왜 중요한지 아시겠죠.두 가지 이유가 있습니다.그 결과 30개의 관련 모델을 사용할 수 있을 정도로 크기가 적당하다는 것을 알게 되었습니다. 마치 한 모델이 다른 모델의 출력을 실제로 사용하는 것처럼 말이죠.어떤 면에서는 서로 관련이 있긴 하지만 다른 이유는 제가 말씀드린 캐싱에 관한 것입니다.너무 많을 수 밖에 없습니다.
이러한 서비스가 캐시에 보관할 수 있는 모델입니다.그래서 만약 당신이 지시한다면
100개의 모델을 실행할 경우 모든 모델을 캐시에 보관할 수 없기 때문에 다시 30개로 줄이면 스왑아웃을 피할 수 있습니다.기억하세요, 시도해 보세요
그런 스왑아웃은 피하세요.그러니까 이건 건축적인 관점에 가깝죠.따라서 30개 모델에 대한 요청 중 하나를 Kinesis를 거쳐야 합니다.
Kinesis는 요청의 메타데이터를 읽고 무엇을 결정하는지 결정합니다.
모델이 적용될 예정이고 이는 중요합니다.이 부분이 바로 그 부분입니다.
라우터 역할을 하는 AWS Lambda로 전송합니다.AWS Lambda에 이렇게 적혀 있습니다. “아, 이 30가지 모델이 필요한가요?당신을 이 작업자에게 보내줄게요. 그러면 SQS에 올라갑니다.
대기열에 넣으면 작업자, ECS 서비스가 대기열에서 해당 내용을 읽습니다.다시 말해,
이 노동자들 중 60명이요.노동자들은 멍청해요.당신이 주는 것은 무엇이든 처리해 줄 거예요.30가지 머신러닝 모델을 처리하라고 하잖아요.캐시에 있는지 확인할 거예요.네, 좋아요, 제가 할게요.캐시에 없어요.모두
좋아요, 제가 내려볼게요.그들은 신경 안 써요. 그래서 모든 사람들이
스마트한 기술이 람다에 있습니다.Lambda는 이러한 30개의 모델 요청 각각을 해당 30개를 처리한 동일한 워커 또는 워커에 보관하려고 합니다.
이전에 스와핑을 방지하기 위해 이러한 벌크헤드 아키텍처를 사용하는 것에 대해 이야기했던 모범 사례에 대해 이야기해 보도록 하겠습니다. 다만 말씀드리려는 것은 아닙니다.
이 사례에서는 세포에 대해 알아보겠습니다.샤드, 특히 셔플 샤딩에 대해 이야기할 거예요.이제 셔플 샤딩에 대해 설명하기 위해 슬라이드를 세 장 정도 가져가겠습니다.자, 경고, 셔플 샤딩은 이제 한 시간 정도 됐어요
이번 컨퍼런스에서는 셔플 샤딩에 대해 설명하기 위해 강연하고 있는데, 저는 세 개의 슬라이드로 나누어 설명하겠습니다.그럼 제가 메시지를 전달했으면 좋겠어요.
안 되더라도 땀을 흘리지 마세요.아직 따라하실 수 있을 것 같아요.좋아요, 이것은 작업자가 여러 명인 일부 서비스의 예일 뿐입니다.EC2 인스턴스일 수도 있고, CPP의 경우처럼
ECS 서비스일 수도 있고, 무엇보다도 서로 다를 수 있습니다.
심볼은 서비스의 클라이언트나 호출자가 다르다는 것인데 여기에는 샤딩이 전혀 일어나지 않습니다.
샤딩은 제가 가지고 있다면 그 클라이언트 중 한 명이
우리가 독약이라고 부르는 것이죠.둘 중 하나를 기형으로 만들 수도 있습니다.
요청, 손상된 요청, 악의적인 요청, 서비스를 중단시키는 무언가
해당 프로세스에서 실행 중인 프로세스.어쩌면 버그가 간지러질 수도 있습니다.
우리가 가지고 있는지도 몰랐던 일이었죠.그 일꾼이 쓰러졌어요.좋아요, 문제 없어요.우린 가지고 있어요
로드 밸런싱 맞죠?그 작업자는 쓰러졌어요.
다른 일꾼을 시험해 봅시다.오, 저거 쓰러뜨리네요다음 녀석도 내려가고 결국엔 잘 될 거야
일꾼들이 한 명도 없을 때까지 모든 일꾼들을 뚫고 다녔습니다.
모두들 운이 안 좋으니까요.이제 모든 클라이언트가 빨간색이에요.아무도 서비스 센터에 전화할 수 없어요.그건 샤딩이 아니에요.그럼 시작해 볼까요?
샤딩을 소개해, 알겠지?이건 샤딩이에요.세포와 달리 자원을 가져가는 거죠.세포는 전체 스택이었죠.이것은 단지 리소스 레이어를 가져와서 여러 덩어리로 나누는 것입니다.
이 경우에는 두 개의 청크입니다.일꾼 두 마리가 각각 파편인데, 이 경우에는 고양이가 제 일을 해서 일꾼 두 명을 죽입니다.고양이와 개는 불행해요
하지만 다른 사람들은 모두 행복해요.이것이 바로 벌크헤드 아키텍처입니다.그 안에 샤드의 장애가 있었고 숫자를 볼 수 있었죠.
영향을 받는 고객 수는 고객 8명을 샤드로 나눈 값, 4. 영향을 받은 고객 2명입니다.
체크아웃이 되잖아요, 그렇죠?좋아요, 이제 셔플 샤딩을 해봐요여기서 흥미로워지죠.이 경우 각 클라이언트에는 고유한 두 명의 작업자 쌍이 할당되지만 작업자를 공유할 수도 있습니다.그럼 이게 무슨 뜻일까요?여기 있는 감독님을 보면, 감독님에게는 이 두 명의 일꾼이 있습니다.저건 비숍 샤드예요.그
루크에는 이 두 명의 일꾼이 있습니다.둘은 독특한 쌍이에요.
같은 쌍은 아니지만 일꾼을 같이 쓰고 있어요.여기도 똑같아요.고양이에게는 일꾼이 두 명 있지만, 다시 말씀드리지만, 고양이만의 일꾼이 있습니다.
독특한 한 쌍의 일꾼.고객 한 명도 없고, 한 명도 없습니다.
여기 있는 여덟 명의 클라이언트 중 같은 두 개의 클라이언트가 있습니다.이들은 각각 다른 샤드와 공유하지만 아무도 샤드를 공유하지 않습니다.
다른 클라이언트와 같은 두개야그럼 이 경우에는 어떤 일이 벌어질까요?
고양이가 제 일을 해서 일꾼 둘을 죽이는 거죠.쓰러졌어요. 하지만 루크는 일꾼 한 명을 같이 쓰고 있었는데도 여전히 건강한 일꾼이 있어요.이곳에는 독특한 일꾼 한 쌍이 있습니다.저도 마찬가지입니다.
비숍과 다른 모든 사람들.따라서 영향을 받는 고객 수를 조합으로 나눈 값인데, 이 경우에는 고객 8명이고 저는 셔플 샤드를 8개로 만들었기 때문에 단 한 명의 고객만 영향을 받았습니다. 즉, 규모가 클 경우 우리의 영향 범위는
12 1/2 퍼센트 또는 1/8, 즉 우리 회사가 800명이라면
100명의 고객들이 영향을 받겠지만, 그보다는 나아질 것입니다.
왜냐하면 사실 저는 8개 이상을 벌 수 있으니까요.
이걸 가지고 샤드를 섞으세요.8명의 작업자와
2개를 조합해서 만들면 아마 아시는 분들도 있을 것입니다.
이 수학은 8과 2를 선택해서 28개의 조합을 만들 수 있습니다. 그래서 실제 범위는
영향력은 훨씬 적습니다. 잘 모르신다면
수학은 걱정하지 마세요.조합의 개수는 이정도입니다.
2개의 워커로 구성된 고유한 세트를 만들 수 있습니다. 만약 8이 주어지면
정말 미쳐가고 싶으시다면, 53번 국도가 있는데 2,000개가 넘는 도로가 있습니다.
파편 4개를 만드는 노동자들여러분의 영향 범위는 7,300억 분의 1입니다.그 시점에서 계산은 좀 이상해지지만, 다시 CPP로 돌아가 봅시다.좋아요, CPP에는 직원이 있습니다.노동자들의 임무는 다음과 같습니다.
30개의 머신러닝 모델의 이러한 워크로드를 한 번에 처리합니다.10,000개가 넘습니다.
머신러닝 모델, 그리고 우리는 처리해야 합니다
30개씩 묶어서 말이죠.그러니까 총 400개의 그룹이 있는 거죠.
400개의 샤드가 필요할 거예요. 주어진 샤드는
주어진 머신러닝 모델을 처리하고 싶습니다.
교체하지 않고 말이죠.그래서 샤드 400개가 필요하겠네요.일꾼이 60명이면
셔플 샤딩을 할 수 있어요.파란색 샤드, 초록색
샤드와 오렌지 샤드, 공유하는 게 보이시나요?
서로 일해요. 하지만 이 경우에는 각각 독특한 세 가지 조합이죠.그렇다면 어떻게 될까요?
오렌지 조각이 쓰러지는 독약 사건, 그 30개의 모델들 때문에
우리가 어찌어찌 부패해서 무슨 일이 생기고
그 파편에 독이 묻었다고요?셔플 샤딩이 없다면
그냥 표준 샤딩으로 400개의 머신 러닝 그룹을 가져와서 20개가 넘는 샤드를 배포했습니다. 왜냐하면 우리가 60개를 취하면
3으로 나누면 20이고, 그 다음 머신 러닝은 20개입니다.
그룹이 영향을 받겠지만 셔플 샤딩을 사용하면
샤드를 400개 만들 수 있습니다.그래서 400개의 기계 그룹이 있습니다.
학습 모델, 샤드 400개그 중 하나가 독에 걸리고,
그러면 그 한 마리만 영향을 받는데, 다시 말씀드리자면, 고양이에게서 본 것과 같은 경우입니다.왜냐하면 하나하나가
세 명의 일꾼으로 구성된 독특한 그룹을 가지고 있는데, 미쳐버리자면 실제로 만들 수 있겠죠.
샤드는 400개가 훨씬 넘습니다. 60개 중 3개를 선택하면 34,000개가 넘습니다.좋아요, 그리고 이걸 실현하기 위해 이들이 하고 있는 또 다른 일은 느슨하게 연결된 종속성을 구현하는 것입니다.그 방법을 보여드릴게요.
효과가 있어요.라우터를 기억하시나요?저기 있는 람다에 모든 스마트 기기가 들어있어요.어떤 샤드를 사용할지를 결정합니다.제가 일꾼이라고 말하기 전에 기억하세요.Lambda는 실제로 요청을 보낼 샤드를 결정합니다.기억하세요, 퍼팅 중이에요.
당시 SQS 대기열에 있는 것들은
작업자가 픽업합니다.그러니까 Lambda가 실제로
해당 대기열을 모니터링하는 거죠.실제로 보고 있는 게
가장 오래된 메시지의 나이죠.나이가 가장 많은 경우
메시지가 꽤 오래되었으니 아마도 그 대기열을 의미할 것입니다.
꽤 느리고 혼잡합니다.그러니까 실제로 역압을 이용해서 안에 있는 작업자를 결정하는 거죠.
샤드라고 부를 거예요세 조각으로 나누면 일꾼을 선택할 수 있습니다.
그쪽이 가장 바쁘지 않죠.저기 보이시죠?
중간에 있는 것이 가장 덜 바쁘기 때문에 중간을 선택합니다.따라서 Lambda는 단순한 라우터가 아닙니다.로드 밸런서이기도 합니다.
배압을 사용하여 샤드에 있는 작업자를 따라 라우팅합니다.자, 하중이 너무 높으면 어떡하죠?스파이크가 발생하면 어떻게 될까요?
그리고 그 안에 있는 모든 노동자들, 세 명의 노동자 모두
샤드에 과부하가 걸린다고요?바로 여기에서 로드 셰딩이 시작됩니다.요청이 전송될 것입니다.
로드 쉐딩 대기열로 이동했다가 15분 후에 다시 돌아오세요.왜 15분이냐고요?글쎄요, 저것들은
ECS 서비스는 자동 확장 중입니다.CPU 수준을 기반으로 합니다.따라서 실제로 사용량이 급증할 경우 ECS 서비스는
CPU가 높아지면 확장될 것입니다.15분 후에 다시 돌아와서 요청을 다시 처리하고
그 시점에서 제대로 작동할 것입니다.좋아요, 이건 사실
오늘의 마지막 예시요.Amazon Search가 어떻게 카오스 엔지니어링을 활용하여 언제든지 프라임 데이를 맞이할 수 있도록 준비하고 있는지에 대해 알아보겠습니다.아마존 서치, 여러분 모두 보셨을 것 같은데요.아마 다 가지고 계실 겁니다.
여기 검색창을 봤어요.카오스 엔지니어링을 검색해서 어떤 결과가 나오는지 확인해 보는 건 어떨까요?좋아요, 1,000개 이상의 결과가 나왔네요.좋아요, 최상위 결과는
케이시와 노라의 “카오스 엔지니어링” 책이죠.그런 식이죠.
카오스 엔지니어링 바이블그래서 좋은 결과네요.서비스 수준 목표인 SLO도 눈여겨 보시기 바랍니다.
강아지와 함께 예약하세요. 그것도 중요할 거고, 여기서는 규모에 대해 이야기할 거예요.그래서 지금 얘기하고 있는 건데요.
수백만 개의 제품.300개 정도를 말씀드리는 겁니다.
백만 명의 활성 사용자.지난 프라임 데이에 대해 말씀드리자면, 초당 84,000건의 요청이 있었죠.
프라임 데이에 최고조에 달합니다.다시 말씀드리지만, 요점은 이러한 서비스의 규모를 보여주는 것입니다.
그리고 어떻게 AWS를 사용하여 이러한 규모의 요구 사항을 충족하고 있는지, 그리고 다른 모든 것과 마찬가지로 Search도 마찬가지입니다.
제가 보여드린 다른 것들은 여러 개의 백엔드 서비스와 여러 AWS 리소스를 사용하는 것으로 구성되어 있는데, 제가 정말 좋아하는 것은
Search 팀에는 자체 레질리언스 팀이 있다는 것입니다.그래서 그들은 여러분의 내장 기능을 갖추고 있습니다.
검색 조직을 위한 운영 레질리언스 및 사이트 안정성 엔지니어링을 담당하는 레질리언스 전담 팀
40개 서비스 전반의 주요 모토는 다음과 같습니다. “테스트하고 개선하고
Amazon Search 서비스의 복원력을 촉진합니다.”어떻게 그런 일을 할 수 있을까요?홍보를 통해 그렇게 하는 거죠.
부하 테스트에 도움이 되는 레질리언스 이니셔티브
그리고 카오스 엔지니어링을 촉진하고 조율하는 데 도움을 주는데, 이것이 바로 제가 말씀드리고 싶은 부분입니다.그래서 이 분야의 모범 사례는
카오스 엔지니어링을 사용하여 워크로드를 테스트하는 경우를 예로 들 수 있습니다.
레질리언스를 테스트하기 위해서요그렇다면 카오스 엔지니어링이란 무엇일까요?제가 슬라이드를 하나 읽어드릴게요.“카오스 엔지니어링은
생산 현장의 난기류를 견딜 수 있는 시스템의 능력에 대한 확신을 심어주기 위해 시스템을 실험하는 것입니다.”생산 현장의 난류 조건.우리 모두 이를 통해 비정상적인 사용자 활동, 네트워크 문제, 인프라 문제, 잘못된 배포 등을 식별할 수 있을 것 같습니다.제 말은, 세상은 엉망이고 우리는 이에 대한 회복력이 필요하다는 거죠.그래서 알아두어야 할 것은
카오스 엔지니어링은 카오스를 만든다는 얘기가 아니에요.이를 인정하는 것이 관건입니다.
이미 존재하는 혼돈에 대비하고 이를 완화하고 그 혼돈의 영향을 피해야 합니다.이것이 바로 카오스 엔지니어링에 대해 생각해 보아야 할 방식입니다.그럼 카오스 엔지니어링은 어떻게 할 수 있을까요?이것은 한 슬라이드에 대한 요약입니다.
카오스 엔지니어링을 하는 방법.카오스 엔지니어링은
궁극적으로는 과학적인 방법이 핵심입니다.이것은 순환 주기이지만 정상 상태부터 시작하겠습니다.정상 상태가 도대체 뭐야?정상 상태는 다음을 의미합니다.
워크로드, 테스트 대상 워크로드는 설계 매개변수 내에서 작동하므로 이를 측정할 수 있어야 합니다.다음을 수행할 수 있어야 합니다.
운영이 무엇을 의미하는지 알 수 있는 지표를 할당하세요.
설계 파라미터 내에서다음은 가설입니다.가설은 다음과 같습니다.
어떤 나쁜 일이 생기면 그 나쁜 일을 구체적으로 말하죠.
EC2 인스턴스가 종료되거나, 가용 영역을 사용할 수 없거나, 네트워크 링크가 끊어지면
제 시스템이 고장나면, 제가 그렇게 설계했기 때문에
안정된 상태를 유지할 것입니다.그 안에 머무를 거예요
운영 파라미터.자, 그렇게 설계하지 않았다면 카오스 엔지니어링은 하지 마세요. 하지만 그렇게 설계했다면
뭐, 이건 테스트하는 거잖아요.그럼 실험을 실행해 보세요.
EC2 장애를 시뮬레이션합니다.네트워크 링크 중단을 시뮬레이션한 다음 검증합니다.가설이 확정되었는지 확인합니다.만약 가설이 다음과 같다면
확인되지 않았어요, 오, 알았어요.일종의 정전이 발생했어요.우리는 밖으로 나갔습니다.
파라미터를 설정했습니다.우리는 안정성을 유지하지 못했습니다.
상태.개선이 필요합니다.재설계하고 모범 사례를 적용함으로써 발전할 수 있습니다.
신뢰도를 기준으로 삼고 나서 다시 테스트합니다.실험을 다시 실행해 보세요.오, 이제 가설이 확정되고 정상 상태로 돌아왔어요. 모든 것이
다시 반복하죠.그러니까 서비스 수준 목표가 달성될 거라고 말씀드렸잖아요.
다시 한 번 말씀드리지만, 여기 있습니다.이게 예시예요
서비스 수준 목표.이것은 그들이 실제로 사용하는 것이 아닙니다.그들은 그걸 공유하고 싶지는 않았지만 공유하고 싶었죠.
형식을 공유해 주세요.자, 이것이 그 형식입니다.28일의 트레일링 윈도우에서,
지연 시간이 1초 미만인 요청의 99.9% 를 보게 될 것입니다.예시는 다음과 같습니다.
검색 팀에서 사용할 수 있는 서비스 수준 목표, 그리고 이 서비스 수준 목표를 통해 무언가를 설정했습니다.
이를 오류 예산이라고 부릅니다.그럼 오류 예산은 얼마일까요?음, 99.9% 는 0.1% 라는 뜻이죠.
1초보다 클 수도 있습니다.이것이 우리의 시작입니다.
예산.그게 우리 예산이에요.하지만 모든 요청에는
1초를 초과하면 그 예산이 소모되고 있습니다.결국에는 그 전체가
모든 것이 소진될 것이고 예산이 부족해질 것입니다.
예산이 얼마나 빨리 소진되고 있는지 실제로 볼 수 있습니다.이를 소진율이라고 합니다.
하지만 좋은 소식도 있어요.28일짜리 트레일링 윈도우가 있어요.즉, 가장 오래된 실패와 가장 오래된 요청의
1초보다 크면 결국에는 시간이 초과되고 결국에는 노화가 끝나게 됩니다. 제가 말해야 할 것은 28일이 넘으면
예산이 충당됩니다.이것이 바로 오류 예산의 개념입니다.그래서 그들은 고객에 초점을 맞추고 싶어하죠.
카오스 엔지니어링.카오스 엔지니어링은 그렇지 않습니다.
엔지니어링 팀을 위한 것입니다.개발자를 위한 게 아니에요.우리가 설립할 수 있도록 하기 위해서죠.
고객의 요구 사항을 충족할 수 있는 경험을 제공한다고 그들은 생각했습니다. SLO는
가장 좋은 방법이죠.매우 고객 중심적입니다.무엇에 초점을 맞추고 있냐고요.
고객이 직접 경험하고, 따라서 실험은 반드시 이루어져야 합니다.
오차 예산 범위 내에서 실험의 중지 조건을 준수해야 합니다. 예를 들어, 연소율이 너무 높은 경우 카오스 엔지니어링 실험의 중지 조건이 항상 있어야 합니다.
오차 예산이 너무 많으면 실험이 중지됩니다.안돈 코드를 당기면그럼 안돈 코드는 다시 돌아가게 되죠.
Toyota 공장에 갔는데, 거기에는 실제로 코드를 가지고 있었죠. 그 곳에서는 다른 사람들이 가지고 있던 코드를 가지고 있었죠.
품질 문제가 보이면 조립 라인을 철수시킬 수도 있죠.여기도 마찬가지예요.세계 곳곳에 있는 몇몇 사람들
조직에서 이 버튼을 누르면 중지했다가 롤백됩니다.
어떤 실험이든 언제든지 할 수 있고, 마지막으로 할 일은 뭔가 진행 중인지 여부입니다.
아마존 IT 전반에서 벌어지고 있는 이런 종류의 이벤트에서
그럼 지금이 카오스 엔지니어링을 하기에 좋은 시기가 아니니까 그만하고
그때도 한 번 돌려 보세요. 이게 그들이 설계한 거예요.건축에 대해 이야기해보려고 왔어요.오른쪽은, 그냥 보고 싶은데요
모든 것이 결함 주입 시뮬레이터에 집중되어 있다는 점을 지적해 주세요.폴트 인젝션 시뮬레이터는 카오스 실험을 실행하는 데 사용할 수 있는 AWS 서비스인데, 그들은 이를 중심으로 구축했습니다.맨 오른쪽은
ECS와 EC2를 볼 수 있습니다.이것이 바로 검색 서비스입니다.40개 이상의 검색 서비스가 있다는 것을 기억하세요.그래서 그들은 결함 주입 시뮬레이터를 사용하여 해당 서비스에 대한 카오스 엔지니어링을 수행하고 있습니다.그들이 만든 것은 왼쪽에 있는 부분이었죠.이게 바로 오케스트레이션 작품이에요.자, 이제 저를 따라오세요.
왼쪽 하단에 있는 API 게이트웨이로 내려가세요.두 개의 API를 볼 수 있습니다.첫 번째는 안돈입니다.
API와 안돈 API는 다음을 설정하고 구성합니다.
안돈 코드.설정을 통해 이 작업을 수행합니다.
FIS가 응답하는 다양한 CloudWatch 경보.FIS에는 가드레일이 있습니다.기억하세요, 제가 좋은 실험을 했다고 했잖아요.
가드레일이 있어야 해요그래서 FIS에는 CloudWatch를 기반으로 한 가드레일이 있어서 누군가 안돈 코드를 당기면 CloudWatch 경보를 울려 FIS 실험을 중단시킵니다.좋아요, 다른 API는 실행 API입니다.실험을 실행하고 나중을 위해 일정을 잡을 수 있는 기능이 있기 때문에 람다가 있습니다.
DynamoDB에 일정을 저장할 수 있는 스케줄러가 있는데, 이 스케줄러는 오케스트레이션을 제공합니다.오른쪽이 보이시죠?
저기 람다가 세 개 있네요.따라서 실험을 실행할 수 있을 뿐만 아니라 실험 전에 작업을 수행할 수 있습니다.뭘 하고 싶으세요?
실험을 하기 전에?보내고 싶을지도 몰라요
여러 직원에게 알리세요.아무거나 멈추고 싶을지도 몰라요
진행 중인 배포.따라서 다양한 것이 있습니다.
실험을 하기 전에 하고 싶은 일들이 있다면
FIS를 사용하여 실험을 실행한 다음 실험 후 작업을 수행합니다. 예를 들어, 일부 실험처럼 정리하는 등의 작업을 수행하면 결함 주입은 자동으로 수정되지 않습니다. 그래서 실제로
가서 수정해서 이런 일을 할 수 있도록 하기 위해서요. FIS가 존재하기 때문에 훌륭한 서비스인데 왜 만들었을까요?
이 오케스트레이션 곡이요?이게 바로 그 이유죠.첫째는 그들이
40개 이상의 팀에 서비스를 제공합니다.단일 창으로 팀 전체에 일관된 경험을 제공하고 매우 쉽게 만들 수 있기를 원합니다.
카오스 엔지니어링을 하는 거죠.그들은 또한 추가하기를 원했습니다.
스케줄링 기능, 배포와 함께 실행할 수 있는 기능 등 FIS에서 할 수 있는 일이지만 기억하세요. 이 40개 서비스 모두가 사용하고 있습니다.
파이프라인 시스템이 공통적이므로 오케스트레이션은
이를 중심으로 설계할 수 있고 아주 쉽게 만들 수 있습니다.
배포와 함께 실행하고 일관된 가드레일을 제공하세요.SLO는 다음과 같다는 점을 기억하세요.
중요한 가드레일.그래서 그들은 실제로
시스템 스토리지의 일부로 다양한 SLO를 모두 저장하여 이를 사용합니다.
실험 중에 가드레일을 제공했습니다.안돈 코드 기능
FIS는 기본적으로 FIS의 일부가 아니기 때문에 이를 제공하고 있습니다.
그리고 지표와 인사이트.물론 FIS는 지표를 내보냅니다. 하지만 이제 FIS는 40개 서비스 전체의 모든 지표를 취합하여 경영진에게 어떤 혼란을 야기하는지에 대한 단일 보고서로 제공할 수 있습니다.
그들은 엔지니어링 작업을 하고 있고, FIS 외에도 운영할 수 있기를 원합니다.
다른 종류의 결함도 있고요.그 얘기 좀 해봅시다.오, 글쎄요, 안 돼요.애들이 이러고 있을 때
여러분, 왜 그랬죠?그들은 왜 오케스트레이터를 만들었을까요?왜냐하면 그들은 준비를 하고 싶어하니까요.
언제든지 프라임 데이를 위해 말이죠.좋아요, 결함 유형이네요.
먼저 FIS 결함이 있습니다.이러한 것들은 모두 FIS에서 지원됩니다 (예: ECS 삭제).
노드, EC2 인스턴스 종료, 지연 시간 주입, 매우, SSM 또는 Systems Manager를 사용하면 원하는 모든 종류의 자동화를 실행할 수 있으므로 시뮬레이션도 가능합니다.
가용 영역 장애, 그런데 어떤 종류의 결함일까요?
그런 짓을 하는 게 FIS 아닌가요?글쎄요, 짐이 많아요
테스트는 사실 아마존 내부의 아마존 팀 전체에서 매우 인기 있는 로드 테스트 도구이기 때문입니다.실험의 일부로 사용할 수 있기를 원하죠. 비상 수단도 있고요.그러니까 비상 레버는 서비스 운영자로서 할 수 있는 것들이죠.
협박을 받고 있는 서비스를 도와주세요.협박에 의한 서비스의 경우
비상 레버를 당기면 서비스가 제대로 작동할 수 있습니다. 예를 들어 모든 로봇을 차단하고 궁극적으로는 제가 달려갑니다.
시간이 좀 부족해서 속도를 내야겠어
이걸 통해 궁극적으로는 그들이 제공하는 게
최종 사용자에게 혜택을 줍니다.한 가지 지적하고 싶은 건
최종 고객을 위한 가용성 향상, 복원력 향상에 관한 것인데, 이에 대해 말씀드리고자 합니다.
우아한 성능 저하 및 비상 지렛대.비상 사태는 무엇을 하나요?
검색용 레버는 어떻게 생겼나요?글쎄요, 그 중 하나는
비상 레버는 사실 레버를 당기면 우아함을 느끼게 하는 것입니다.
의도적인 성능 저하이것이 바로 완전한 검색 경험인 검색의 모습입니다.
레고를 찾고 있는데 비상 레버를 당기면 중요하지 않은 서비스가 꺼집니다.그러니까 이미지, 제목, 가격 같은 중요한 서비스는 그대로인데 리뷰나 연령대 같은 중요하지 않은 서비스는 없어요.따라서 아래 시스템의 경우
협박, 이게 도움이 될 수 있어요. 그리고 그들은 이걸 시험해 보죠.
카오스 엔지니어링 사용.레버가 작동하여 검색이 가능하다는 가설이 있습니다.
스트레스를 감당하기 위해서요말 그대로 에너지를 생성하죠.
테스트 중에 로드를 한 다음 레버를 당겨서 시스템이 제대로 작동하는지 확인합니다.
협박을 감당하기 위해서요좋아요, 요약하자면 제가 다룬 다섯 가지 서비스입니다.저한테는 이 부분이 중요해요.좋아요, 그럼 이 모든 정보를 얻어서 여러분과 공유할 수 있도록
재밌게 봐주셨으면 좋겠어요. 저는 많은 똑똑한 사람들과 함께 일해야 했어요.
여러 팀에 있는 엔지니어들, 그리고 똑똑한 엔지니어들이 멋진 일을 하는 이유는
그들은 정말 바빠서 시간을 내서 저와 함께 이 문제에 대해 설명해 주었어요.
여러분과 공유할 수 있어서 정말 고마워요
그 엔지니어들께, 그리고 이 분야에 대한 저의 경외심에
그들이 해낸 엔지니어링이죠.정말 감명 받았어요.여러분도 이 제품에 깊은 인상을 받았으면 좋겠어요.몇 가지 자료.안 할게요
여기서 너무 많은 시간을 보내세요.진짜 사진 빨리 찍고 싶니?다룰 수도 있는 예정된 강연
앞서 이야기한 내용과 제가 다룬 두 가지 예시는 실제로 다음과 같은 외부 자료를 확인해 볼 수 있습니다.
더 알아보고 싶으실 겁니다.