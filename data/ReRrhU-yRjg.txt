hello everyone welcome to this session Arc 312 journey to cell based microservices architecture on AWS for hyperscale my name is Anand and I'm a Senior Solutions architect at AWS supporting digital native business customers these are customers who take a cloud first approach to it and operate a hyperscale environment to support the growth in their business and I'm joined together by my colleague from doordash J Wallace uh hi everyone my name is Jay I'm an engineering manager for cloud infrastructure at doordash I've been with the company for about five years which is great because I've been able to see our architecture change over the course of those five years this morning both the non and I are going to be walking you through today's session so let's take a quick look at the agenda we'll be sharing with you the Journey of doordash a technology company with his fastest growing lost my delivery platform supporting local Commerce we'll begin with the early days of the monarith and how it has evolved into a micro Services architecture and the adoption pattern we'll look at what hyperscale means challenges of running microservices at hyperscale and how cell based architecture helps in addressing those challenges we'll also take a quick look at how AWS helps in that design will then turn our attention specifically to doordash supercell or project where they have implemented this architecture and then end the session with some key learnings and next steps you are expected to have some familiarity with AWS networking services and then some of the compute and database options available on AWS both me and Jay will be interleaving our talk as Jade walks through the lower Dash's journey and I talk through the concepts of the architecture and the AWS service ecosystem let's start then by looking at the doordash's business model awesome thanks and on I'm sure many of you out there have probably heard or used doordash but for those of you who may be less familiar we're a tech tech company and our main goal is to connect consumers with their favorite local businesses and today we operate in 27 different countries um a little bit more about our business we operate a three-sided Marketplace that consists of our consumers our merchants and our Dashers we operate in various verticals everything from food to groceries to Home Goods and today we have around 25 million consumers 550 000 merchants and over a million Dashers to filling delivery on the platform and of course none of this would be possible without the hard work for our 15 000 employees across the globe I want to start this morning by providing a bit of doordash history and specifically looking at how our architecture has changed over time and what drove some of those decisions um 2013 is actually a really great starting place for this because this was the year that doordash was founded and shortly after our monolith was born um as with most early stage startups the focus early on is building and delivering so Django was chosen for our monolith this is what our Founders knew and it allowed the team to move quickly and provide a unified framework for both the front and the front and in the back end if we take a look at the picture monolith it's not too much like our own our monolith was a Django web app and our main DB was postgres and this is basically a lot of monoliths so monolith is a typically a large system using a single code base and it's deployed as a single unit or a single service and it's usually just put behind a load balancer and early on there's a lot of advantages to this um one of the first advantages is around Simplicity monoliths are simple to build test and deploy they're easy to scale horizontally and then there's lots of sharing between the various components so you only basically have to do something once so you could share components like you can share across your components things like logging pipelining config management and then of course lastly performance is a really great Advantage with regards to the monolith because you don't have to contend with any inner service network latency because everything is an in-memory call between the various services that are part of your monolith this of course doesn't come without its own drawbacks um otherwise we'd always be using monoliths the first is tight coupling that affects the management or scalability and continuous deployment um of of the code base so if you think about it if you wanted to cut a new feature you'd have to do a full deploy of the monolith to land that new feature as the code base grows it inevitably becomes more difficult for a engineer to understand and begin contributing adding new capabilities is super slow and then lastly although I kind of shared earlier that you know sharing is an advantage it also poses as a disadvantage where if you have sharing of modules across your code base you end up with a blast radius issue where if someone makes a change to a shared module it may result in an undesirable consequence an unrelated service causing a failure and for us that was pretty much the state of the world up until 2014 when we had our first microservice it was our Logistics AI service and I want to use the term microservice here very Loosely as it wasn't part of any sort of grand re-architecture or anything like that but it was rather because Pi uh rather because Scala was a better choice for CPU intensive tasks in Python and this actually isn't too uncommon um you might actually be seeing this as in your own Journey right now you might have a team that might be spinning off some new service or working on something independently and they want to take advantage of a specific framework or technology that's just better suited for their needs by 2017 we had begun spinning up various Services outside our monolith but really they lacked clear Direction and intent our monolith had really begun to hit some serious limitations for one the team itself had grown significantly since the early days meaning that we released software more often and that there weren't and since there weren't individual Deployable units it was super slow our pipelines began becoming painstakingly slow because we had unit tests that were stacking up which resulted in either Engineers omitting opting out of of some unit tests which meant there was less guarantees in a given deployment or accepting that deploying software was going to be slow um plus as I previously mentioned for engineers to get started there was a steep learning curve when it came to learning the monolith code base um lastly given as I alluded to in the previous slide there was no failure isolation so updates in one place would often result in issues in another our next phase was in 2018 when our company went all in on microservices during this phase we continued to grow as a company and grow into various new verticals and our order volume increase our customers increase our Merchants increased but the focus at this time was really to allow our Engineers to work on components and services independently so they could be more agile but at this point in time the monolith itself was still at the middle of a lot of critical flows we had a bunch of microservices that folks could work on independently in kubernetes but unfortunately a lot of these microservices had this issue of strong dependencies where the reliability actually became the product of the reliability of the strongly entangled services this is actually called a distributed monolith and it's an anti-pattern in 2019 we refocused most of our efforts and we took a much more thoughtful approach on how we were thinking about microservices and there was this aggressive push to get everything out of main DB and into domain-specific databases we also created standards across the board I think the most significant that would be worth calling out is that we push for the adoption of kotlin across all of our services to give you some context in the early days it wasn't too uncommon for a service to be written in python or go or Java but adoption of these new standards allowed the team to invest heavily in authoring common libraries that solved common problems or shared issues that all of our services had such as providing out of the box requests fallback techniques load shedding better observability through things like distributed tracing and then overall what really emerged was this multi-layer architecture consisting of multiple front ends a BFF layer which provided functionality to the front end by orchestrating um interactions with the back end we had our core functionality or or back-end layer which is pretty much all of our microservices we had platform services that were leveraged by those core Services you can think of this as something like identity is a good example of a platform service and then the underlying infrastructure itself so you know things like databases databases caches cues everything that was sort of managed by the infrastructure team but I'll pass it to Anon to share a bit more about what microservices look like that he's maybe seen in what a typical adoption looks like for most sure Jay and so rightly mentioned here right like microservices allows your application to be composed of loosely coupled small autonomous services that can be deployed and scaled independently and it offers several advantages so first of all to scale the application you only need to scale certain components of the application and which optimizes resource usage now these micro services are Loosely coupled so they are not interdependent on each other and so they are can be individually tested so this makes the application more adaptable to changes over time they help break down this large confusing applications into smaller ones which the teams can specialize in so the development teams get productive more quickly they can deploy these Services independent of the other services each of the smaller Services is easy to test easy to debug now this kind of architecture also gives this team some technological Freedom as they are not tied to the same language or technology stack that the original Monet started with all of this translates into more agility for the teams so just as we saw in the doordash's journey here one of the ways to migrate to a microservices architecture is a pattern invented by Martin Fowler called the Strangler pattern and the main idea behind this pattern is you will start building new capabilities and systems around the current systems so both will coexist at the same time and this allows your migration to the microservices to happen without impacting the application and ongoing operations so essentially refactor the application in two ways add new capabilities as separate micro services and make the microservice call the monolith instead of the other way around and then take some functionality out of your monolith and make it as a separate microservice now one of the pattern to avoid here is building new capabilities on the monolith itself as as well as you know making the moderate application call your microservice as that introduces additional code on the monolith instead of shrinking it another pattern to avoid is going to granular with your micro Services approach where you use technology and infrastructure for decomposition instead use approaches like domain driven design for service decomposition now the technological Freedom that we were talking about means you can run some of your services on elastic compute Cloud ec2 for short which provides virtual machines with variety of instance types to suit different types of workloads and run them behind elastic load balancers you can run some of your services as containers using elastic container service a fully managed service which allows you to build deploy and scale containerized applications now let's say if you have kubernetes workloads you can take advantage of elastic kubernetes service which manages the availability and scalability of the kubernetes control play now let's say you have Services which are more event driven use AWS Lambda which provides the serverless event-driven compute and provides ability for running any kind of application without having to provision or manage servers then for the choice of databases you have choice of purpose-built databases starting with Amazon RDS the relational database service supporting a variety of database engine types and Amazon Arora which provides High availability and scalability with full MySQL and postgres compatibility if you are looking for nosql data stores you have Amazon Dynamo DB which is a fully managed service and serverless which provides you that nosql data store and for all other types of data you have Amazon S3 which provides 11 lines of durability and high availability now there are some messaging services on AWS as well like Amazon SNS Amazon sqs event Bridge you can assist data streams all of these help you to build those Loosely coupled architectures and then you can expose your microservices as apis using Amazon API Gateway which is a fully managed service which allows you to your developers to publish and secure apis now this microservice architecture is definitely promising right but there are several challenges that you encounter along this path first of all identifying the right service boundary so that all these Services operate in a Loosely coupled and in the autonomous way is challenging in complex environments in this system with each service managing its data independently across data stores the problem of data redundancy becomes real take for example data that might be stored for a particular transaction in one service that same data might be stored in another service for reasons such as analytics reporting and so on and the overall system would be in a consistent state only after all the services have performed their work now what was an in-memory call within your monolith becomes one that transits between processes on the network bringing with it concerns about latency performance delays additionally if you if you don't have any monitoring tools and testing Tools in place things can quickly get escalated out of control there are more services to monitor each service has its own set of locks and uh you know this file makes it hard to find the source of the problem so these interdependencies between Services needs to be closely monitored and any downtime of a service due to outages or upgrades can result in cascading Downstream effects building deploying and running microservices at scale requires you also to have consistent CI CD practices and each service will have its own parameters based on which they scale all of this adds to the operational complexity um I can say that we at doordash definitely went through a lot of these when we were undergoing hyperscale I think the most notable that was probably that's probably worth calling out it's just the sheer increase in complexity which required us to invest heavily in better observability so we adopted things like ebpf to monitor our Network calls and distribute tracing so we really understood the call patterns and dependencies of our microservices yeah so just as Jay mentioned a term called hyperscale right so let's understand that by looking at business in the form of a S curve the S shape represents growth over time starting out slowly picking up speed during rapid growth and then tapering off hyper growth occurs when the organizations compounded annual growth rate exceeds 40 percent which is much more than normal growth and Hyper growth businesses are the Envy of all venture capitalists however the sad truth is that most companies that experience hyper growth fail because their management teams are not set up to achieve scalability and handle some of the challenges that come along the way so for companies to uh you know maintain a sustained hyper growth phase over several years requires these companies to be constantly innovating launching new Services rapidly for their customers and improving customer experience they also want to make their application or platform highly available and performant so as to prevent any kind of Revenue loss these companies see a tremendous increase in the use of application or platform which requires them to have high level of scaling which is what we call hyperscape such hyperscale brings with it its own set of challenges and doordash has been one of those companies who has successfully navigated these challenges which we'll uncover in this session so what did hyperscale look like for doordash J yeah yeah awesome just to pick a few numbers that can provide a bit of insight into the scale and complexity at which we operate today we see an average of 150 000 requests at the front door a single HTTP request can often fan out into thousands of requests internally and today our engineering team manages a little over a thousand Services across 20 kubernetes clusters and one of the main takeaways that I can share from operating at the scale and seeing our own architectural texture evolve over time is that no matter how well something's architected something is a failure is always within the realm of possibilities it's uh literally a universal truth with anything involving involving humans so they should build with this in mind we dug in a bit further at doordash and we were able to bucket failures into three major types uh testing is the first worth calling out this is where a problem isn't caught in um early in the release process to avoid these types of failures uh we invested heavily in things like unit testing integration testing load testing performing Progressive deployments to catch problems early before they impacted our users the second type is around blast radius where there's a single point of failure in your system either due to some sort of shared state or strong coupling a good example of something like this maybe your Secret store or mini Services rely on it to retrieve secrets and if it's down then obviously all your services are impacted we're a t0 service where it becomes if it's unavailable it results in Upstream cascading failures to mitigate these types of failures we tried to limit the impact of a given change through domain isolation and putting guardrails in place like load shedding and circuit breaking to minimize the impact and then the last failure is is due to the failure type is due to complexity Uh Wood serum put simply is that if the system uh can get too complex for a single agent's model to be accurate meaning that it makes making much a mistake much more easier and uh making the system and overall harder to debug a non-um kind of curious what types of failures have you experienced yeah rightly said so when you hit that hyper growth phase the scale expected of your application reaches New Heights so when demand increases for the app all the underlying components have to be coordinated for scale and your app will start experiencing super linear scaling factors such as cross node lock coordination another challenge is that of deployment complexity when you want to add new features or capabilities to your application oftentimes we need to deploy multiple microservices in parallel and problems of bad core deployment and unintended CI CD config are not uncommon this can potentially bring your application down impacting all your customers now at hyperscale even your network is being stress tested this can introduce significant latency and timeouts if it's the network is not properly provisioned and configured and this can impact your online workflows resulting in poor customer experience while accessing the application your services might be relying on cloud Resources with regional limits or account limits which can quickly get escalated exceeded and you it's important to think ahead and plan for these limits for example the number of clusters in Aurora DB in a particular region is 40. and some of these are soft limits so you can adjust them ahead of time but then there are some which are you know hard limits think about ec2 instance Network bandwidth for example so in essence the non-linear scaling of microservices allows your application or takes your application closer to Hidden contention points this results in what we call cascading failures or timeouts which are harder to test this results in you know outages or downtime impacting all your customers and from that standpoint it represents a larger blast radius and if you remember some of the key goals of hyper growth such as Innovation uh improving customer experience preventing Revenue loss all of that can take a big hit and this is not to say that you know microservices in complex environments is a bad idea but it is so hard to get it perfect then that things can quickly get worse uh at doordash we considered all the previously mentioned challenges that both the non and I shared and we've accepted that failure was inevitable in fact we use this as a major design principle for how we're thinking about our next phase of architecture moving forward eventually converging on cell-based architecture which we codenamed internally project Supercell to give you a little bit of background in 2020 a single deployment of doordash its services storage everything served all traffic for all markets out of a single VPC in a single account where that single deployment failed it meant the entire doordash business was impacted project supercell sought out to add incremental multiple isolated deployments of our business to contain the blast radius of any potential failure and we had a few main requirements these main requirements were that we needed failure isolation from variety of failure scenarios whether they were caused by a user change a service failure or failure of a managed cloud service we also need to ensure that there was no overhead for our developers so they could continue to develop as usual without any additional friction and then we need to be able to ensure that we supported our existing microservice architecture along with the new supercell deployment which meant that we needed to ensure that Services were routable regardless of where they were running now let me pass it back to Anon to tell you a bit more about cell-based architecture for those who might be less familiar thanks Chief and so before we dive into project supercell let's understand this architecture and how it helps in addressing some of the challenges of hyper growth environments so this architecture is based on the premise that massive scale requires parallelization and this requires components to be isolated from each other these islands of isolation are called cells each cell is a complete independent instance of your application and has a fixed maximum size now your application can be comprised of ec2 instances with auto scaling groups or multiple containers or kubernetes clusters you essentially run separate copies of each of these in each cell and then your environment can grow by running you know uh by adding more cells to support additional users or traffic and again failure in one cell does not impact the others now do not confuse this with things like spreading across availability zones or regions which provides physical isolation at the infrastructure level whereas this isolation is at an application Level where the application um you know is at a logical level where the application environment itself are separated from one another and then finally you build a thin layer on top to Route traffic to these cells we call it the thinnest possible layer just to remind everyone that this should not be some complex logic but an efficient routing mechanism so let's look at some of the benefits of this architecture let's say there is a catastrophic failure in one of the cells the others are unaffected by it and so only customers served by that cell are impacted so in that sense they represent bulkhead units that provide containment for common failures on our Leos and which which means lower blast radius now not only is the blast radius reduce so is the probability of an outage since these cells have a consistent capsides kept size which is regularly tested and operated you should you know see a higher mean time between failures these cells are also easier to recover because they limit the number of hosts that need to be touched and analyzed for problem diagnosis and for emergency code deployment and configuration so you should see a lower mean time to recover now the cap size also allows for well understood and testable maximum scale Behavior so provides for better testability you can run your Canary tests in one of the cells or tests such as chaos monkey and without impact to other other cells now let's say you have version one of your application deploying all of the cells and you are coming up with new features in version 2. you can deploy it in one of the cells make sure it works and then roll it out to the others now let's say if the deployment goes wrong you can always roll it back without having an impact to the other cells so this reduces the blast radius from from the standpoint of problematic deployments as well and finally as a side effect it allows for a scale out rather than a scale up and provides for higher scalability now let's look at some of the concepts of this architecture one is of course the placement strategy which decides how customers or workloads are mapped to cells router is the component which handles the customer traffic or request and directs it to the right cell and then this architecture benefits from maintaining a maximum size of a cell and you want to decide on the cell size based on predetermined scale of traffic let's look at each of these in little more detail so the placement algorithm is a foundational component it keeps track of cells their configuration and how different customers or workloads are mapped to them and it requires some uh you you could use for example market segments or customer accounts as a mapping criteria and it requires some thinking about you know how you want to partition your data or traffic based on the business context and it could be different for let's say b2c applications versus B2B applications but the key thing to keep in mind is that is that you want to partition based on the grain of the service that allows for complete isolation now your placement strategy will also have a bearing on the growth so as you add new cells to your environment it will need to keep track of that now when customers access the platform they should be able to conveniently um you know route it to the correct cell end point this function is handled by the router now this could be an out-of-band Discovery mechanism where which allows the clients to figure out which sell endpoint they should connect to for example you can achieve this using Amazon Route 53 a highly available and scalable DNS service which provides several important features for the routing layer such as distinct domain names for cell routing policies health checks based on which you can redirect to a failover cell and so on or the router could be a proxy service like Amazon API Gateway which frontends the request and then directs it to the appropriate backend cells in this case the request of the parameters in the request itself can be used to determine the routing path and clients will be completely ignorant of the backend cell design routers should introduce minimal latency ideally in microseconds and should not have too much complexity and again it is a component which is an important one from an overall availability perspective and should not itself become a single point of failure coming to sizing it's an important consideration to determine based on your routing criteria so um more smaller cells means smaller blast radius it's just easier to test easier to diagnose and resolve issues because there are fewer log entries to query fewer host to log into and so on and just makes it easier to operate at an individual cell level on the other hand if you have few large cells it provides for better capacity utilization you can accommodate big customers as in Wales into those large cells and then just makes it easier to operate at a system level now here is an example uh application which is spread across multiple availability zones to take advantage of that ha architecture and this is how a cell based architecture will look like each cell providing the full capabilities of the application and they are completely isolated from one another and there is no intercell communication so the router is the only component that knows about the underlying partitioning now there are several ways you can partition if you want to partition or segment based on geographical location you can use geobase cells or if you want to do it based on customer accounts you can use some kind of a hashing algorithm to map those accounts to specific cells for efficient allocation and utilization you still want to ensure High availability within a given cell so spreading your application and application components within a cell across availability zones is highly recommended as you all know Amazon virtual private cloud or VPC for short spans across multiple availability zones and probes and can serve as your cell boundary it doesn't have to be a single VPC though your cell can comprise of multiple vpcs and this might be determined based on your VPC sizing based on the application needs or the isolation that is required between different teams and their application components so you want to separate them out in different vpcs or just separating out business logic components from data layer components and so on PPC peering provides the low latency networking connection between vpcs and allows to Route traffic between them as if they are on the same network and it provides a really good Network performance to support this architecture now one of the best practices to have separate AWS accounts for your router as well as for each cell AWS accounts provides a natural isolation boundary so action in one AWS account does not impact the others it also reduces the risk of hard service quota limits which are mostly at the account level you can use AWS organizations to create that multi-account environment to support this architecture and then there are also some services like AWS control tower which makes it easy for you to set up and govern a secure multi account AWS environment or the landing Zone and it brings in ongoing uh you know account management and governance best practices that we have learned over the years by working with customers as they move to the cloud so now you can have multiple cells within a given region and have an ability to interconnect them using AWS Transit Gateway AWS Transit Gateway is a highly scalable and highly available centralized routing Hub and simplifies your network topology and it need not be for your online customer workflows but can provide an out of band uh you know path for your devops teams to get into these cells for troubleshooting for deployment and so on using services like AWS Direct Connect site to site VPN allows you to connect from your corporate Network or your on-premises data centers into this architecture remote Workforce can use AWS client VPN as well to connect to this architecture now this Transit Gateway Hub and spoke um you know model can enable connectivity for other offline use cases let's say you have a data platform environment which needs to connect to these cells to capture all the data or query all the data uh for building use cases such as for analytics and machine learning and so on now you can take it to the next level where you are building cells across multiple regions this could be for reasons such as you know survey or having cells closer to those markets where uh for obvious benefits of performance and latency improvements or purely for scaling out to adjacent regions for higher scalability or to get past Regional limits on cloud resources it could also be for regulatory reasons such as data locality while serving certain markets but keep in mind that this is not to represent active active environment across regions as this architecture again relies on complete isolation between cells with no dependency between them and there are some good Global AWS services that can Aid in this design we talked about Amazon Route 53 which is our DNS service with variety of routing routing policies which allow you to Route requests to a sales DNS endpoint across regions there's Amazon Cloud front which is our content delivery Network which allows you to securely deliver content with low latency and high transfer speeds using Amazon Global backbone it uh it allows you to serve content from multiple Origins using a feature called behaviors which can determine the routing path and then there are functionalities like cloudfront functions Lambda at Edge which can be used to write custom routing logic to map to the right cell endpoint now the main goal of a cloudfront CDN is to Cache content and serve it from Edge locations but let's say if your application requires users traffic to flow all the way to the backend application but still get good uh you know latency performance uh you can use AWS Global accelerator Global accelerator is a networking service which improves the use performance of your users traffic by up to 60 percent by using the global Network infrastructure when internet is congested it can optimize the path your user's traffic takes to your backend application using anycast technology and it keeps the packet loss Jitter and latency consistently look it has a feature called custom routing accelerators which can again be used to write your routing logic and map your users to specific endpoints or of cells behind the global accelerator now there could be scenarios where let's say a cell goes down and you want to redirect to another cell to serve traffic from that other cell and this is where Route 53 application recovery controller can help so application recovery controller service helps you to make sure that your applications and components within a cell are ready to receive traffic and then helps manage and coordinate failover using Readiness checks and routing control features so essentially you will be able to make sure that the new set is ready to receive traffic and then shift traffic to that cell using this service so as you can see here there are different ways you can you know design and implement this architecture you may also consider some homegrown implementations or third-party solutions to come up with this design AWS provides a good set of tools for you to uh you know think about this design and implement this architecture so now that we have looked at this architecture um you know let's look at how do Dash has implemented this to support the hyperscale growth uh thanks and on I think a good starting place would be to first look at our Global cell it's worth calling out that this is basically our existing microservice architecture we just renamed it the global cell and services yet to be migrated to supercell live there by default it includes our monolith which is running in kubernetes these days main DB various other domain specific databases as well as a handful of kubernetes clusters that house our Global Services all of this is deployed across multiple azs and we introduced a service and internal router to support our various routing needs as part of project supercell this is all deployed to our main account where everything had been previously deployed since literally the beginning of doordash here's a look at one of our cells that's part of supercell it's pretty straightforward it's uh basically some storage some compute this time we have a single router our service router we should we includes console for Service registration Discovery as well as a handful of kubernetes clusters some domain specific databases that are still shared across ourselves today and then of course we use transient Gateway and peering for different use cases which we'll get in a little later so looking at the VPC networking and connectivity for user access we use Transit Gateway alongside the AWS client VPN for accessing various internal resources for service to service networking we use peers to avoid any additional hops that might introduce unnecessary latency we ensure that there's no network path between ourselves and though it may appear that routing might between them might be possible because there's a Transit Gateway attachment we make sure that all the routing tables are configured as such where we restrict any cell to cell traffic on the topic of cell size and allocation something that Anon mentioned we began thinking about how we were going to break up ourselves in terms of what gets routed where and naturally for us markets were a perfect fit for this our business is made up of multiple markets that are distributed across multiple geographies and determining them is pretty straightforward for the most part um because we can use something like deliver the delivery address to determine a user's Market in which they preside the market is returned to our various clients using BFFs which we'll get into on the subsequent slide but I want to call out a little more about why markets um as one of my guests markets some markets are smaller than others in terms of water volume additionally we see order volume surge for different markets at different times of the day considering these Pro properties we distribute the order load of the markets evenly across three of our four cells and we keep our first cell intentionally smaller to ensure deploying to it always has a undersized impact having different peak times actually lends itself as a useful property to perform Progressive rollouts to markets during different times of low volume as I mentioned on the last slide um we rely heavily on BFS to return our user's Market ID BFF for those who are less familiar stands for backend for front-end it's basically a proxy between our clients such as the mobile app is a good example and a back-end service so maybe like the consumer service a typical flow looks like this the client calls the BFF first for instance the mobile client calls the mobile BFF then the BFF calls the consumer service internally the backend service uses something like the consumer's home address to determine which Market they reside in and then it Returns the market along in the response to the BFF and subsequently onto the client that's that's a cookie with that market ID now I've given everything that I've shared thus far let's take a look at what a typical life cycle of a request might look like um first when the client makes a request it typically goes well it always goes to our CDN and it's likely at this point um that first request doesn't have any information like um the market that it request belongs to the CDN worker then round Robin's request to one of the four service routers residing in our cells if the request is a cell service we'll get into other types of requests a little later um from the service router from within the cell the request continues on to the BFF layer which interacts with the appropriate back-end service to determine the market that the user belongs to the market ID cookie is then set diving into this in a bit more detail again the request is made to our CDN our CDN is used for various reasons DDOS URL rewrites load balancing but I think the important one worth calling out here is that is the worker functions that act as middleware for handling which service router to route a request to based on the market ID so first we check for that market ID cookie and check if it exists if the request is missing the cookie again gets round robin to one of our four cells so in this case maybe the request goes on to sell two and as I mentioned the request then goes on to the service router which is an ALB where the underlying instances are running on both Envoy and console console in this case is actually to support service Discovery is actually for service Discovery purposes and it uses um it's hosted using a specific domain that that Envoy uses for its lookups not pictured and um and important for how this whole thing works is that there's a catalog sync running in each kubernetes cluster in the cells their register services to console when a service comes up in the cluster making it available for service Discovery Envoy then of course uses service Discovery like a service Discovery domain like I mentioned for static V host entries for each cluster to resolve a given workload into one of those kubernetes clusters the actual Envoy hidden fig itself is pulled down periodically via runtime Daemon on the Node um from S3 let's say the request in this case is trying to resolve the consumer BFF well it uses the console DNS server to First resolve the request and we return an IP for that service the whole return in the IP thing is only really possible because we operate a flat Network by leveraging the AWS VPC cni in kubernetes which basically means that every workload in our cluster gets a resolvable IP address now that we have an actual IP the request then flows on to the consumer BFF and that BFF then interacts with some back-end service to determine the user's Market ID usually using something like home address like I mentioned earlier so then on subsequent request we now have a market ID cookie when the request arrives at the CDN and the CDN then takes out requests and then routes it to this the specific service router based on that market ID then the service is routed to a workload running in the kubernetes Clusters in the appropriate cell um and then of course looking at this in a bit more detail the request arrives at the CDN there the worker function performs a cell lookup based on the market ID once we know the appropriate cell the worker then knows which service router to send the request to this time in our example let's say that our Market ID cookie is set to one two three requests for Market one two three are sent to the service router in cell one and um and that's basically it in some cases there may be requests that are made to a service that has yet to be onboarded to supercell and it might still live in our Global cells for these they're configured AS Global Services in our CDN and the requests are sent to the service router running in our Global cell from there the surface router then routes the request onto one of the kubernetes Clusters running in that Global cell let's say that we're trying to resolve I don't know a hypothetical global Service called The Food Service based on DNS records configured within our CDN the request is then sent to the global cell um it then goes to the service router running in that Global cell which like the cell service router is running as an aob with some instances running console and Envoy um basically identical to our cell service router to resolve Global Services and the global kubernetes clusters of course there's still one more scenario that's worth calling out it's that's when um requests need to be routed to our monolith unfortunately there are cases where this is still happening so the request first goes to our CDN and then on to the service router um as usual in our Global cell the big difference here is that the service router in this case is configured to send any request to the monolith onto what's called an internal router from internal router there is a configuration where the request to the monolith are sent to a service type load balancer managed in kubernetes where our monolith is running and here's a bit more detail and more rather a look at that routing the important thing here to call out is that there is an entry in the global Service router where it knows to send certain requests to the internal router and then the internal router has some endpoints that are configured for the monolith that sends those requests to the service type load balancer for our monolith the internal router is also used for properly forwarding requests onto the appropriate service router so let's imagine you have a global Service that needs to resolve the cell service if first sends that request to the internal router then the internal router basically forwards it along to the appropriate service router um in the in that example it would send it on to the cell service router and the supercell VPC I'm fine as we've seen thus far we operate a global Service cell and four supercells so I know it's a bit confusing so I wanted to share a bit about the considerations that are made when we're considering onboarding a service to supercell and which sort of stay in the global cell the first thing that's important to understand are our services dependencies so basically what are our services calling and where are they running the reason this is important is because we need to make sure that the network paths are in place for our services to to be able to communicate and and that traffic can go from one VPC to another so there's no disruption when the service gets migrated the other thing that's important to understand is the latency impact of moving your service the last thing that you want is you want to is moving a service uh incurring a bunch of unnecessary latency that impacts the overall performance of of your system because most of its dependencies are not running alongside it so like let's imagine I move a service to supercell but most of the things that calls are running in the global cells or maybe it's sort of all over the place it's going to result in a bunch of unnecessary traffic hops between the global and supercell cells and also there's obvious cost implications to this traffic as well some things that are sort of out of scope first for supercell are things like Singletons where you don't want a bunch of them imagine workloads that aren't item potent or schedulers they're just not great candidates because you're not going to run multiple copies of them so they'll always likely live in a single cell and for us that's the global cell lastly tier is the absolute most important thing when we consider onboarding for services to supercell we focused on tier zero Services as they are the services that are required for our business to operate and really what we cared most about with project supercell is ensuring the business operated and we had the ability to um to run multiple copies of the entire business I want to chat a little bit about deployments and what a typical deployment looks like so here's a quick look at that um so if it's in supercell remember again that the first cell is intentionally smaller so when we do a deployment there's a much smaller impact when we're deploying to cell zero one first we do a canary deployment to cell zero one we follow that up with a canary analysis make sure everything looks good if it does look good we proceed on to a blue green cell deploy into that cell where we basically run the old and the new version of a service in that cell and gradually shift traffic over from the old to new while maintaining the old for potential quick roll backs if necessary if all looks good we then continue on to the remaining cells and that's pretty much it and honestly this is the exact same strategy for a global Service cell all right finally I want to wrap things up and I want to talk about resilience so let's imagine a cell faces some sort of hypothetical failure it could be a runtime change maybe it's some sort of bad change to a c grid maybe a deploy just goes wrong resulting in cascading failures the first thing to point out is that the impact is isolated to that single cell only markets that reside in that cell are impacted the other three cells are operating completely as expected with no issues once the outage occurs the first thing that we do is we shift traffic over to the other three cells basically we remap all the markets to those other three cells and we remove um that particular cell from around robinpool to ensure that no requests are being sent to it once we're ensured that traffic is completely steered over from the the bad cell the team then determines and resolve the issue within that cell and finally after everything is fixed the cell is re-enabled with the markets map back to the cell as well as it being added back to the round robin pool all of these operations are driven by a CLI that we have internally it's called healer all right um as I mentioned much earlier in this talk the shared storage thing is kind of an important thing or rather a shared domain specific database is a very important Point uh some of you might wonder why we're able to get away with this steering of traffic between one cell and the other three healthy cells in the previous example and this is primarily because supercell is a multi-phase project for us the focus for phase one was all about isolated compute getting our foundational networking routing in place while still maintaining that shared storage uh for us this means that software deployments can now be faced across multiple cells and with the shared storage model we're able to fail over markets to Any Given cell if necessary the shared storage model will obviously be the next thing to go um in phase two is we don't want that to become our single point of failure so if you had something go wrong with one of those shared databases it would literally it would impact all of the cells today so in phase two the focus is on data sharding where each cell has a subset of multiple markets data for each domain specific database so we still get the benefits of having multiple cells that conserve uh serve a given Market though it is worth calling out that phase one alone offered a lot of benefit in tackling the compute problem first and deprecating our monolith was important to us plus sharding stateful systems is expensive and um and and so you know we kept that in mind too when we were thinking about phase one uh our last phase phase three for us will be literally just getting this entire setup deployed to one more region to avoid um the very unlikely Regional outage uh one thing I can say though from the overall experience with this new architecture is that it's been great it's the ability to re-steer traffic to a healthy cell feels almost like a superpower um plus Engineers have this improved confidence iterating on changes and shipping new features knowing that the impact of their change is is minimal and that if necessary we could steer traffic or disable entire cell and then steer traffic to the other three and we've also seen this drastic decrease in mttr as well as an increase in Dev velocity across the board since the beginning of supercell though overall things have been great um it hasn't been without its own handful of challenges here and there though most of these came from the complexity that we accepted in the short term to support our monolith or Global and our supercell deployment the first notable challenge was around onboarding we honestly expected folks to onboard much quicker than they did but various service dependencies made this more challenging than expected initially we also saw higher than expected latency across our from across our cell DPC to our Global VPC so we invested in tooling and observability to identify bottlenecks in our Network paths we moved all these Network fast appears which greatly improved our latency lastly the router has somewhat became this um really the single point of failure and and fragile to the overall system so config changes need to be rolled out slowly and properly validated for us the journey isn't quite over um and we're still working on various initiatives around supercell a lot of which I've already sort of shared at this point as I mentioned these include data isolation per cell simplifying our overall routing setup and there's actually this opportunity to remove a internal router once a service mesh is fully adopted and then investing in improve investing in tooling to improve the overall onboarding experience and or rather minimize the number of PRS that Engineers are having to open across a bunch of repos to onboard their service so the what what the way Cloud's tackling this is through sane abstractions for folks onboarding to supercell where they only have to go to one place to get their service onboarded we're also working on ensuring that multiple markets can continue to be served by multiple cells and then finally um I keep saying it but we want to deploy this whole thing to multiple regions um and that's supercell in its brief entirety I'll pass it back to Anon for some parting thoughts sure thanks a lot Jay for sharing this journey with us and so based on this journey I mean some of the best practices or considerations that come to mind and I wanted to share with you so first of all as we discussed cell sizing right so more smaller cells means smaller blast radius so it always goes to approach the design from that angle it's faster to diagnose resolve issues with smaller cells easier to test and you can leave more room to grow if needed right the other thing is ensuring that you have a good health check for each cell so building you know cell level metrics like you know calories metrics alarms dashboards at an individual cell level so that you understand the health of each cell is is pretty important now of course from that standpoint the other consideration is it does increase a little bit of you know the operational complexity and then of course you are going to deploy to these cells in a staggered manner so the deployment duration might also increase so please take that into account in terms of the operational complexity May make it part of your operational Playbook and so on now one thing you might have realized from the cell router perspective it can become a single point of failure so don't think of it as a single entity trying to distribute traffic to cells but rather take advantage of some highly available highly redundant services or managed services to to implement that you know router now coming to the last point the cell migration and cell migration essentially means moving your workloads or customer traffic to another cell and this is a complex problem because it does invert data migration and it's okay to do that especially for scenarios like failure recovery where you are one cell is down and you want to redirect traffic to another cell but then there could be other scenarios where you can think about cell migration let's say there is a there are one customer or a few customers occupying the cell which is like overloading the cell and you want to move them elsewhere or you want to do some kind of optimization in your cell design so you want to move certain data partitions um to another cell so don't boil the ocean on this one as um you know it's not going to be a problem for on day one and since you are adopting the cell based architecture and you still uh you know going with the strategy of ensuring the right cell size and so on the problem of you know One customer or few customers overloading yourselves is not too high all right so with that I want to come to the final parting thought and like our city of Wonder vogels mentioned everything fails all the time and you can you can't control that failure but you can definitely control the impact so as we discussed today hyper growth you know companies face challenges of scale deployment complexity and resource limits this makes it hard you know to deploy new features quickly while maintaining uptime and has an impact to revenue and customer experience the cell-based architectures that we saw today has some promising characteristics and you can to enable you know the failure isolation and get the benefits of scale out also it is not an alternative to micro Services architecture rather a way for you to you know approach your micro services with agility and confidence while ensuring that you know impacts of failure are minimized so given the concepts that we have learned today and some of the learnings from the Doda story I strongly encourage you all to bring in that thought process and choose among the various services available to come up with this architecture for your environment AWS provides a good set of tools and services to help you with this design as we discussed and here are some resources for you to look at on those similar topics starting with you know the cloud native architecture series there's a good blog series that you can take advantage of a reliability pillar of well architected provides good information on the resiliency Focus as well as the resilience Hub as lot of useful resources and then one of the blog which talks about the fault isolation handling within our own AWS services and how it uses some of the cell based architecture principles someone all right so with that um on the last topic I want to point you all to some related sessions on similar topics which were part of AWS re invent 2022 they should be available on our YouTube channel of AWS I strongly encourage you all to take a look at those and with that from both me and Jay thank you for your time and we welcome your feedback thank you thank you so much