- So this talk is
building production-grade resilient architectures with Amazon EKS. In short is building Amazon
EKS cluster with resiliency. My name is Carlos Santana. I don't play the guitar. I always have to make that joke. (audience laughing) So you can go to the House
of Blues here in Las Vegas, and here's some Carlos Santana music. But I'm a senior specialist
solutions architect in containers that includes EKS and ECS. And with me is Niall. He's a principal specialist
solutions architect in containers, both on EKS
and ECS, pretty much, right? But for today, we're going to
be talking about Kubernetes running on AWS using EKS. So the agenda for today, we had a lot of topics to choose from. We one hour of a session, we had to choose some
aspects that come together in a story around platform
engineering on top of EKS so we choose to talk about management, observability, and governance. So that's what pretty much
what we're going to talk today. Talking about platforms. Everybody starts building their platforms and builders, right? These are the folks that are in charge of the cloud resources
in the organization. And what builds a platform
is building for teams. They're organized by teams. And then you have your applications that you run to run on AWS and then you have your infrastructure. So pretty much, it boils
down to three aspects. You have teams, you have applications that you have to put those
applications somewhere in the infrastructure. Talking about platforms, the idea is that these platform
engineers build abstractions and they accelerate how they
build these applications on AWS providing cloud infrastructure
as easy as possible with autonomy to those teams. This is a stat that by
July, last July of 2024, there's more clusters being managed by AWS in this terms of EKS. 33% growth year over year. So the amount of clusters
that folks are configuring, deploying, and managing and upgrading, it's increasing year by year. So there's a stat that's in there. And what are the challenges that platform teams have when they come into managing this cluster. So today, we're going to
look into three areas. One is this life cycle of
the cluster from deployment to upgrade on how to develop a
consistent ecosystem process. The next one is enforcing best
practices across all clusters by not monitoring them
and also the applications. But for today, we're going to concentrate more of the clusters and the system wise software
that runs on those clusters. And last, providing guardrails
to support the teams that need these clusters and giving them the right
amount of autonomy, right? They need a cluster, they're developers, but they need guardrails. So let's start with the
first topic of management. And this is how it looks like. We generally see customers
with a repo Git resource. Some kind of infrastructure
as code tooling, like Terraform, CloudFormation. I heard somebody here in the
audience talking about CDK. Pulumi and others infrastructure as code that the state lives in
a object store like S3. And then they will deploy a cluster. Then they will use the same
infrastructure as code tooling to deploy Kubernetes resources, saving the state of these
Kubernetes resources in the same location at,
for example, the S3 bucket. For Kubernetes resources,
this might be pragmatic, but for simple cluster
management, not a problem at all. And then, things starts growing. Then you as a platform engineer,
you start getting requests as that teams need another EKS cluster. So you copy paste, right? That pipeline move it on,
and you get a second cluster and create a new pipeline. Then it gets to this level. So raise your hand if
you have an organization that it looks like this. Having a lot of pipelines,
deploying a lot of EKS clusters. It grows, the growth continues. This becomes unmanageable. Too many versions of the pipeline because you're copy pasting to track which clusters is up-to-date. These without thinking what is inside of each cluster, right? The resources and application. It's not uncommon to see many
pipelines in our organization. So this is a lot of snowflakes, right? Each one individually, you can say that it's an cluster, but you look closer, they have a different version. So what are the application
they have inside? And each one has an old version of a variation of a pipeline. So what are the key challenges to restate in terms of management? When you have unmanaged
growth of clusters, what are the some key challenges? Enforcing standards across
your fleet of clusters. So now we're talking
about fleet of clusters becomes hard to automate
those deployments. When you have automation, how do you configure a
single source of truth for all of them? Out of management becomes
something that gets out of control with so much open source projects. The team cannot be an expert
on every add-on out there that you put in your clusters. And lastly, workload management. How to match the best
instance type, in case of EC2 to a workload? And also do cost
optimization on top of it. So this is something that
we see from the field. Me and Niall, we work on
the field with customers, probably some of you. And we work with customers
that are trying to configure and deploy EKS clusters and
building kind of like a pattern or how do we serve these teams
that are in the organization. So we see a lot of ways of clusters that are provisioning organizations. Templates as a service,
this is in the website. It's used by many platform teams initially since it's a low barrier to entry, like the platform team will
be, I create a template, give it to the dev team, and then they will create a cluster. But this becomes hard to manage a scale. Development teams guess what? Not to upgrade their clusters. If you are in the platform
team dealing with EKS clusters, you know that you need
upgrade them, patch them, and release new updates. It just give them that
template to the dev team. They will just run it once and then never go back to
upgrade those clusters. Platform teams taking more
ownership, providing a service. So let's take a look at what we advertise. So the first one is customers
cluster as a service, giving a cluster to a team, but the platform team
managing the cluster itself, making sure that they're the
ones upgrading the cluster, upgrading the add-ons. Another one is namespace as a service. So namespace as a service is something that the team would have a discussion or maybe a form that asks the team, like, how many resources do you
need in that namespace? So the platform teams
can set resource quotas and Kubernetes that are
APIs that you can configure or maybe limit ranges or
default for request and limits. And that way, you get a namespace created by this platform team. And then the team can
deploy inside of them. And the last one is maybe the
team, the only team that needs to provide to the platform team is give me an application,
I'll give you a ripple. And they don't know that their application is running on Kubernetes. They only care about what
getting their application on Git. And then the platform
team stays care of taking that application, building the containers and deploying the application, so the team gets isolated from that infrastructure management and the team only focus on their app. So we take that last one
is very, very familiar to some of you, who here is using GitOps? Okay. So you can relate to what are
the benefits of using GitOps and how I can manage the methodology we can build a platform on top of it. By using the iterative configuration,. it reduces complexity, right? When using Git, we can track changes and enhance level of visibility, right? So we can even roll back. When you have security standards that are put automatically
increase the security. So this is something that
we have seen in the field with some teams that once
the security teams know that they can use GitOps
to have their software or policies automatically
on all EKS clusters that actually start learning GitOps. And that's something that
I learned from the field. And when you want to be sure that if someone makes a mistake, makes a mistake making a change somewhere, it can automatically reconcile
that change back, right? Roll it back automatically. So in GitOps, it always
reconciles the desired state to the current state. That's if you have automatic syncing. There's different ways
of configuring GitOps. So you can have manually or automatic, it depends on your organization and the environment that
you're working with. If we're going to deploy and upgrade an EKS cluster with GitOps, let's take a look at what it
looks like, bill of materials. At high level, there are three major areas to consider when you are
configured in EKS cluster for your organization. There's many more as some of
you already working with EKS. But for this instance, we contemplate talking
about three of them. The first one is control plane. This is the thing that like
configuration of the EKS service that you can configure with the AWS API. Things like do you want encryption
on your FCD, for example? Or you want to configure
cluster access management, which is a recent feature of Amazon EKS that some of you were
maybe using the config map. Now you don't have to use the config map. You can use cluster access management and that you can configure
through, like I said, the address API of EKS. Next one is pod identity. Maybe you were using
(indistinct), things like that that configures the service
account with an annotation. Now we have a better or a new approach of doing
it with pod identity. That that configuration, you actually do it at the
address API on the EKS service that you can associate a service
account with the IAM role. And most important is upgrading
the version of Kubernetes, like going from EKS 1.30 to 1.31, upgrading the minor release. That's something that is
available at the control plane. And that's a lot of considerations to take as I mentioned a few, that you will have for that bill of materials
that at some point, you'll be configuring that cluster or many cluster at the same time at that level of configuration. And then it can be like
one unit of change. The next one is the data plane of workers. In Kubernetes, they're called worker nodes where you have, for example,
your tuple, your container ID. And now with Karpenter, we
have much better control on how we configure the EC2
instances using Kubernetes API. So when I meet Kubernetes, API, it's usually your helm
chart or YAML files. And we see a lot of organizations already have their Karpenter helm chart that they can create their
classes and more importantly, the no pools based on themes
that they can configure. And those helm charts usually
are deployed through GitOps. That's a better management
that allows the team to configure the no pools
that can match the workloads that when they deploy
the pods, for example, that request the EC2 instance. And the last one is the add-ons. And this can be add-ons
that were provide by AWS, the EKS add-ons and other type of add-ons that are systemwide software that you deploy on your Kubernetes
cluster, usually package as a helm chart that come from open source or can come from a AWS partner and vendor. So if we are going to rely on GitOps for tooling for fleet
management, we can have a process that will constantly try to
reconcile the source of truth with the actual state of the cluster. That bill of materials that we discuss, we can express as Kubernetes files and Git in different... Specialists have visibility and control over the
definition of the dependencies. So we have here like
applications, infrastructure, policy and security all configured in YAML files in this
respect for this slide, we're just going to name it
cluster YAML for simplicity of the workshop, of this presentation. So once you have your
configuration that you have as like I said, the different areas of that building materials, you're changing the control plane, you're changing something about Karpenter or you're changing something about add-on, you have that in Git. And we can have a GitHub
agent reconciling. Our Gits agent in here. This is an icon of Argo CD. This is a CNCF open source project that many customers actually
use their other GitOps tools that reconcile these resources
into the Kubernetes cluster. On this cluster, we can have
cluster Kubernetes controllers like ACK. Have ACK there. ACK stands for AWS Controls
for Kubernetes, a tool that is open source and
is fully supported by AWS. This is the EKS team providing
this open source project. So if you have any issues or you have feature requests,
you can just open a Git. In the Git repo, you can open
an issue and request that and the team will look into it. And also we have a channel
in the Kubernetes Slack. So for example, now that we have ACK, ACK could be something
that can configure we Git, that will get you an Amazon EKS cluster. So this is the first layer
that I was talking about, about the control plane. Then we have a kind of inception of like deploying Argo CD again inside the Kubernetes cluster. And that Argo CD is deployed
there to have some isolation. That way, that Argo CD can only care or handle that Kubernetes cluster. And as an agent, that it
sits down configuring that. Then the Argo CD is the one
managing that EKS cluster. So it would deploy the add-ons. For example, if you have
open source Karpenter, we deploy a Karpenter that
will give you those notes for your workloads. And if your workloads needs
access to an AWS resource, for example, an S3 bucket,
you can have ACK install inside that cluster to deploy the S3 bucket that your application needs. So this is a typical architecture
that is used by folks that are using GitOps to
configure both the control plane, Karpenter for the data
plane and then the add-ons. And this way, they can do
everything defined in Git as as YAML files
declarative configuration. So let's talk about now a little bit more about resiliency, right? So now we can build a
cluster, we can use GitOps, we can use open source
tools, we can use AWS APIs, we can use open source tools like ACK. So we have a lot of clusters, but without velocity, as you heard before, many organizations have multiple clusters for different reasons. It could be for different
regions, different teams, different AWS accounts. Without velocity, they fall behind, cannot upgrade everything serially. It will take forever to make one change if they have to upgrade
everything serially across. Some organizations have
hundreds of EKS clusters. Upgrading in batches, however, it needs safe cars in place to ensure resiliency and availability. So this is kind of like the
misconception that people think that they can set the desire configuration and suddenly, every cluster
around the world gets updated automatically at once. That's not something
that is best practice. It doesn't give you high resiliency. So consistency of components
and versions help and scale. So this is how EKS team presented last year in reinvent 2023, we have massive amount of EKS cluster, like I mentioned in the stat. Year over year is growing, the number of clusters that we manage. For example, when there's a patch released for the Cube API server. Cube API server is a component that runs on the control plane
that AWS takes care of that, that you don't have to worry about. But we have to patch it. If there's an upstream, what I mean upstream is the
upstream kubernetes project. There's a patch that needs to be rolled out across all
our e cluster around the world. That's something that
we take very seriously. But like again, we have velocity and resiliency both
together on that balance. So grouping clusters in cells
help manage the blast radius. We have the bake time between waves, we call them like one wave of a number of cells that get upgraded. And this time, that is the bake or soak time that you people refer to, decreases as the amount
of cells increases. And we have different levels
of tests that are done between every wave of
waiting for that soak. And every cell could be
about a dozen clusters. And there's a YouTube video
about this talk online. So applying this pattern to your own EKS clusters
in your AWS accounts in your organizations. We can have some type of fleet management that we can have updates be rolled out across your organization,
but in the resilient way. So how this looks like, obviously, depends on your organization. And in this example, what could be a cell? A cell could be one units of work, it could be one cluster for example. And to explain this concepts, we're going to use like
one cell is one cluster. So how many cells you can do in one wave? Well, it matters managing
the velocity and resiliency. As you progress, the
confidence tends to increase. So for example, as you roll out one of the add-ons,
like I mentioned, right? Or you're changing a Karpenter
class, a configuration that is used across all your e clusters, you have more confidence as you start patching those clusters. And then you diminish the amount of time that you wait for those. And you can pick up more
cells as you upgrade. So in this example, we have waves sandbox. That is the one cluster
that the platform team uses for testing their change. If you are doing platform engineering, usually, you call them a sandbox. And you can have waves on
one single environment. In this case, we're showing
different environments. So we have dev and then you do staging. So in staging, you do more clusters. In production, you can have
not update all clusters in production as you increase
that, but you can divide how many clusters you have
in production to update. So in this case, we have
prod one and prod two. Maybe they're divided between regions or maybe they're divided
by address accounts, it really depends on your organization. But the idea is as you
dynamically increase the number of cells on a wave, you
decrease the amount of time that you wait for that. So in practice, let's take
a concrete example of this. So we already talk about
the cluster that TAML that defines your bills of materials. If you need to change
something, for example, upgrading an EKS cluster,
you will push that into Git. Then you have a rollout. And then this rollout could be a pipeline, a custom pipeline that you write. It could be a process
that you follow manually or it could be a combination of pipelines and manual process that you use something like a ticketing system to
follow the change management. But we're using a tool like
GitHub, we can use Argo CD to orchestrate that rollout with Argo CD. So we would take wave one,
you would do your checks. So before upgrading, you
can automate the checks to see like, can I do the upgrade or not? And then, once the check pass, then you actually do the update. After the update, you will have your test. And this test could be
related to the specific change that you do or it could be a test that you always do regardless, what is the change that you did. So the change could be
at the control level, a control plane level, it could
be at the data plane level, like a change in Karpenter
configuration of Karpenter, or it could be one of the add-ons
for example the BPCC and I or any open source project
that you have as an add-on. Then we have the soak time. And the soak time is the amount of time that you're going to wait. And in here, you're
going to start monitoring of if things are happening,
like give it enough time to monitor that the system is healthy to move to the next wave. What's the next wave? Wave number two. In this case, we increase
the amount of clusters by known number of cells when we do the same exact same test again, right? We check and we test and we soak. And the amount of time
that you wait it decreases. And it continues. And then that you're
orchestrating with Argo CD. So let's take a look at
if you are using Argo CD or could use another tool, but in this case, Argos are
going to take a concrete example 'cause people like to
see concrete example. So this is using Argo CD and we're going to look at what it means to check or the precinct. So Argo CD has a system that you can configure
precinct hooks that you can, it's a Kubernetes resource
with a specific annotation and things that you can do here is gates. A checking that, for example, using Amazon EKS upgrade insights, there's an API that you can
call to see what are the things that you need to do or if
everything's green to go ahead and continue with the upgrade. If not, you will stop. The other one could be set up. Something that you need to set up before you actually do the upgrade. Could be a migration to
a database, for example, if it's for a workload. Or it could be something
to configure secrets that they need to be there, or it could be another add-on
that needs to be healthy before you continue with the
upgrade of this next add-on. And the prereqs could be CRDs. CRDs that need to be installed. So you always have the
same precinct hooks. Doesn't matter if it's a upgrade
or maybe it's a deployment. So people use the same hooks. Then it goes to the actual
deployment, which is Argo CD to do the dynamic upgrades. And then this is where Argo CD
talks to the Kubernetes APIs and you leverage that. And Niall is going to talk
more about those hooks. And but then after the hook,
we have to do our tests that we mentioned, right? So Argo CD also has a posting hook that you can do some
validation on the environment, like some functional
tests to see the health of the add-ons are correct, maybe the EKS upgrades are correct. Maybe going back to Argo insights. You can also do AC tasks
like launch a pipeline for example or something in Jenkins. Or it could be Argo workflows
if using Argo workflow, which is a CI/CD pipeline or workflow orchestrator that
you can run inside Kubernetes or just notifications
that everything went well. And with that, I'm going
to pass it to Niall to talk about the next two
stages of observe and govern. - All right, good morning, everyone. So thanks, Carlos. He's, I think, hit his CNCF
bingo card for the morning so far in the first section, but I get to talk about the
slightly less interesting stuff. But still, we're gonna
switch gears a little bit now that we've gone through Carlos's section around how we provision
and upgrade these clusters using our nice and get driven process. We'll switch gears to coral areas and we'll start with observability here. So I don't think there
exists an AWS presentation that has the word "observability" without having the slide in
it, but it's a catchy quote. So I threw in here. At the end of the day, regardless of whether we have one cluster or 100 clusters, things are gonna break. We need a strategy to
find issues, detect them, help us remediate them. And if we can't do that
as a platform team, then we can't operate a service that our developers can rely on. So both in this section
and the next section, we don't have time to
do observability 101. Instead, what we're gonna
do is focus on specifically some areas that we've seen
come up again and again with customers that they struggle with these particular areas
when they're operating clusters at maybe a larger scale. And that could even be when
you start to reach 10 clusters, these issues can start to
manifest pretty quickly. But we'll go through this one
by one and just take a look. So the first area is gonna be
roles and responsibilities. So Carlos already mentioned at
the start of the presentation that at least most of the
customers that we talk to when they start to manage
lots of EKS clusters, they tend to be
organizationally structured around a platform team
and an application team. Is that necessary? No, but it tends to be what
we're seeing in the field as an effective way to scale your use of EKS across lots of clusters. Now the platform team is typically not just a
cluster factory, right? The name suggests they
typically offer more in terms of some sort
of shared capabilities that the development teams could rely on in the observability sphere. That could be managing the observability infrastructure itself. But they can also, in some cases, maybe even pre-provision dashboards and alerts as they
onboard application teams to get them up and running quicker. By the end of the day, the
platform team is responsible for keeping those clusters up and running. They're not building the clusters and throwing them over the wall. That's a kind of the opposite
of what we're looking for. We want them to provide a
service to the development teams where the developers or customers, right? That's how we think about. That's whole idea of platform as a product and we need an observability strategy that reflects how we do that. So in order for the application
teams to trust the service and the trust is a critical part here, and we'll talk a bit
about that later as well. The platform team needs to be monitoring the signals coming from
the clusters constantly as the application teams are monitoring their workload signals. The platform team is working
hard to monitor dashboards and alerts with their
instrumentation coming back from the clusters and
providing that reliable service that the app teams are depending on. So, and many of you have probably already gone down this journey. You know, it's not unusual for us to see customers start off just by investing heavily
in dashboards, right? Dashboards are easy, they look great. You get your telemetry into your system. You install in dashboards and
you're doing observability and everything's great,
you can find your problems, but obviously, this doesn't scale very effectively pretty quickly, right? As soon as you start to
monitor a handful of clusters, you're not gonna be sitting, staring a dashboards waiting
for a bomb to happen. You need something that scales a bit more effectively than that. So pretty quickly, you
know, folks will migrate to something along the lines of relying significantly
more on proactive alerting. As a platform team, again, for that aspect
of trust, we don't want, ideally, our customers to even
notice if there's a problem. If something breaks some of our clusters, we want to know about
it and hopefully fix it before they even know that it's there. We definitely don't wanna be
relying on them telling us that it's broken because
that is just fundamentally how that trust starts to erode in the platform team and the platform. And this starts to dovetail
into a topic which we talk about all the time, which is platform adoption. If the trust erodes
between the platform team and the application teams,
then you quickly run into an issue where
they're not gonna wanna use your platform and you
need to keep that trust by having these proactive alerts and having a very proactive approach to keeping those clusters up and running regardless
of where on that spectrum that Carlos showed you land. Even if you're just
vending a cluster to them and the patterns that
we're talking about here, it's your responsibility to keep those up. So you need a solid system here. Alerts also, just having
alerts by themselves, you know, end up in a couple of
different holes here. Alert fatigue is an
old term at this point. Everyone knows it. So having the right alerts
is obviously very important, but also having an alert by itself doesn't tell you to fix the problem. And to scale your support
organization, you need to have run books tied
effectively to your alerts so that you can scale your platform team and that folks can fix
the problem at two o'clock in the morning when they get an alert and aren't scrambling through
through documentation. Also something that
input in the slide here, but as we increasingly start to see automated remediation
start to become more and more of a pattern. I know I was at CubeCon
a couple of weeks ago and I think the thing I saw the most on the vendor floor was ops remediation vendors we're now pretty
heavy in the expo. If you don't have runbooks to find, then you're not gonna be able to automate these processes eventually. So this is something which
people have been doing for a long time, but as we start to see this automation trend
increase, if you have runbook, increasingly there are tools coming up that can take those runbook and potentially apply them
automatically for you. So you've now got a new motivation to have these runbooks
actually defined ahead of time. And finally taking this. So far, we've talked about
things which probably aren't directly related to, you know, necessarily managing all our clusters, but this nice continuous delivery flow that Carlos gave us earlier, from my perspective, it's
basically a prerequisite to have this feedback loop in your process to make this work, right? This is how, as Carlos said, the EKS service team operates
is as they're rolling out through those cells and
through those waves, the proactive alerts become something that is telling them whether or not their process should
continue or not, right? They're not sitting watching
dashboards as you can imagine. So the other thing obviously,
as we go through here, Carlos talked about tests. So it's not unusual for us
to talk to platform teams who have maybe built a way to test and upgrade in their sandbox cluster. Maybe they've got a small cluster they use for functional tests. And that will uncover
uncover several issues. But as you progress through
your waves, when you start to reach staging and deployment, at the end of the day, it's no mystery. You're gonna uncover issues
that you just can't find in dev. And you can continue to
iterate on your tests and continue to find
those problems earlier on. But at the end of the day,
the soak period isn't useful unless you're monitoring something and you're alerting on things. I ideally in an automated way that will effectively sort
of act as a circuit breaker for your process, right? So this robust set of alerts
really just ends up being a mechanism we can use to cancel a rollout as it's going through if we
start to run into problems. So this sort of feeds back into that whole continuous
delivery process. This is probably the
question I get asked a lot, which I wish I had a better answer for. I wish I had a list for you
of what you should monitor and what the alert thresholds would be. If anyone has one, please send me it. But at the end of the day, a
lot of you folks are, you know, running Kubernetes already. You know how many components
you can install in a cluster, you know how many different workloads you can have running in a cluster. It can be pretty difficult
to figure out, you know, a set of things that we can
just tell you monitor this stuff and you're, you're gonna be good. Some of these things, there are some great stuff
online you can use as benchmarks, but I think instead of
trying to give you a list, really, what we wanna talk
about here is just emphasizing the breadth of what you have to monitor to effectively do this. To have trust in your process that it will short circuit your deployment if something goes wrong. This can be anything
from did the error rate on your Kubernetes control plane increase after your upgrade? Are your nodes coming up healthy? Did core DNS latency increase? Core DNS is still a problem in 2024. And if we can, can we understand if our
customers workloads are degrading if we've done an upgrade? Right? Did the error rate on developers team, or sorry, application A, B,
C in experience more latency or experience higher error rate. And if we can especially get
that sort of information, we understand not just
the health of the cluster that we're providing, but
the health of the experience that we're giving to our developers. Now, something which recently
launched, which is related to this would be that we did
in the last couple weeks launch the improved EKS cluster
control plane monitoring, where you can now get a whole bunch of extra metrics outta the control plane, which is something you
should definitely check out. We'll throw a link in some
of the related resources at the end of the session,
but that gives you a whole set of metrics that you
didn't previously have, as well as extra mechanisms
built into the console around things like
looking at your audit logs and stuff like that to identify issues. So that's been a great recent launch, which you can start to leverage. Now, not strictly observability, but we're gonna throw under this umbrella is maybe switching,
switching off a little bit, which is cluster inventory. So this is less around,
you know, finding issues and troubleshooting
issues and more around, how do I get a picture of all the clusters in my organization? Especially as it starts to scale and especially as it starts
to scale across regions, but accounts and whole
AWS organizations, right? It can be tricky to navigate
between all the console screens and sort of keep track of what's going going
on in aggregate view, there's lots of ways
that you can solve this. We see people doing it with vendor tools. We see them using Grafana
and observability tooling. One thing that we started to see is folks using developer
portals like Backstage. So I'm a big fan of Backstage if you've seen it myself before, but we have seen some customers, especially Kubernetes native customers, starting to use the ability to ingest information
about their EKS clusters into developer portals,
whether it's an open source one like Backstage or vendor products that kind of provide the equivalent. So you get that cross-account,
cross-region view of all the clusters that you have there. I think this is probably a better option than some of the other
approaches that we see, like building your own portal from scratch as a custom web app. Which a lot of people were doing with developer portals before Backstage. But this is something that I
think we're, we're starting to see some of our more
advanced Kubernetes users take advantage of, as
well as an aggregate list of all our clusters, especially
tools like Backstage, which are super flexible,
gives us the opportunity to customize and start
to aggregate information not just from EKS but
from related systems. So we could start with basic stuff, right? Maybe our account ID, our
AWS region, EKS version, the add-on versions, just
basic stuff we can pull from the API and maybe some of the places, but then we can start to
add in extra information like a cost tab where we
maybe pull the information from Cube cost or open cost. Maybe we start to provide
feedback on policy violations and findings in Backstage and
pull that to the same place. Maybe we show the SLOs for the
cluster so dev teams can see what the historical availability
of their cluster has been. Right? Again, not a monitoring tool,
but just a heads up display of how has this cluster
been performing that users or managers or somebody
else can just take a look at to get a heads up view. And then if there's information that we don't wanna rebuild
all of our other tools in developer portal site Backstage, we then start to provide
out links to all the places that we need to go, right? We're trying to get rid of
that big bookmark folder that you have where especially
if you've got 100 clusters, you're gonna have a
pretty bad time in Chrome trying to keep track of all this stuff. So we provide deep links
into all the places for a given cluster so it
becomes not just a single pane, but also a launching point
often to all the other systems that you need to actually
run your cluster. And it gives you just that
basis for being able to navigate through across your fleet. The last thing that we
find this useful for is relationships. So it's not unusual for developer portals to let you express relationships between whatever they call
items in their catalog. And in the stage case,
it would be entities. We can start to model things like what workloads
depend on a given cluster. Then the team ownership
we already have modeled typically in our Backstage. At this point, we can then start to model which clusters are dependent on each
other by understanding which workloads depend on each other. And we can start to build up this graph of metadata about our
clusters, our workloads, and our organizations that are not, we're not talking about
pods and deployments, we're talking organizationally about how these things fit
together so we can start to understand the impact or blast radius of changes potentially if we needed to figure something out. Or what would the impact be if this cluster in the
picture here went down in terms of not just the
workloads and the teams, but in terms of maybe downstream clusters where the workloads are interlinked. And we can start to do that
by building up this graph. And this can all be done
automatically, right? We don't need to do this manually. Just last week, we open sourced
a new plugin for Backstage where you can now ingest
AWS infrastructure straight into the Backstage
catalog through AWS config. And that means that there
were some existing plugins, which that you do parts of this before that were a little bit patchy in terms of a specific AWS services. Now anything that lives in config, you can pull straight into
Backstage including EKS clusters and then map them through
in terms of dependencies, using things like tags to build up this graph
completely automatically and start to get an idea
of how this fits together. So in a similar vein, Carlos talked at the
beginning about Guardrails. So talk about governments a little bit. One of the themes that
Carlos discussed earlier was consistency, right? When we start to talk to customers that have 100 EKS clusters,
you can't necessarily, well, it's harder to manage those if you don't have a
measure of consistency. So what Carlos talked
about earlier gave us that consistency at the
EKS cluster layer, right? We're getting all of our clusters up, we're getting our nodes up, we're getting our add-ons installed, but unfortunately, we've
got some pesky developers actually putting stuff in the clusters. And depending on your organization, you may or may not have control
over what that looks like, depending on the spectrum
that Carlos talked about of where you end up from vending a cluster all the way through to
a platform abstraction. That gives you more or less control there. So really, in any of those cases, governance really becomes
a key for achieving that consistency as we scale, especially as we're trying
to balance autonomy, right? Between more autonomous dev
teams that have opportunities to do more in their clusters versus maybe a more guardrailed approach where we are helping developers figure out what we should be putting in our clusters. Now again, this is not gonna be a policy as code 100 session. Instead we wanna talk
a little bit more about how we're seeing people
apply this specifically for problems at scale. But just as a quick primer,
obviously for everyone, policy as codes, engines like
OPA gatekeeper by Kyverno are exceptionally popular tools that you can get from
the open source world that will allow you to essentially express policies as codes. For example, Kyverno
typically is a YAML file, and those will often, you can enforce them in
different ways, right? So a pretty open one would
be you apply the policy and the cluster, a set of controllers are reconciling against
changes that are made. And you can flag a policy
report of everything that violates the policy
that needs to be addressed. A little bit of a harsher approach is that you can use things
like admission control to simply block app changes to the cluster that don't apply your policies correctly. Now, certain organizations
are willing to apply that and others are not, again,
on that autonomy scale, that gives us a nice way to
just stop things going in full stop that we don't wanna be there. But oftentimes, development teams will say that it slows them down. And in many cases, it can, right? So you've gotta balance
about which one of these that you're going to apply
effectively to your organization and the nature of how you're
building your platform. But specifically what do we see as some friction points
similar to observability where policies start to
come into the picture. So similar to what to alert on, we have what policies should I write? And this is potentially
even more of a blurry area because I don't know
what your policies are. There's certain natures
of policies that we have that are probably why we
consider a solid baseline, right? Those ones around say security, are you running read only file systems in your house, in your pods? Are you locking down,
adding extra capabilities and running as a root user? All those, I would say boring things that are kinda like your
foundational set of policies that you should really
be putting in place. Probably some basics around
things like are you tagging or labeling all of your pods correctly so you can do chargebacks
for cost as you scale up because you need to keep
your costs under control. Where we start to see this
specifically come into play for larger fleets of clusters. Firstly, keeping rollouts on track. So I've talked to so many customers where they're call them a Kubernetes team or a platform team, are
unable to upgrade clusters because of what developers
have deployed to them, right? As Carlos said about giving folks infrastructures code templates and them never upgrading their clusters, deprecated APIs, deprecated
CRDs, pod disruption budgets that basically get your node
rollout stuck in the mud. This is where we immediately
get into the discussion of policy as code. And you have to be able to
provide guardrails for people so that your cluster
rollout stays on track. You're not gonna be able to
go to every developer team and tell them that they
have to operate their CRDs. If you're running hundreds, 200 clusters, you're gonna spend all
of your time doing that. You have to start to
push that process left and keep them on the rails. And I know it sounds obvious, but my hope here would be that if you are in a similar position or you think you might
be in a similar position, this is a problem that a
lot of customers are having. And the solution is
unfortunately this, right? There's no magic technical
solution to this problem. A lot of it is organizational
as it probably comes across. Policies give us a way to implement an approach to solve this. But this is a recurring problem
that we see people having and you just have to plan for it. So if you are having that problem and are struggling to get
people to get on board with how to fix it, this is
really what we're seeing. The other aspect is availability. We've talked to customers where
they say they can't upgrade the clusters because in
the development clusters, their application teams have
only deployed with one replica. And when they upgrade the
cluster, the teams complain that the cluster went down
and the platform's not stable. This can even go up to
staging and production if they haven't used
pods disruption budgets. And obviously this impacts more than our upgrades
hurting your cluster. If there was other issues, you know, with the cluster itself,
it will also manifest. But if we're going with this
process of regular upgrades, we wanna keep things rolling, we're constantly changing stuff,
we want flexibility there. If these things are not in
place, again, that trust, even if it's just the perception of trust eroding in your platform, you're gonna run into issues
again with adoption teams complaining and it's not necessarily fair, but using policies has
to be one of these tools that you leverage not
just for the resiliency of keeping your apps
running, but for making sure that you can apply these updates
regularly without getting negative feedback on the
platform that you've built and the process that you've
spent all your time building that Carlos talked about earlier. In terms of managing policies. I mean, Carlos has already done all the hard work for us here, right? We've already got our ship and process that can take whatever
artifacts that we want and push them out. When you start to get up to scale, what we're really trying to do here is manage consistency,
again, across our clusters, but we have to be able to handle variation and we have to be able
to handle exceptions. So to get that consistency, one pattern that we see often is
just, it doesn't matter what cluster it is,
you have one helm chart of all your policies and
it goes everywhere, right? That's your baseline. Starting to patch work, deploying one policy to one
cluster and one to another, we'll work early on. But as you start to get
up to a larger scale, you're gonna need to just start to say, I'm throwing everything everywhere. And then start to be able to apply options within your helm chart, save
values files that allow you to enable or disable certain policies. Maybe you have different policies in your web services cluster than you have in your
data cluster, for example. So that could be one sort of
maybe bar that you use there. And then we just roll that out with our GitHub-driven process
to all of our clusters. It's just something else that we deploy. In terms of exceptions, so tools like Kyverno have exceptions as a first class process. You need to be able to handle this, right? So it could even just be as simple as you have that third party software that just won't run if it's not run as a root user or something like that. You know, we all have one. So being able to handle
exceptions is something that you need to be able to do eventually, but we want to be able to do
in a way where we can continue to have this single
artifact that we deploy out. So making sure to plan for exceptions, depending on the tooling that
you're using, is something that we wanna make sure we plan for. Now I talked about
earlier around, you know, the different ways that you can run these policies, code frameworks. The simplest being we're just
gonna assess the resources in the cluster and maybe create a report. So Kyverno, for example, will drop in something
like a cluster, you know, a policy report into your cluster when it does its assessment. At first, you can
probably just take a look into the clusters to
see what's not running. But once you start to really
scale out your EKS fleet, you're gonna need something
a little bit more aggregate in order to effectively
police these violations of your policies. So Kyverno, as an example,
has the related project of the policy reporter,
which you can install in your cluster, aggregate
all your findings into a system like S3 or
Elastic Search or Security Hub. And that gives you that single pane where you can aggregate all
those violations in one place. So we have an example
blog post for both OPA and Gatekeeper, which this
screenshot was derived from where we're shipping all
of our kernel findings straight into security hub, now we can search them, we can
search by account, by region, by cluster, maybe by workload if all the right metadata is there. And then we can rely on
whatever other features we want from security hub, say
remediation workflows and all that sort of stuff is things that we can build off
of to effectively start to actually not just
detect these violations, but have a system around remediating. So with that, we're at
the point of the talk where we're gonna start to wrap up. So really, this reflects the
three themes at the start of the talk that Carlos talked about. Applying Git GitOps
effectively is something that we're really
starting to see as a trend from these customers that are building out larger fleets of EKS clusters. GitOps, whether it's
a prerequisite or not, is really the trend that we're seeing around technology and things
like ROCD in some cases flux and also piggyback on top of GitOps with Kubernetes-driven
infrastructure provisioning, whether it's ACK cross-plane
and technology like that. Investing in proactive monitoring, right? Of being alert driven, not just so that you can detect problems, but you can, so you can feed it back into that GitOps process so that you can have
this largely hands off and you have confidence in the
system that you're building and the process that you're using to keep your customers,
sorry, your clusters updated. And finally, we wanna use
governance as our key to scaling. There are some problems
that we keep seeing over and over again as customers
try to manage EKS clusters and start to scale up their adoption. That policies are effectively the key to, and you know, planning for that and, you know, committing to that being the solution is
something that, you know, we have to talk to customers
about pretty regularly. Related sessions this week. So this has been pretty high level, right? We've been talking themes and concepts. Carlos has put together a workshop, which is being run twice this week, once tomorrow and once on Wednesday. This will also be available on GitHub and the link is in a
link which will be coming in a couple slides, but
that will actually give you hands-on experience in what
we've talked about today - In the three areas of
like you have experience of managing multi-clusters
with GitOps using Argo CD. In the workshop, we have an
example of using Terraform. A lot of folks use Terraform. So the pattern shows you
how to manage Terraform for AWS APIs and then Git
Argo CD for the GitOps. So a lot of folks have difficulties how to put those two tools together. So we have an example of that and then going into management
of observability of like, a lot of platform teams worry about the applications observability, which is just like net good, but forget about like monitoring the observability of the add-ons. Like how is the open
source Karpenter's doing? How Accordion is acting,
BCC and I subnets. That's observability that
the platform teams owns. So they have to be aware of that. And that ends with, you know, some learnings about policy management. Would you be using Kyverno? So we'll be using the Kyverno and Kyverno reporting,
configuring actually this example of the security hub. So you have hands on screens on that. So get there early if you can. - And it will be available
on, it's already available. - Yeah, and it's available. Everything is on GitHub open source. So you can start looking at
it, taking extracts of it to apply it to your own organization. - Next, there is Cube 301. So this is a talk with Adobe
and some folks from AWS on how they've built
scalable platforms with EKS. They've previously done
some talks at CubeCon, and some other stuff, CNCF-related talks around how they do this and you know, they're applying
some of the principles that Carlos talked about
around pre-flight checks and post-flight checks and
all that kinda stuff, right? So definitely a talk I
think is worth seeing because you'll see
again some of the themes around platform building but also, how Adobe is doing in practice. So definitely recommend that one. And then finally there is some chalk talks that are getting done on
playing off things like infrastructures, code, GitOps,
CI/CD, the different options that you have, pros and cons and what maybe you should be considering for your organization
that you can catch again a couple times this week, which
will also be great Sessions, Oh. (indistinct). In terms of extra
resources, so EKS workshop, hands-on labs if you
haven't already done it. We're continuing to evolve that with the new launches as they come out. We're running that a
couple of times this week, but we didn't include
on the extra sessions. But if you want to run this yourself, you can run it self-service. We can also run this as what
we call an immersion day. Talk to your account team. You can have specialists come in, help deliver this to your
teams and learn EKS hands-on. It's very modular. We can tailor it to your organization depending on what it's you wanna learn. So definitely worth taking a
look if you haven't already. Best practices guide. So many of you'll probably
already be familiar with the EKS Best Practice Guide. It's been around for a while, but recently we did take what
is this great set of knowledge for day two operations and it's now officially part
of the EKS documentation. So all the same great knowledge,
but it matches the docs and it's all in the same place now. So it could be easier to find. And finally, who doesn't love a badge? Get your badge put on
LinkedIn, that kind of stuff. I don't have mine yet, do you? - [Carlos] Huh? - Do you have your EKS badge? - Yes.
- Oh, okay. I need to catch up. (audience laughing) This is a link to a GitHub well, page. You'll find all the Kubernetes
sessions in this GitHub. There'll be a directory for
each Kubernetes session. It'll mostly be links to resources. We're not necessarily putting code here, a bunch of links to stuff
that you can look at related to each session. So that link should
take you to our folder, but that repository, you can pretty easily
find your way around it. We've added some links in
there to some of the stuff that we've talked about,
some of the extra sessions. - Yeah, the main rig me
has all the Cube track that we are part of. Like EKS group track. - Yep. - And there's other workshops if Niall mentioned Backstage. There's also some workshops that you're going to get hands
on experience with Backstage. I think is Cube 308. Cube today is if you want
to get hands on doing EKS and Backstage and there's other ones of gen AI on Backstage. - [Niall] Yes.