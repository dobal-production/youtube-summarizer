- 좋아요, 좋은 아침이에요KUB314 방문을 환영합니다.어떻게 해야하는지 얘기해 볼게요
EKS에서 Gen AI 워크로드를 실행하세요.지난 며칠 동안 많은 분들이 계실 거라고 생각합니다.
기조연설에 가보셨어요.제 생각에 피터는 월요일이었던 것 같아요.
밤, 맷은 어제였어요.방금 들었는데
강연하는 동안 머신러닝 기조연설이 진행되고 있습니다.강연에 참석해주셔서 감사합니다.기조 연설 중 상당수는 머신 러닝, 제너레이티브 AI 기능에 대해 이야기하고 있습니다.
곧 발표될 예정입니다.그래비톤 4, 트레이니엄 2 울트라 서버, 이 모든 멋진 기능들.많은 분들이 참여하신 것 같아요.
이 방에서 말하길, 네, 좋긴 한데 EKS와 많은 고객들, 그것도 규모가 큰 고객들과 통하지 않는 이상 사용할 수 없습니다.
시중에 나와 있는 기본 모델들, 아시다시피 그 중 상당수는
쿠버네티스에서 트레이닝했습니다.이것이 바로 그것입니다.
오늘 이야기할 내용은 EKS가 실행을 위해 수행하는 작업입니다. 제목에는 Gen AI가 있지만 실제로 모든 기계 학습 워크로드를 EKS에서 더 쉽게 수행할 수 있습니다.그리고 저는 Rama와 함께 EKS와 쿠버네티스가 왜 이렇게 발전했는지에 대해 조금 이야기할 예정입니다.
ML 워크로드에 적합합니다.그리고 특별 게스트로 엘리 릴리 앤 컴퍼니의 CA들이 자신들의 프로젝트를 어떻게 구축했는지에 대해 이야기할 예정입니다.
EKS의 머신러닝 플랫폼과 이들이 거둔 성공.좋아요, 정말 빠르네요. 왜 제너레이티브 AI일까요?제너레이티브 AI란 무엇일까요?여러분 모두 알고 계실 거라는 걸 알아요. 어쩌면 여러분 중 일부는 늦은 밤을 보냈을 수도 있습니다.첫 번째 강연이에요.
여기로 아주 빨리 갈게요.인간의 콘텐츠처럼 보이는 콘텐츠를 만들 수 있는 AI는 대부분의 경우 제작할 수 없습니다.
차이점까지 구분할 수 있죠.그리고 이건 다음에 의해 구동됩니다.
아마도 내년이면 최대 수천억 개의 파라미터를 가질 수 있는 기본 모델일 것입니다.
1조 개에 육박하는 파라미터.제너레이티브 AI의 진정한 혁신은 비용을 줄일 수 있다는 것입니다.
이제 ML 애플리케이션을 구축할 시간입니다. 이런 것부터 시작하기 때문이죠.
거대한 기본 모델, 그리고 조금만 걸리면 됩니다.
필요한 작업을 세밀하게 조정하고 수행할 수 있도록 약간의 교육을 받아야 합니다.
특정 작업에 적합합니다.사용 사례에 관한 한
지금까지 살펴본 주요 버킷은 네 가지입니다.
제너레이티브 AI 사용 사례, 아마도 가장 친숙한 사례는 고객 경험 향상일 것입니다.앤디가 어제 무대에 올라 amazon.com에서 쇼핑을 도와줄 수 있는 새로운 에이전트인 루퍼스에 대해 이야기했던 것 같아요.직원 육성
생산성 향상은 또 다른 문제입니다.어제 있었던 또 다른 몇 가지 발표는 코드 마이그레이션 도구였습니다.에 대해 이야기한 것 같아요.
아마존에서 Java 11에서 Java 17로 전환하면서 개발자 시간을 X시간이나 절약했는지는 잘 모르겠습니다.Windows 현대화 코드 마이그레이션도 발표한 것 같아요.이런 사용 사례를 많이 볼 수 있습니다.콘텐츠 생성, 이미지
제너레이터도 멋지고 비디오 제너레이터도 멋집니다.그리고 제 생각에 더 흥미로운 것은 비즈니스 운영이라고 생각해요.실제로 이야기를 나눴는데, 고객 미팅을 많이 해봤어요.
지난 며칠 동안 많은 분들이 이미 Gen AI와 현재 개발 중인 플랫폼 중 일부를 사용하고 계십니다. 로그 분석을 통해 문제를 빠르게 해결하거나 개발자를 온보딩하는 것까지 말이죠.
플랫폼 구축 속도가 빨라졌습니다.그래서 실제로 효과가 있는 몇 가지 사용 사례가 보이기 시작했습니다.지금까지 살펴본 몇 가지 트렌드는 좀 더 복잡한 제너레이티브 AI 워크플로우, 에이전트가 에이전트와 대화하는 것, 프로덕션으로 넘어가는 것 등을 들 수 있습니다.
모델에는 가드레일과 기타 기능 세트가 필요합니다.제 생각엔 마지막 모델인 것 같아요.
가장 흥미로워요, AWS, 기조 연설을 보셨잖아요.AWS에는 다양한 도구가 있습니다.어제 베드록에 대한 얘기가 많이 나왔어요.Amazon Queue, 스택에는 다양한 계층이 있습니다.EKS는 정말 필요한 레이어의 맨 아래에 있습니다.
최고의 유연성, 가장 많은 사용자 지정,
최고 수준의 확장성.하지만 대기업에서는 EKS, Bedrock, Queue 및 기타 도구를 혼합하여 사용할 수 있다고 생각합니다.2023년이 해였다면
차세대 AI, 개념 증명, POC 등 2024년은
제작 연도.그리고 그런 제작 사고방식으로 전환하려면 정말 필요한 것이 하나 있습니다.
사고방식이 달라요.어떻게 하면 돈을 벌거나 비용을 절감할 수 있을지 생각해보세요.POC를 영원히 운영할 수는 없잖아요.대규모 추론 비용, 대규모 지연 시간에 대해 생각하기 시작하죠.성능이 가장 뛰어난 모델을 선택하고 싶지 않을 수도 있습니다.비교해서 따져봐야 합니다.
프로덕션 환경에서 실행하는 데 비용이 얼마나 드나요?지연 시간은 얼마이고, 어떤 영향을 미칠까요?
사용자 경험?그리고 다음으로 이동하면
프로덕션에서는 보안, 규정 준수, 윤리적 제약과 같은 것들에 대해서도 생각해야 합니다.그 외에도 훨씬 더 많은 것들이 있습니다.
그리고 건물을 짓는 데에는 훨씬 더 많은 노력이 필요합니다.
프로덕션 애플리케이션.이것이 바로 트렌드입니다.
올해부터 시작했어요.그래서 라마한테 넘겨서 그 중 몇 가지에 대해 얘기해 볼게요
제너레이티브 AI 과제와 EKS가 이를 해결하는 데 어떻게 도움이 되는가. - 마이크 감사합니다. 안녕하세요 여러분, 안녕하세요. 좋은 아침이에요.저는 라마 폰누스와미예요. 전 세계를 운전해서 가세요.
EKS에서의 AML 시장.이제 고객이 프로덕션 환경에서 세대 AI 워크로드를 확장하기 시작할 때 직면하게 되는 몇 가지 일반적인 문제에 대해 간단히 설명해 보겠습니다.세 가지와 거의 비슷합니다.
버킷이 다르죠?마이크가 말했던 것처럼
팀마다 요구 사항이 다릅니다.
단일 조직 내에서 말이죠.그리고 한 모델로는 불가능할 거예요
전부 다 맞죠?그러니까 아마 그럴 가능성이 높죠
여러 모델을 실행해야 하고, 각 팀의 특정 요구 사항에 맞게 모델을 다양한 방식으로 사용자 지정해야 합니다.여기에 추가된 기능도 함께 제공됩니다.
둘째 날 수술에 따르는 부담입니다.어떻게 관리할 건데?
그 모델들의 버전 관리?새 모델이 출시되면 이 모델들을 어떻게 업그레이드할 건가요?가드레일은 어떻게 만드나요?
이 모델들을 상대로?그리고 액세스를 어떻게 관리하시나요?
이러한 모델의 데이터에 대해 알고 계신가요?이런 것들을 커스터마이징하기 때문이죠
모델은 쉽지 않을 거예요.모델을 커스터마이징하려면 다음이 필요합니다.
도메인별로 특정되고 이러한 모델이 특정 작업을 수행할 수 있기를 원하는 특정 사용 사례에 맞는 큐레이션된 특정 데이터
그 도메인에 맞죠?이는 또한 여러 데이터를 통합해야 한다는 의미이기도 합니다.
소스, 데이터를 유지 관리하면서 이 모든 데이터에 대한 액세스를 관리해야 하는 경우
데이터 보안 요구 사항.마지막으로 중요한 것은, 이 모든 것이 다음을 수행해야 한다는 것을 의미합니다.
점점 까다로워지는 대규모 인프라를 관리하세요.확장을 시작하는 고객을 많이 보아왔습니다.
생산 규모가 커지면 고객은 더 많은 인프라를 위임하고 있다고 느끼기 시작합니다.
통제력을 갖추면 어느 것이든 제어할 수 있는 능력이 떨어집니다.
이런 매개변수들 맞죠?그래서 고객들이 보이기 시작했죠.
Gen의 중요한 규모와 생산량에 도달했을 때
AI 워크로드는 더 많은 통제력을 필요로 합니다.
인프라에 대해 알아보고 옵션을 살펴보세요.
필요한 제어를 할 수 있는 EKS처럼
운영하려는 모든 모델과 ML 환경을 사용자 지정할 수 있고 ROI 목표를 달성하도록 비용을 최적화할 수 있어야 합니다.따라서 이러한 문제들이 현장의 과제입니다.
조직 차원에서요.조금 생각해 봅시다.
데이터 사이언티스트나 ML 엔지니어가 일상 업무에서 겪는 어려움은 무엇일까요?데이터 과학자는 인프라 관리를 정말로 원하지 않습니다.필요한 것은 극히 드문 경우뿐입니다.
코드 배포만 가능한 가용 인프라
그들의 모델은 물론 확장도 가능하지만 그들은 별로 원하지 않습니다.
관리를 위해 실행해야 하는 보일러 유료 코드나 스크립트를 만들고 싶었죠.
ML 모델의 라이프사이클대신 그들은 다음을 원합니다.
이러한 문제를 해결할 수 있는 즉시 구축된 플랫폼
워크로드를 효과적으로 배포하고 확장하는 데 필요한 모든 기능
한 걸음 더 나아가기도 하죠.그리고 그들은 할 수 있기를 원하죠.
이러한 요구 사항을 보다 최적화된 환경에서 충족하기 위해서입니다.
반복 가능한 방식으로, 아주 짧은 시간 안에 무언가를 만들 필요 없이 계속 반복해서 할 수 있도록 말이죠.
필요할 때마다 개성 있는 임시방편으로
모델을 구축하고 확장하세요.용어로 요약하자면
프로덕션 환경에 제너레이티브 AI를 배포하는 측면에서 주요 과제나 고려 사항은 세 가지로 요약할 수 있습니다. 그렇죠?빠르게 움직이고 싶고, 데이터 과학자와 ML 엔지니어가 다음을 수행할 수 있도록 지원해야 합니다.
모델을 빠르게 구축하고, 빠르게 배포하고, 향후 확장도 가능해야 합니다.두 번째로 중요한 것은 커스터마이징을 활성화하는 방법에 관한 것입니다.
당신의 모델들 맞죠?규모에 맞게 말이죠.할 수 있으면 좋겠어요
여러 팀의 특정 사용 사례에 맞게 여러 모델을 사용자 지정합니다.
조직 내에서또한 이러한 모델을 현재의 요구 사항만 충족하는 것이 아니라 미래의 요구 사항에 맞게 원활하게 확장할 수 있어야 합니다.그리고 점점 높아지는 추세에 따라 비용을 지속적으로 최적화할 수 있는 방법은 무엇일까요?
향후 규모를 확대할 계획인가요?ML 모델도 확장하세요.바로 여기가 바로 여기입니다.
많은 고객들이 EKS가 만든 제품을 언급하는 것을 볼 수 있습니다.
특히 더 많은 것이 필요한 환경에서는 이치에 맞습니다.
인프라를 제어하여 데이터 보안 문제를 해결하고,
비용 최적화 문제를 해결하려면 다음이 필요하기 때문입니다.
ROI 목표 달성 등등.그럼 좀 자세히 살펴보죠. EKS가 이러한 이점을 제공하는 곳은 어디일까요?왜 우리는 이런 것들을 많이 볼 수 있을까요?
고객이 제너레이티브 AI 워크로드를 배포하기 위해 EKS를 활용하기 시작했을까요?다시 말씀드리지만, EKS를 통해 어떻게 하면 더 빠르게 이동할 수 있을까요?두 가지죠, 그렇죠?첫째는, 대부분의 고객들이 이미 가지고 있는 경우입니다.
이를 수행하는 방법으로 쿠버네티스에서 표준화했습니다.
애플리케이션 개발은 그냥 확장만 하면 됩니다.
이미 구축했기 때문에 기존 플랫폼을 기반으로 ML 모델을 배포할 수 있습니다.
그들에게도 필요한 기본 기능
ML 워크로드용.두 번째 이유는 오픈 소스 도구를 사용할 수 있기 때문이죠?ML 공간은 일반적으로 빠르게 변화하고 있으며, 그 중 상당수가
오픈소스 분야에서 혁신이 일어나고 있죠?그리고 이러한 오픈소스 도구들은
대부분은 보통 바로 사용할 수 있는 상태로 제공됩니다.
쿠버네티스 통합.따라서 고객은 EKS를 통해 해당 ML 전용 OSS를 통합하기만 하면 됩니다.
이를 기반으로 데이터를 지원하는 솔루션
앞서 살펴본 것처럼 과학자들에게 모든 엔드-투-엔드 ML 운영 라이프사이클 기능을 제공하여 더 빠르게 움직여야 한다는 것이 주요 과제 중 하나입니다.확장성 측면에서 쿠버네티스와 EKS는 이와 같은 이점을 제공합니다.
모든 AWS ML 인프라 서비스에 기본적으로 통합되므로 다음과 같이 원활하게 확장할 수 있습니다.
ML 모델에 대한 수요도 증가합니다.사용자 지정 측면에서는 전체 인프라를 제어할 수 있기 때문입니다.
EKS를 사용하면 인스턴스 수준까지 특정 요구 사항에 맞게 환경을 유연하게 구성할 수 있으며 특정 이유로 인한 제한을 받지 않을 수 있습니다.
당사가 제공하는 기능.아시다시피 EKS 내에서 ML 환경을 구성하는 방법에는 제한이 없습니다.이를 통해 다음을 수행할 수 있습니다.
특정 요구 사항에 맞게 원하는 대로 사용자 지정할 수 있습니다.그리고 할 수 있기 때문이죠.
예를 들어 여러 가지 선택을 할 수 있게 되면
적절한 인프라 규모, 적절한 인스턴스를 지속적으로 선택하고 최적화할 수 있습니다.
ML 워크로드에 따라 더 확장하기 시작하면 지속적으로 비용을 절감할 수 있습니다.이제 유연성이 어떻게 발휘되는지 좀 더 자세히 살펴보겠습니다.
그리고 비용 최적화는 ML용 EKS에서 작동하는데, 실제로는 세 계층에 걸쳐 이루어집니다.첫 번째는 즉각적인 수준입니다. EKS는 모든 것을 지원하기 때문입니다.
EC2 인스턴스 중 GPU, 엔비디아 기반, GPU 인스턴스는
트레이니엄, 안드레아, 인텔 기반 인스턴스 등AWS에서 사용할 수 있는 모든 EC2 인스턴스를 EKS와 함께 활용할 수 있습니다.따라서 유연하게 선택할 수 있습니다.
워크로드 및 워크로드를 기반으로 사용할 인스턴스
워크로드 요구 사항.그러니까 이건 그냥 그런 게 아니에요.
충분할 거예요, 그렇죠?이상적으로는 원하지 않는 것 같네요
스케줄링을 위해 새 워크로드가 들어올 때마다 이러한 선택을 수동으로 하는 것이 좋습니다.바로 이 부분에서 Karpenter의 성능이 발휘됩니다.카펜터와 함께라면,
특정 인스턴스를 프로비전하려는 매개 변수를 제공하고 들어오는 워크로드를 평가하여 해당 인스턴스에 적합한 인스턴스를 선택하도록 함으로써 이러한 모든 선택을 효과적으로 자동화합니다.이 모든 것은 Karpenter에 의해 자동화되고 반복해서 반복됩니다.
새 워크로드가 예약될 때마다 사용자를 대신하여 이 작업을 수행합니다.게다가 Karpenter를 사용하면 프로비저닝도 훨씬 쉬워집니다.그래서 카펜터와 함께
EKS에 최적화된 군대는 모든 내장 장비와 함께 제공됩니다.
필요한 종속성 및 가능한 기능
수요가 증가할 때마다 인스턴스를 빠르게 가동할 수 있으며, Karpenter는 빠른 확장을 제공합니다. 하지만 더 중요한 것은
또한 수요가 증가하지 않을 때마다 다시 축소됩니다.따라서 비용을 최적화할 수 있고 GPU 공유 메커니즘도 활용할 수 있습니다.또한 EKS의 내장된 멀티 테넌시를 통해 제한된 GPU 리소스를 여러 팀에서 효과적으로 안전하게 공유할 수 있습니다.
조직 내에서 활용도를 높이세요
더 나아가 GPU도 마찬가지죠.게다가 사용 가능한 솔루션의 대양인 ML 전용 OSS 솔루션을 통합할 수 있습니다.필요한 기능에 적합한 도구를 선택하고 사용할 수 있습니다.
그런 다음 EKS와 통합하고 모든 것을 얻을 수 있는 조직
데이터 과학자가 원하는 ML 전용 기능에 필요한 유연성도 제공합니다.간단히 말씀드리자면, 극소수의 고객을 빠르게 강조하고 싶었습니다.저희 회사에는 많은 고객이 있습니다.
EKS에서 A세대를 운영하고 있지만, 저는 EKS에서 A세대를 운영할 수 있었던 몇 가지 업체를 선정하고 싶었습니다.
제너레이티브 배포를 통해 특정 이점을 얻을 수 있습니다.
EKS의 AI 워크로드.Vannevar Labs는 국방 기술 스타트업입니다.이들은 국가 정보기관에 색다른 인텔리전스를 제공합니다.
기관 및 국방부.그래서 그들은 다음과 같은 곳에서 활동합니다.
최고 수준의 기밀 및 데이터 보안 문제EKS와 레이, 카펜터를 함께 활용하여 45% 의 절감 효과를 얻을 수 있었습니다.
추론 비용에서 말이죠.그리고 그들은 할 수 있었죠.
이는 CPU와 GPU를 혼합한 인스턴스 유형을 사용하고 GPU 인스턴스에서 CPU 사용량이 많은 워크로드를 스케줄링하여 사용량이 적은 인스턴스를 활용할 수 있기 때문입니다.
GPU 인스턴스 내 CPU.따라서 전반적으로 증가하고 있습니다.
GPU 사용률이 크게 향상되었습니다.그리고 최근에
그들과 함께 블로그를 게시했습니다.이러한 비용 절감을 위해 어떤 구성을 했는지 자세히 알아보려면 AWS 컨테이너 블로그 채널에서 언제든지 확인하시기 바랍니다.다음은 Informatica입니다.이들은 LLM을 구축한 또 다른 고객입니다.
다중 교육 및 미세 조정을 위한 운영 플랫폼
EKS 기반의 LLM 모델.그들은 다음을 달성할 수 있었습니다.
매니지드 서비스 대비 약 30% 의 비용 절감
이전에 사용하던 것들이죠.그리고 그들은 해낼 수 있었죠.
관리형 서비스의 제약을 받았기 때문에 구성 용이성이 향상되었습니다.
매니지드 서비스의 커스터마이징 기능
제공하고 있었습니다.Zoom은 또 다른 분야입니다.
멀티모달 호스팅을 수행할 수 있는 유사한 LM 운영 플랫폼을 만든 고객은 지속적인 수요에 맞춰 안정적이고 효율적으로 확장할 수 있게 되었습니다.제가 원했던 또 다른 주요 고객
가장 강조하고 싶은 것은 허깅 페이스입니다.허깅 페이스 허브를 사용한 경우 프리미엄 가격은
첫째, 오늘날 기본적으로 EPS를 기반으로 운영되는 세 가지 티어 제품인데, 처음 시작했을 때 당면한 과제는
이 플랫폼을 구축하려면 전용 모델을 사용하여 수백만 개의 모델을 추론할 수 있는 플랫폼을 구축해야 했습니다.
추론 엔드포인트와 다양한 도메인 이름도 그곳에서 호스팅해야 했습니다.그리고 이 모든 것에는 3단계 요금제가 제공되어야 했습니다.따라서 비용과 인프라 비용을 극도로 최적화해야 한다는 것이 당면 과제였습니다.
이를 다음과 같이 제공할 여유가 있으려면
프리미엄 가격 등급도 마찬가지입니다.그래서 그들이 내놓은 해결책은
EKS, 여러 EKS 클러스터, 2000개 이상의 노드에 배포하면 내부의 많은 부품을 효과적으로 핀 패킹할 수 있습니다.
인스턴스가 매우 적죠.그리고 그들은 또한 할 수 있습니다.
모델을 교환하는 GPU와의 시간 공유를 활용하고
또한 몇 초에 한 번씩 출력됩니다.이를 통해 ML 개발자들이 협업할 수 있는 플랫폼을 전 세계에 효과적으로 제공할 수 있게 되었습니다.그래서 한 가지 말씀드린 게 있습니다.
앞서 이미 쿠버네티스를 사용하고 있는 고객들은
애플리케이션 개발을 위한 표준 플랫폼으로.이러한 플랫폼이 어떻게 생겼는지 보여주는 일종의 스냅샷이죠?그들은 이 모든 것을 구축합니다.
로깅, 모니터링, 재해 복구, 대규모 보안을 수행하는 방법과 같은 기본 기능이 이 플랫폼에 이미 포함되어 있습니다.이러한 문제가 모두 해결되었습니다.그리고 고객 여러분,
ML의 경우, 다시는 이러한 문제를 해결하고 싶어하지 않습니다.
아니면 바퀴를 다시 발명할 수도 있겠죠?그들은 단지 모든 것을 사용하고 싶어할 뿐입니다.
이러한 기본 기능을 확장하여 ML을 실행할 수 있습니다.
이 외에도 워크로드도 포함됩니다.이것이 한 가지 주요 이점입니다.두 번째 장점은 고객입니다.
쿠버네티스를 표준 레이어로 사용할 수 있습니다.
온프레미스든, 온프레미스든,
엣지, 클라우드에서도 마찬가지입니다.따라서 쿠버네티스를 제공하고 사용하면 이러한 작업을 통합하는 데 도움이 됩니다.지난 일요일에 출시한 최신 릴리스에 관심을 기울이셨다면 이그레스 하이브리드 노드도 릴리스하여 이 작업이 훨씬 쉬워졌습니다.
고객이 EKS 하이브리드 노트를 사용할 수 있도록 하면
클라우드, 온프레미스, 엣지의 노드를 단일 EKS 관리형 노드에 연결할 수 있습니다.
원하는 경우 컨트롤 플레인을 사용하여 모든 쿠버네티스를 오프로드할 수 있습니다.
컨트롤 플레인 관리를 통해 다양한 혜택을 누릴 수 있습니다.
전체 인프라의 논리적 매핑
이러한 모든 환경은 가능한 모든 곳에서 GPU 리소스를 활용하고 비용을 최적화할 수 있는 더 나은 방법을 제공합니다.
여러분도 잘 따라가실 수 있습니다.골라서 선택할 수 있어요
어떤 워크로드가 어떤 환경에 배포되는지
기대하는 비용, ROI 이점 기준
이러한 워크로드도 마찬가지입니다.그래서 우리가 하는 것으로는
고객들이 언제 그렇게 하는지 보아왔고, 언제 시도하는지 보아왔습니다.
이러한 플랫폼을 확장하려면 앞서 말씀드렸듯이 OSS 솔루션을 통합해야 합니다.
그 위에 있죠?이를 통해 쿠버네티스는 잘하는 기능인 인프라 오케스트레이션을 수행할 수 있습니다.그리고 여기에 OSS 솔루션을 통합하여 ML을 제공합니다.
특정 기능.예를 들어 많은 고객이 VLLM과 함께 많은 기능을 제공하는 EKS를 채택하는 것을 보았습니다.
메모리 및 모델 최적화와 같은 기능,
멀티모달 관리, 요청 일괄 처리, 쿼리 큐잉 등
추론 인프라.따라서 이것은 다음을 제공할 수 있습니다.
바로 사용할 수 있는 추론 엔드포인트 또는 데이터 과학자가 사용할 수 있는 추론 스택입니다.
모델을 배포하고 추론을 실행할 수 있는 엔드포인트를 얻을 수 있다는 것이죠.어떤 의미에서는 이 모든 것을 요약하자면, 고객이 제너레이티브 AI에 EKS를 사용했을 때 얻을 수 있었던 것은
환경을 더 잘 제어하려면 다음과 같은 이점을 얻을 수 있습니다.
고객의 특정 요구 사항에 맞게 사용자 정의하고 원활하게 확장할 수 있어야 합니다.
미래의 수요를 충족하고 ROA 목표를 충족하기 위해 지속적으로 비용을 최적화할 수 있어야 합니다.이제 Mike에게 구체적인 내용을 말씀드리도록 하겠습니다.
EKS가 제공하는 기능들. - 좋아요, 고마워요, 라마.좋아요, 제가 좀 할게요
EKS에서 AI ML 제너레이티브 AI 워크로드를 더 쉽게 실행할 수 있도록 지난 12~18개월 동안 출시한 많은 기능에 대한 간략한 개요를 살펴보겠습니다.먼저, 이 그림은 Rama가 방금 출시된 모든 도구에 대해 이야기한 내용을 좀 더 구체적으로 보여줍니다.
시작할 때 사용할 수 있는 부분은 크게 두 가지입니다.오픈 소스 스택은 처음부터 시작하는 경우가 거의 없습니다.그리고 쿠버네티스 라마가 언급한 바 있습니다.
레이 (Ray), VLLM, 주피터 허브 (Jupyter Hub) 등 이 모든 도구들은 쿠버네티스와 호환됩니다. 아마존에서 얘기하고 싶은 것은 플라이휠 효과입니다. 새로운 머신러닝 툴을 만들려고 한다고 가정해 보자면
정말 좋은 생각이 있네요.그걸 입양하고 싶다면, 그럴 가능성이 매우 높죠.
쿠버네티스에서 다음 애플리케이션 중 하나로 잘 작동하는지 확인하세요.
첫 배포 대상.그리고 이러한 프레임워크 중 상당수가
시중에 나와 있는 도구가 바로 그 역할을 합니다.쿠버네티스에서 잘 작동합니다.이들 중 몇몇은
기여하기로 선택하세요.Ray에 몇 가지 기여를 했고 제대로 작동하는지 확인했습니다.
뉴런 프레임워크와 함께 말이죠.그리고 다른 쪽 끝에는
EKS에서는 AWS의 인프라 혁신과 서비스가 이러한 규모를 뒷받침하는지 확인하는 것입니다.
스케일 트레이닝 추론은 EKS와 잘 작동합니다.따라서 CSI 드라이버, ECR 등 많은 것들이
ML 워크로드에 사용해야 하는 기본 서비스가 제대로 작동하는지 확인합니다.
EKS와 정말 잘 어울립니다.좋아요, 기능, 이거, 컨트롤 플레인의 확장성을 위해 우리가 하는 일에 대한 애정이 부족한 것 같아요.우리는 현재 기능을 거의 하지 않습니다.
새 포스트 EKS가 좀 더 실적이 좋은 것 같네요.하지만 사실 6년 전 서비스가 시작되었을 때 EKS 클러스터를 만든 고객이 있는데 그들은 이제 막
한 점 10에서 점 30으로 계속 업그레이드했습니다.그리고 그 클러스터들은 그 당시의 모습과 전혀 달라보이지 않습니다.
6년 전에 출시되었습니다.따라서 EKS 클러스터를 생성할 때마다 전용 클러스터를 생성합니다.
여러분을 위한 인프라.EC2 인스턴스, nat 게이트웨이, NLB 등 많은 작업을 수행합니다.시간이 흐르면서
새 인스턴스이든 관계없이 지속적으로 성능을 추가하세요.
유형, 더 빠른 볼륨, 더 나은 네트워킹.그리고 올해 저희가 많은 노력을 기울인 프로젝트 중 하나는
CD에서의 관리 방식을 훨씬 더 큰 규모로 바꾸었죠.특정 고객이 있습니다.
이는 10,000개, 20,000개, 50,000개의 노드 클러스터를 요구하는 것입니다.이를 실현하기 위해서는 몇 가지 조치를 취해야 했습니다.
백엔드인 CD에서 관리하는 방식을 대대적으로 리팩토링한 것입니다.
쿠버네티스 데이터베이스.아마도 내년이면
이에 대해 좀 더 발표하겠지만, 그건 뭔가
저희가 작업 중이었어요.EFA는 낮은 예시입니다.
수준: 인프라 혁신: AWS, 즉 OS입니다.
높은 수준의 성능을 허용하는 우회 하드웨어 인터페이스
다중 노드 통신.디바이스 플러그인을 통해 EKS 및 Kubernetes에서 쉽게 사용할 수 있습니다.몇 가지 주목할 만한 것이 있다고 생각합니다.
최근 개선 사항, EFA에 대한 지원이 추가되었습니다.
크로스 서브넷 통신.이전에는 다음과 같이 제한되었습니다.
EFA가 있는 단일 서브넷.이제 크로스 작업을 할 수 있습니다.
동일한 AZ 내에 있는 서브넷이므로 꼭 원하는 것은 아닙니다.
비용이 많이 들지만 IP 주소 고갈에 도움이 되기 때문에 AZ 전반에 걸쳐 교육을 실시하세요.그리고 또 다른 한가지는
이는 IP 주소 고갈에 도움이 되는 경우가 많습니다.
ML 교육 워크로드는 그냥 거기에 있습니다.
계산도 많이 하고, 계산도 많이 하고, 사실 필요하지도 않아요.
인터넷과 대화하기 위해서요.클러스터의 다른 노드들과만 서로 통신하면 됩니다.그래서 최근에 지원을 추가했습니다.
EFA 전용 ENI 지원이므로 필요하지 않습니다.
클러스터의 EFA 디바이스에 실제 IP 주소를 추가합니다.이제 대규모 실행 시 EKS에서도 이 기능이 지원됩니다.
교육 작업을 확장하려면 모든 데이터를 처리할 대규모 스토리지 백엔드가 필요합니다.
처리 중인 데이터입니다.아마 없을 겁니다.
S3, S3보다 더 큰 규모의 스토리지 백엔드에서 작년에는 마운트 포인트 (Mount Point) 라는 기술을 탑재할 수 있는 오픈 소스로,
S3 버킷을 EC2 인스턴스에 연결하고 파일 시스템 명령을 사용합니다.그리고 자동으로 전달됩니다.
S3 오브젝트 명령으로 변환되었습니다.그리고 CSI 드라이버를 통해 EKS에서도 사용하기 쉽게 만들었습니다.
쿠버네티스 네이티브 인터페이스.최근의 주목할 만한 개선 사항
CSI 드라이버가 좀 더 세밀하게 추가되었습니까?
액세스 제어.잠금을 해제할 수 있게 되면
클러스터의 어떤 트레이닝 포드에 있는지 아세요?
어떤 버킷에 액세스할 수 있나요?제너레이티브 AI 트레이닝
추론은 저렴하지 않습니다. 이러한 인스턴스는 수요가 많습니다.비용이 훨씬 더 비쌉니다.
표준 CPU 인스턴스.AWS는 많은 작업을 수행했습니다.
지난 5년 동안 Trainium을 낮추고, 가격 대비 성능을 개선하고, 비용을 낮추었습니다.제 생각엔 딱 어제인 것 같아요
저희는 Trainium2를 발표했고 앞으로도 계속
최저 수준의 실리콘에서 혁신을 이루어 내세요.
Trainium 워크로드에 가장 적합한 가격 대비 성능을 제공합니다.다시 한 번 말씀드리지만, 이 강의실에 계신 분들께는 EKS와 함께 작동하지 않는 이상 도움이 되지 않습니다.그리고 지난 몇 달 동안 우리는 두 Amazon Linux 2023에 가속화된 군대를 도입했습니다. 이것은 최신 일반 버전입니다.
목적: 아마존의 리눅스 배포판뿐만 아니라 우리의 컨테이너인 보틀 로켓 (Bottle Rocket)
최적화된 운영 체제.이 모든 소프트웨어와 함께 작동합니다.
다양한 EKS 컴퓨팅 옵션, 자체 관리형 노드 그룹,
카펜터 매니지먼트 그룹, 그리고 아마도 하나일 수도 있겠네요.
과소평가되거나 논의가 잘 안 되는 것들
저희가 하는 일은 무대 뒤에서 진행되는 테스트입니다. 금액은
드라이버, 프레임워크, 라이브러리, Cuda 툴킷,
예를 들어, n에 들어가는 이 모든 것들은 복잡합니다.그들은 빠르게 변화하고 있으며, 우리는
꽤 정교한 테스트 프레임워크인데, 우리가
트레이닝 작업 실행, 이전 작업 추론
이러한 ams를 출시합니다.그러니까 그냥 가져가서 실행해 보고 확신할 수 있죠.
워크로드에 적합할 거예요특히 교육 워크로드에 대해 많이 논의하는 것으로 알고 있습니다.
요즘은 카펜터 (Karpenter) 지만 노드 그룹 관리는 저희의 오리지널 관리형 컴퓨팅입니다.
5년 전에 re:Invent에서 출시한 제품이 지금 나온 것 같아요.이제 노드 그룹 관리
훈련을 자주 하기 때문에 실제로는 교육 워크로드에 적합합니다.
워크로드는 동적이지 않습니다.그냥 큰 용량만 있을 뿐이죠.
교육 작업을 실행하는 데 필요한 정적 용량 풀입니다.자주 오르락 내리락하지 않습니다.따라서 어떤 그룹도 관리하지 않는 것이 좋습니다.
거기에 딱 맞습니다.그리고 EC2를 출시했습니다.
몇 달 전에는 용량 블록 예약이라는 기능이 있었습니다.다시 한 번 말씀드렸듯이
인스턴스 유형은 수요가 많고 용량 블록은 마치 호텔과 비슷합니다.
예약 시스템이지만 EC2 인스턴스의 경우
최대 8주 전에 GPU 용량을 예약할 수 있으며 다음과 같은 이점을 얻을 수 있습니다.
특정 시기에 말이죠.최근에 그들은 예약을 연장할 수 있는 기능을 제공했는데, 이제는 기본적으로 작동합니다.
관리형 노드 그룹 포함.따라서 특히 교육 워크로드의 경우 이는 훌륭한 개선 사항이라고 생각합니다.GPU 인스턴스는 다음보다 더 자주 실패하는 경향이 있습니다.
CPU 인스턴스보다.복잡한 수학 계산을 실행하고 있죠.EC2 팀에서 하고 있는 것으로 알고 있습니다.
아시다시피 데이터 센터 수준에서 성능을 높이기 위해 많은 노력을 기울였습니다.잘 모르겠어요. 데이터 센터의 습도가 1% 변화하면 습도가 10% 증가할 수 있습니다.
GPU 인스턴스의 수명이긴 하지만 가끔 오류가 발생하기 때문에 올해 우리는 이러한 장애를 탐지하기 위해 많은 노력을 기울였습니다.따라서 인스턴스가 하나도 없습니다.
그러면 클러스터가 실제로 아무 작업도 하지 않는 동안 비용이 낭비되고 맙니다.이것은 바로 지난 주에 EKS 자동 모드로 출시되었는데, 이에 대해서는 잠시 후에 설명하겠습니다.다른 곳에서도 출시될 예정입니다.
다음 한두 주에 걸쳐 계산을 할 수 있지만 실제로는 두 부분으로 나눠서 계산해야 합니다.노드 상태 모니터링입니다.
클러스터에서 Damon이 말한 것처럼 실행할 에이전트는 모든 종류의 문제를 찾아보라고 했다.저희는 6년간 많은 쿠버네티스 클러스터를 운영한 경험이 있습니다.우리는 가능한 모든 것을 보았습니다.
노드가 실패할 수 있는 방법.그리고 우리는 그 지식을 이 노드에 적용했습니다.
GPU 인스턴스에 특히 초점을 맞춘 상태 모니터링 복구 에이전트.그리고 자동 복구 측면에서는 관리형 노드 그룹이 자동 복구 기능을 사용할 수 있게 됩니다. 이 기능을 통해 “예, EKS, 문제를 발견했군요.” 라고 말할 수 있습니다.원하는 경우가 있을 수 있습니다.
인스턴스를 재활용하기 위해서요.경우에 따라 인스턴스를 재부팅해야 할 수도 있습니다.
실패 유형에 따라 다릅니다.그리고 곧 출시될 예정입니다.그리고 마지막 슬라이드는 옵저버빌리티 모니터링에 관한 것입니다.작년 re:Invent에서 있었던 것 같아요.의 버전 2를 출시했습니다.
CloudWatch 컨테이너 인사이트는 V1보다 훨씬 낫습니다.몇 년 전에 V1을 사용해 본 사람이 있다면 문제가 있었을 것입니다.의 새 버전은
CloudWatch 컨테이너 인사이트는 정말, 정말 훌륭합니다.또한 비용도 더 효율적입니다.
컨테이너 워크로드 및 최근 개선 사항용
컨테이너 인사이트 플러그인.그리고 이걸 그냥 다음과 같이 실행하면 됩니다.
EKS 클러스터의 에이전트, 이제 자동으로 실행되나요?
NVIDIA 및 뉴런 인스턴스 유형 모두에 대한 모니터링 플러그인이 포함되어 있습니다.이러한 메트릭을 스크랩할 것입니다.
데이터를 CloudWatch로 전송하면 GP 또는 리소스가 얼마나 효율적으로 소비되는지 모니터링할 수 있습니다.
워크로드가 어떻게 실행되는지 이해하고 하이퍼 파라미터를 조정하는 방법을 알아내는 등의 작업을 하세요.해야 할 일이 정말 많습니다.
팀에서 바로 사용할 수 있는 체크박스, GPU, Nvidia 모니터링 경험을 제공하기 위해 최선을 다했습니다.그래서 강력히 추천합니다.
아직 못 보셨다면 한 번 확인해 보세요.좋아요, 얘기하고 싶어요
구체적으로 추론에 대해서요.제가 처음에 말씀드렸듯이 2024년은 생산의 해입니다.그리고 생산은 일반적으로
추론을 의미합니다.라이브 워크로드가 있는 경우
고객에게 요청을 처리하고 있으며
이로 인해 어려움이 따르는데, 특히 이런 경우에는
아시다시피, 대규모 언어 모델이죠.대규모 추론 실행
복잡할 수 있습니다. 어떻게, 어떻게 서비스를 제공하나요?
비용 효율적이고 원하는 성능을 갖춘 모델인가요?지연 시간, 처리량, 가용성.대규모 추론을 실행할 때는 이 모든 것이 문제가 됩니다.그리고 우리가 생각하는 방식도
EKS에서의 추론에 대해 실제로 이 세 가지가 있냐고요?
왼쪽의 측정값, 처리량, 아시다시피 초당 토큰 개수
모델이 서비스를 제공하고 있습니다. 지연 시간은
첫 번째 토큰에 도달하는 데 걸리는 시간과 그 다음은 비용이죠.
GPU 컴퓨팅 사용률, 실제로 사용하고 있는 GPU의 양그리고 이런 것들이 많이 있습니다.
현재 개발 중인 기능 및 프레임워크 개선
우리가 하고 있는 일은 처리량, 지연 시간, 비용 간의 균형을 맞출 수 있도록 설계되었습니다.완벽한 답은 없겠지만 알려드리고자 합니다.
이런 일을 쉽게 할 수 있게 해주는 도구들은 혼자서 할 수 있습니다.제로 스케일링, 빠른 스케일 업.ML 컨테이너 이미지 최적화.이것은 컨테이너 D 프로젝트에서 상당히 많은 작업을 하고 있는 작업입니다.아시다시피, 종종
이 ML 이미지는 수십 개이고 가끔은 수백 기가바이트를 본 적이 있습니다.그리고 인스턴스를 시작할 때 15초까지 기다리지 않아도 됩니다.
단 몇 분 동안 인스턴스는 비용이 많이 들고
이미지가 다운로드되고 있습니다.그래서 우리는 정말 많은 일을 했습니다.
제가 방금 말씀드린 것처럼 속도를 높이려면 거기서 노력하세요.
하드웨어 장애 최소화.그리고 공개적으로 많은 기여를 하고 있습니다.
이런 것들이 많이 인기가 있는지 확인하기 위한 소스 공간
프레임워크는 EKS에서 잘 작동합니다. 그리고 말하자면, Ray VLLM은 그 중 두 가지입니다.
지난 이틀 동안 고객과 대화를 나누면서 이 두 프로젝트에 대해 들어본 적이 있습니다.
현재 고객들이 프로덕션 환경에서 추론 워크로드를 처리하기 위해 다음과 같은 방법을 사용하고 있는 것이 다른 어떤 제품보다도 많이 언급되어 있습니다.
디바이스 플러그인, EFA, Nvidia 뉴런 액셀러레이티드 드라이버 등 모든 것이 매우 중요합니다.
우리가 보고 있는 일반적인 인프라 스택
EKS를 통한 추론에 사용하세요.아, 아니, 무슨 일이에요?알겠어요, 거기서 무슨 일이 벌어지는지 모르겠어요.어쨌든, 건너뛰겠습니다, 라마
카펜터에 대해서는 이미 말씀드렸지만, 카펜터는 특히 추론에 사용되는 도구입니다. 노드 관리에 대해 말씀드렸습니다.
그룹은 훈련에 좋습니다.추론은 일반적으로
훨씬 더 동적인 워크로드.Karpenter는 도움이 될 수 있기 때문에 이 분야에 적합합니다.
폭과 깊이를 더 쉽게 활용할 수 있습니다.
VC2 인스턴스 유형경우에 따라 다음이 필요할 수 있습니다.
추론에 Graviton 인스턴스를 사용하고 싶을 수도 있습니다.
반드시 GPU 인스턴스를 사용할 필요는 없습니다.일반적으로 카펜터는
추론 워크로드 분야에서 고객들이 큰 성공을 거두는 것을 보고 언론에서 뜨거운 반응을 보이는 분도 계실 겁니다. 발표 내용을 보신 분도 계실 겁니다.
지난 며칠 동안 저희가 소개한 내용은
얼마 전에 자동 모드라고 부르는데, 자동 모드의 일부로 Karpenter를 EKS에 통합하여 사용하기가 더 쉬워졌습니다.자동 모드는 일반적으로 필요한 컴퓨팅 스토리지 네트워킹인 Kubernetes 데이터 플레인을 위한 새롭고 간편한 버튼입니다.
이제 클러스터에서 실행하는 기능이 EKS에 기본적으로 포함되어 있습니다.구체적으로 말씀드리자면
이 프레젠테이션에서 다루게 된 것은 카펜터 앵글 때문입니다.카펜터는 추론에 적합합니다. 이제 기본적으로 내장되어 있습니다.그 중 일부는, 아시다시피
오른쪽의 세 가지 요소로는 컴퓨팅 최적화, 비용 효율성, 다양한 워크로드 지원이 바로 Karpenter에 내장되어 있습니다.그게 바로 이 회사가 하는 일이죠.아직 어떤 프레젠테이션에도 가보지 않으셨다면, 자동 모드를 통해 이뤄낸 흥미로운 혁신이 바로 그것입니다.
하지만 EC2에는 이런 새로운 운영 모델이 있습니다.이전에 사용하던 곳
완전히 가지고 있던 AWS에서 컴퓨팅을 실행하는 두 가지 방법
컴퓨팅이 실행되는 관리형 람다 파게이트 모델
AWS 소유 계정 또는 표준 EC2를 사용하는 경우
여러분이 소유한 것은 EC2에서 관리하는 새로운 모델이었습니다.
그들이 호출하는 인스턴스는 그 중심에 있습니다.
또는 계정의 EC2 인스턴스일 수도 있습니다.하지만 EKS와 같은 AWS 서비스가 실제로 소유하고 있습니다.
해당 인스턴스의 수명 주기.따라서 E와 I를 삭제하려고 하면 인스턴스를 삭제하면
서비스에서 사용 중이라는 오류가 발생하면
EKS를 통해 사용해야 합니다.제가 이 이야기를 꺼낸 이유는
특히 여기 맨 위에는 EKS에서 머신 러닝 워크로드를 시작하는 데 필요한 기능, 어려운 부분이 많다고 생각합니다.
적합한 디바이스 드라이버, EFA 플러그인, 구성
인스턴스 스토리지.로컬 디스크가 있는 경우
예전에는 고객이 직접 해야 하는 경우가 있었습니다.
정말 복잡한 사용자 데이터 스크립트를 사용하여 RAID 0을 설정하세요.
인스턴스의 볼륨.이제 그 중 많은 부분이 완전해졌습니다.
EKS 자동 모드로 자동화되었습니다.따라서 Raid zero를 자동으로 구성하고 자동으로 구성합니다.
Nvidia이든 Neuron이든 관계없이 장치 플러그인은 이미 포함되어 있습니다. GPU 인스턴스를 시작하면 적절한 옴니를 선택할 수 있습니다.
해당 인스턴스와 일치합니다.그러니까, 생각해야 할 일이 훨씬 적다는 거죠.트레이닝 워크로드에 대해서는 잘 말하지 않겠습니다.
아직은 자동차가 아주 적합합니다. 인스턴스 수명은 최대 21일입니다.경우에 따라 몇 개월이 걸리는 교육 작업을 진행하는 고객과 이야기를 나누는 경우가 많습니다.하지만 추론 워크로드, 프로덕션 실행 모델 및 프로덕션 자동 모드의 경우
잘 맞을 수도 있겠네요.우리가 보고 있는 꽤 흔한 스택의 예일 뿐이죠.다시 말씀드리지만, 이것은 이미 검증되었습니다.
지난 이틀 동안 고객과 대화를 나눴는데 Ray, VLLM, 그리고 아시다시피 카펜터를 사용하고 계시는데, 곧 가동될 것입니다.
헤드 포드를 구동하기 위한 표준 CPU 인스턴스와
큐브레이 오퍼레이터를 사용하면 예를 들어 추론 인스턴스가 가동되어 실제로 서빙 워크로드를 실행할 수 있습니다.그래서 이건 점점 더 많아지고 있습니다.
실제로 고객들이 흔히 볼 수 있는 패턴입니다.
EKS를 사용하여 프로덕션 환경에서 추론 워크로드를 실행합니다.그리고 마지막으로,
몇 가지 고객 사례를 더 보자면, H2O, Omi, Unitary와 같은 모든 고객들, 이 모든 고객들이 전부라고 생각합니다.
사례 연구든, 블로그든 모두 읽어 볼 수 있습니다.이들은 모두 추론 워크로드를 처리하기 위해 Karpenter로 이전했습니다.제 생각에 H2O는
흥미로운 건 병형 로켓을 사용했다는 겁니다.
컨테이너 이미지를 프리페치하기 위해서요.그 방법을 알려주는 멋진 블로그가 있습니다.15분이 걸린다고 말씀드렸는데, 최적화가 가능하지만 가장 빠른 방법은
사실 컨테이너 이미지를 미리 로드하기만 하면 됩니다.
Bottle로 할 수 있는 인스턴스 자체
이제 아주 쉽게 로켓을 만들 수 있습니다.그리고 다른 것들은, 네, 그냥 카펜터를 사용해서 비용을 줄였죠.우리는 이런 것들을 많이 봤어요.
고객은 Karpenter를 통해 성공적으로 이전했습니다.
고객의 추론 워크로드좋아, Cas에게 넘겨서 다음 중 하나에 대해 이야기하겠습니다.
그 고객 중 한 명인 그 고객들은
EKS를 성공적으로 활용했네요. - 좋아요, 고마워요 마이크.안녕하세요 여러분, 저는 카스 스타락입니다. 저는 제품 매니저 리더입니다.
엘리 릴리 앤 컴퍼니에서는 연구와 AI 제품에 집중하고 있습니다.그리고 조금만 말씀드릴게요.
이는 EKS에서 차세대 AI 기능을 구축한 고객 경험에서 나온 것입니다.먼저 잠깐 얘기해 볼까 생각했죠.
Lilly의 기술 발전에 대해 조금 설명해 주세요. 도움이 될지도 몰라요
Eli Lilly에서 온 누군가가 EKS에 가입한 이유를 설명해 주세요
re:Invent의 Gen AI 트랙입니다.에 대해 조금 얘기해 볼게요
EKS와 Gen을 포함한 우리의 플랫폼 개발
AI 스케일업은 현재 진행 중입니다.그러니까 조금만
역사적 맥락을 말씀드리자면, 여기 사진들은
기술을 바탕으로 한 릴리의 역사를 조금이나마 엿볼 수 있습니다.
왼쪽, 실제로는 1987년이고 전형적인 80년대의 모습입니다.
기업 경영진들, 이 일에 매우 흥분하고 있습니다.
가장 최근에 출시된 일부 개인용 컴퓨터 기술그리고 오른쪽에는 몇 가지가 있습니다.
그게 뭔지 아실 수도 있고 거기에 있는 텍스트를 보실 수도 있습니다. 사실 Cray-2 슈퍼컴퓨터죠.그리고 1989년에 릴리가 최초의 컴퓨터였죠.
그 중 하나를 미국에서 구입한 것은 비정부기구입니다.말씀드리자면, 1년 전에 릴리에 합류한 후 기분 좋게 놀랐기 때문입니다.
초기 기술 채택과 혁신의 역사에 대해 배우기 위해 저는
그냥 150년 된 회사에서 찾을 수 있을 거라고는 예상하지 못했어요.그리고 거기, 있잖아요,
소프트웨어 개발, 데이터 사이언스,
이러한 기술을 도입하는 데 필요한 모든 것.안타깝게도 거기서부터 올라가는 궤도만 있는 게 아니었어요. 소위 말하는 AI 겨울처럼 말이죠.
산업은 발전했습니다.우리만의 기술 혁신이 있었던 겨울이 있었는데, 실제로 2001년 즈음에 시작되었는데, 이를 용어로 X년이라고 부릅니다. 그리고 그 해는 최초의 Prozac이 탄생한 해였습니다.
특허가 줄고 특허에서 사라지기 시작했죠.그 당시에는 실제로 릴리 수익의 1/3이었죠.다른 약도 몇 가지 더 있었습니다.
그건 특허에서 사라졌어요.판매라고 상상하실 수 있겠죠.
극적으로 보면 비용에 대한 검토가 많이 이루어졌고 비용도 낮췄고
아시다시피, 인용하자면, 핵심이 아닌 것들이었죠.
예를 들어 소프트웨어 개발과 그 기술 중 일부
혁신은 큰 타격을 입었습니다.그래서 제 생각에는 이런 긴 기간이 있었을 것 같은데요.
초기의 혁신은 조금은 위축되기 시작했습니다.하지만 오늘 당신은 앞을 내다보셨군요.
완전히 뒤집혔고 인식이 생겼습니다.
회사 전체에 걸쳐 우리에게 정말 필요한 것은
계속 성공하기 위한 기술 회사가 되는 것
의약품 회사로서 앞으로 몇 년, 수십 년 동안 말이죠.그리고 고맙게도 우리는
최근에 나온 몇 가지 의약품으로 성공을 거두었기 때문에
매우 탄탄한 파이프라인을 갖추고 있어 투자할 수 있게 되었습니다. 그리고 이러한 기술 역량, 소프트웨어 엔지니어링 역량, 로봇 자동화, 데이터에 상당한 투자와 성장이 이루어졌습니다.
과학, 모든 유형의 AI.그리고, 아시다시피
앞으로 더 많은 내용이 추가될 예정입니다.그리고 아시다시피, 그 중 하나는
이 진화를 거치면서 우리가 하고 있는 일들은
투자는 실제로 현대 소프트웨어의 대부분을 채택하려고 노력하고 있습니다.
개발 사례.그 중 일부는, 아시다시피 팀이 단 몇 명만에 구성되었다는 것입니다.
몇 년 전에 저도 그 팀에 속했었죠.이게 바로 소프트웨어입니다.
제품 엔지니어링 팀.전직 직원이 이끌고 있습니다.
애플의 엔지니어링 리더 고 콜 래드 크리슈넌.그리고 이 팀은 정말 집중하고 있어요.
다음과 같은 확장된 제품과 플랫폼을 구축하는 데 집중하고 있습니다.
신뢰성과 사용량이 높아질 것입니다.저희는 정말 독특한 접근 방식을 취하려고 노력하고 있습니다.
Lilly의 개발자들에게 이러한 관행을 시연하고 널리 퍼뜨리는 데 도움을 주는 소프트웨어 엔지니어링 우수 센터입니다.그리고 그 중 큰 부분이 바로
우리가 생각하고 있는 이 플랫폼 접근법은
공유 라이브러리, 재사용 가능한 컴포넌트에 대해서요.우리는 많은 것을 활용하는 경향이 있습니다.
오픈 소스 기능 및 도구, 그리고 조직 전체에서 사용할 수 있는 광범위한 옵저버빌리티 도구 구축과 같은 작업을 수행합니다.그리고 그 플랫폼 중 하나는
이 CATS 플랫폼은 저희에게 정말 큰 투자였어요.그래서 이것은 클라우드 애플리케이션이자 서비스형 기술입니다.이게 뭐냐면
포괄적인 클라우드 애플리케이션 개발 및 호스팅 솔루션.그래서 우리는 이를 AWS에 구축했고 일련의 오픈 소스 및 AWS 관리형 서비스를 사용하여 이를 지원합니다.이 다이어그램은 매우 단순화되었지만 몇 가지 핵심 사항은
아시다시피, 쿠버네티스 관리에는 EKS를 사용하고, 컨테이너 실행에는 EC2와 Fargate를 사용하고, 다른 AWS도 있습니다.
여기에 통합된 서비스 (예: S3 RDS, 비밀 관리자,
정말 많은 것들이 있죠.우리는 또한 GIT 운영, CICD 자동화도 수행합니다.따라서 팀은 커밋부터 프로덕션까지 원활하게 진행할 수 있습니다.GitHub Actions로 시작해서 도커 이미지를 ECR로 푸시하면 Argo가 이를 감지하고
이를 적절한 쿠버네티스 환경에 자동으로 적용합니다.자, 아시다시피
개발자들에겐 정말 새로운 개발 방식이었다는 뜻이죠. 이제 개발 방식을 바꾸었죠.
클라우드는 속도와 효율성이 뛰어나므로 많은 인프라를 만들 필요가 없습니다.
의사 결정은 말할 것도 없고 인프라를 구축하는 것은 말할 것도 없고, 시작할 수 있는 메뉴를 마련해 놓으면 자동화 배포와 같은 속도를 얻을 수 있습니다.이런 부분이 저희 기술 혁신의 큰 부분을 차지했어요.
클라우드로 진화하면서 상당한 활용도가 생겼습니다.또한 보안도 제공합니다.그래서 표준화를 많이 해서 사람들이 따라갈 수 있도록 몇 가지 경로를 짜는 거죠.그래서 그들은 강력한 보안 관행을 도입할 수 있습니다.취약점 스캐닝 기능을 갖추고 있으며 관찰 기능도 제공했습니다.
도구, 그 중 일부는 아마존 서비스와 함께 제공됩니다.일부 오픈 소스도 사용합니다.
로키와 그라파나 같은 도구들.그래서 팀들이 이걸 기반으로 작업을 할 수 있도록
로깅, 경고, 보고를 많이 받죠.
시각화는 그들이 자신의 일을 관리하는 데 도움이 될 수 있습니다.
확장에 따른 애플리케이션.그리고 마지막 부분은
실제로 가장 큰 장점은 규모와 안정성입니다.그러니까, 아시다시피, 일단
여기서 애플리케이션을 개발하면 확장될 수 있습니다. 아시다시피
Lilly의 모든 인력은 전 세계적으로 확장할 수 있지만 실제로 그럴 수는 없습니다.
팀에 추가 작업이 들어갔지만 여전히 높은 수준을 유지하고 있습니다.
그럴 때 안정성이 보장되죠.앞서 말씀드린 것처럼, 우리는
지난 몇 년 동안 상당한 규모의 채택이 이루어졌습니다.
이 플랫폼을 몇 년 동안 사용했었죠.수백 명의 개발자가 이 플랫폼을 사용하고 있습니다.애플리케이션 수가 증가하고 커밋 수도 증가하고 있습니다.
아시다시피 많은 국가에서 글로벌 배포가 기하급수적으로 증가하고 있습니다.제가 좋아하는 지표 중 하나는 여름이 끝날 무렵에 강연에 참석했는데 거의 같은 느낌이 들었다는 것입니다.
모든 인턴이 CATS가 어떤 용도인지, 그리고 CATS가 개발 속도를 높이는 데 어떻게 도움이 되었는지에 대해 이야기했습니다.
그들의 인턴 프로젝트는그래서 좋은 것 같아요.
그 효과의 척도요.상상하실 수 있겠죠.
이 제품의 사용량 증가 중 일부는
이는 AI 세대로부터 나온 것이며 저희가 만들어 오고 있습니다.
거기 큰 투자를 했죠.몇 가지 맥락을 말씀드리자면, 릴리가 실제로 하고자 하는 일은 차세대 AI의 영향을 도입하는 데 있어 제약 업계 선두주자입니다.그리고 저희는 그렇게 생각합니다.
우리가 하고 있고 이미 투자하고 있는 많은 일을 근본적으로 혁신하고 있습니다.그래서 릴리는 실제로
신약 발견에 AI를 사용한 역사는 작지만
거대 분자의 스크리닝을 위한 도구도 있고, 사실 꽤 괜찮은 게 있죠.
모든 LMS가 나오기 전부터 NLP 작업을 해왔던 탄탄한 데이터 과학팀이었죠. 임상 요약 같은 일을 하기 때문이죠.그 역사가 하나 있습니다.
이를 기반으로 발전하고 있지만, 실제로는 차세대 AI에 집중하고 있습니다. 그리고 우리는 이를 바탕으로 작업하고 있습니다.
아시다시피 하이퍼스케일러는 이제 막 만들어지고 있습니다.
솔루션: 소규모 기업, 스타트업과 협력하여 조직별 또는 사용 사례별 솔루션을 더 많이 개발하고 있습니다.하지만 우리는 다음과 같은 작업도 진행하고 있습니다.
내부 개발도 많이 했고 이 기술의 일부로서
우리가 겪고 있는 변화, 우리는 어디에 있는지 찾고 있습니다.
우리가 그런 걸 좀 갖는다는 게 말이 되나요?
내부적으로 능력이 있나요?그 중 일부는 용도입니다.
사례와 개념 증명, 하지만 실제로 우리는 집중하려고 노력합니다.
회사에 서비스를 제공할 수 있는 확장된 제품과 플랫폼이 무엇인지에 대해 말이죠.그리고 플랫폼 중 하나는
우리가 구축하기로 결정한 것은, 널리 사용할 수 있는 개발자를 위한 차세대 AI 플랫폼으로, 릴리에서 이러한 작업을 가속화할 수 있도록 도와주기로 했습니다.사실 여기선 조금은 할 수 있어요.그래서 우리는 AI 세대가
규모가 커질 거고, 안정적이길 바랐고, 우리는 원했습니다.
전 세계적으로 많은 사용자를 확장할 수 있도록 말이죠.그래서 우리는 그것을 만들기로 결정했습니다.
방금 안내해 드린 CATS 플랫폼 위에서요그리고 우리는 이러한 주요 구성 요소를 모아 2023년에 개발을 시작했습니다.첫 생산을 진행했습니다.
12월에 출시되었으며 이전과 마찬가지로 다양한 기능을 추가하고 있습니다.
한 해 동안 말이죠.기초 수준에서
모델 라이브러리이기 때문에 팀이 다음 중에서 선택할 수 있기를 바랍니다.
최고의 최신 LLM, 그리고 저희는 지속적으로
업데이트하고 있습니다. 아시다시피 저희는 열렬한 팬입니다.
인위적인 모델 라인도 있지만 오픈 AI, 쌍둥이자리, 포옹하는 얼굴, 라마도 있죠.거의 모든 제품을 배포할 수 있습니다.
법적 승인을 받으면 얼굴을 꼭 껴안고 모델링을 하세요.그리고 저희는 팀원들이 할 수 있도록 허용하죠.
선택하시면, 소매점으로 호스팅할 수 있고, 오픈 소스도 있고, 미세 조정 모델도 있고, 로컬에서 호스팅할 수도 있습니다.
GPU 클러스터에서 말이죠.그리고 그 위에 우리는
오케스트레이션 도구를 만들었죠.그래서 시작은 아주 간단했고 우리는 랭 체인을 우리의 것으로 사용했습니다.
메인 오케스트레이션 프레임워크로 시작해서 공정하게 시작했습니다.
신속한 엔지니어링과 모델 체인을 위한 몇 가지 도구를 사용하면 간단합니다.그리고 우리는 점점 더 많은 기능을 추가했습니다.
이제 복잡한 오케스트레이션 도구를 통해 에이전트 워크플로와 다중 에이전트 시스템을 지원할 수 있습니다.그리고 운영, 규모 조정 및 유지 관리를 위한 도구도 있습니다.그래서 저희는 주의를 기울이려고 노력합니다.
팀들이 걱정할 필요가 없도록 많은 규모 조정이 필요하겠죠.그래서 저희 중앙 팀은 작전 팀을 염두에 두고 있습니다.
용량 프로비저닝에 대해서요.우리는 최근에 다음과 같이 작업했습니다.
AWS 팀은 교차 리젠트 추론 및 속도 제한 용량과 같은 일부 기능을 최적화하여 더 나은 성능을 얻을 수 있도록 지원합니다.
규모에 맞게 지연 시간이 짧습니다.그래서 저희는 이렇게 하고 있습니다.
그런 것들을 중앙에서 관리해서 다른 팀들이 하지 못하도록
품질 및 모니터링뿐만 아니라 이에 대해서도 걱정해야 합니다.그래서 우리는 다양한 제품을 만들었습니다.
팀이 규모를 확장할 때 사용할 수 있는 평가 도구
편차를 모니터링하거나 평가를 원하는지 모니터링할 수 있습니다.
최신 모델이라 쉽게 확인할 수 있습니다.
성능이 어떻게 변하는지데이터도 있습니다.
아시다시피 구체적인 통합
릴리 환경에 대해서요.우리는 항상 새로운 기능과 정보 검색 도구를 추가하려고 노력하고 있습니다.저희는 상당히 단순한 의미론적 검색 기능을 갖춘 벡터 데이터베이스로 시작했는데, 점점 더 복잡해져서 그 요소들을 조정할 수 있게 만들었죠.따라서 팀에서 최적화를 원하는 경우, 알다시피,
청크 사이즈와 오버랩은 팀원들이 들어가서 해낼 수 있죠.그리고 나서 문맥 청킹과 하이브리드 검색과 같은 다른 것들을 추가하기 시작했고 계속해서 증명해 나갔습니다.
정보 검색.다시 말씀드리지만, 팀에게는 그럴 필요가 없습니다.
나가서 그걸 만들어야죠.일종의 최적화 기능도 있고요.
릴리 환경에서는 이 모든 것이 규정 준수 및 보안 계층의 감독 속에 담겨 있습니다.따라서 전체 입력 출력 로깅이 가능하기 때문에 사이버에서 모든 구성과 모든 사용자에 대한 가시성을 확보할 수 있습니다.또한 스캐닝과 알림 기능도 있는데, 이는 사이버 팀에서 구성할 수 있습니다.일부 AWS 도구와 일부 개방형 도구를 사용합니다.
이를 위한 도구.그리고 기부하는 데 많은 도움이 되죠.
우리 사이버 팀을 안심시키고 실제로 어떤 점이라도
이 플랫폼이 가장 선호되는 플랫폼입니다.그리고 그게 뭘 가지고 있는거죠?
다시 한 번 말씀드리지만, 개발을 가속화할 수 있게 되어 팀이 할 수 있게 되었습니다.
매우 빠르게 시작하고 POC와 MVP에 빠르게 도달할 수 있습니다.선택사항도 있습니다.아까 그 사람들 얘기 들으셨잖아요
아시다시피 다양한 모델의 중요성에 대해 이야기해 보세요.
사용 사례에 따라 다릅니다.앤디가 얘기한 것 같아요
어제 그의 기조연설에서 그랬어요.그래서 팀원들이 쉽게 할 수 있습니다.
앞뒤로 전환해서 어떤 모델인지 알아내세요.
해당 사용 사례에 적합합니다.제가 말씀드렸듯이, 품질
보안 규정 준수가 내장되어 있습니다.그리고 우리가
AWS 기반 EKS에 구축했더니 확장성이 매우 뛰어납니다.그래서 다음과 같은 사용 사례가 있었습니다.
아시다시피 소규모 초기 테스트에서 배포로 넘어갔습니다.
아시다시피 전 세계적으로 중대한 문제나 다운타임이 발생하지 않고 팀에서 많은 노력을 기울일 필요도 없습니다.
실제로 개발을 하고 있습니다.그래서, 아시다시피, 우리는
원래는 2023년 12월에 생산을 시작했습니다.알다시피, 1년도 안 됐는데 우리는 그 활용에 깊은 인상을 받았고 정말 큰 영향을 미치고 있습니다.
우리 비즈니스의 모든 부분에서 이러한 변화가 일어나고 있습니다.몇 가지 예를 들자면
아마도 제가 가장 좋아하는 것은 이 에이전트 디스커버리 어시스턴트일 것입니다.화학 정보학 담당자가 있습니다.약 20개의 다양한 도구를 이용할 수 있습니다.그 중 일부는 케민입니다.
정보학 도구 중 일부는 릴리 분자 정보가 있는 내부 데이터베이스입니다.외부 데이터베이스이고 LM은 다음과 같습니다.
질문에 답하거나 실행하는 것, 아시다시피
사용자 프롬프트 워크플로우 및 이러한 도구 중 사용할 도구 선택그리고 이런 것들이 바로 가능하죠.
과학자들에게 도움을 줄 수 있는 강력한 워크플로를 실행할 수 있어야 합니다.
발견 초기 단계에 있습니다.우리 안에는 많은 것들이 있습니다.
임상 실험 공간, 그러니까 우리가 어떻게 하는지 도와줘요
규제 기관에 대응하세요.우리는 수천 개의 질문을 받고 다음과 같은 질문에 답해야 합니다.
전 세계 규제 기관.제조 공간에 도구가 있으면 LM이 도와주는 것은 당연한 일입니다.우리는 글로벌 제조 시설을 보유하고 있기 때문에 우리의 분야에 큰 초점을 맞추고 있습니다.
제조 라인 가동 시간.그래서 이 어시스턴트는 구조화 및 구조를 사용하는 이 플랫폼을 사용하여 개발되었습니다.
라인 운영자에게 도움이 되는 장비 센서 데이터뿐만 아니라 같은 SOP에서 수집한 비정형 데이터
업타임을 모니터링하고 관리합니다.클레임 초안 작성 도구가 있으며 개발 중이기도 합니다.
플랫폼 최초의 환자 대면 Q&A 도구.이러한 채택을 보게 되어 정말 기쁩니다.그리고 결과 측면에서 보면 가속화되는 것을 볼 수 있었습니다. 그래서 팀들로부터 일화적인 이야기를 들었고, 그 다음엔 팀을 봤죠.
아시다시피, 몇 주 안에 그들은
POC를 출시하거나 몇 달 안에 출시할 수 있습니다. 실제로 MVP는 이러한 기능을 갖추고 있기 때문에 프로덕션 단계로 밀려날 수 있습니다.
도구가 준비되어 있고 사용할 수 있습니다.품질을 충족할 수 있습니다.
보안 규정 준수.정말 선호도가 높아지고 있습니다.
저희 사이버 팀이 만든 플랫폼, 그리고 아시다시피
사람들이 이걸 사용하도록 권장하세요. 우리에겐 이 모든 것이 있으니까요.
가시성, 로깅, 알림 등을 통해 빠른 확장이 가능해졌고 많은 채택이 이루어졌습니다.그래서 500명이 넘는 개발자가 있습니다.
Lilly에서 플랫폼을 사용하고 있습니다.끝은 수천 개야
30개 이상의 국가에서 활동하는 사용자들, 그리고 한 달간 우리는
플랫폼에서 처리하는 토큰은 수십억 개입니다.자, 이제 규모가 커졌네요.
알다시피, 우리가 바라던 것이 바로 이것을 뒷받침하고 있습니다.
릴리 에볼루션의 기술.분명히 우리는 건물을 짓고 있는 중이에요.
그 위에 있는 제품들은 영향력이 크지만, 왜냐하면
우리가 하고 있는 일은 이런 종류의 것이기도 합니다.
아시다시피 인재를 유치하고 유지하는 선순환 구조죠.
이 문제를 해결하고 싶어하는 거죠.제가 말씀드렸던 변화를 지원하는 거죠.그리고 마지막으로 말씀드리자면
끝으로, 몇 가지 교훈을 얻었는데요, 제 생각에 가장 중요한 교훈은 이런 플랫폼 접근 방식을 통해 확장 가능한 플랫폼을 사용한다는 우리의 믿음과 직감입니다. 예를 들어 EKS처럼 이런 독단적인 접근 방식을 사용한다는 것입니다.
차세대 AI 개발에 대해서요.효과가 있었고, 개발자들이 사용하고 있고, 가속화되고 있습니다.
사물이 확장되고 프로덕션 환경으로 옮겨가는 것을 목격하고 있습니다.그러니까 아마 그럴 것 같아요.
가장 큰 교훈을 얻었죠.하지만 다른 몇몇은
그 과정에서 생긴 것들은, 만들면, 알다시피, 몇몇은 올 거예요.따라서 이런 플랫폼을 만들면 자연스럽게 사용하기 시작하는 얼리 어답터 몇 명이 생길 것입니다.특히 다음과 같은 곳에서는
릴리, 당신은 일종의 곡선을 그리게 될 거예요
개발자 팀은 정말 유능하고 빠르게 움직이지만, 그 뒤에는 전체 배포 곡선이 있습니다.그리고 더 많은 것을 얻고 싶다면
채택을 위해서는 복음화, 질문에 답하기, 지원 문서 작성, 지원 팀 구하기, 제품 사고방식 등을 다루기 위해 많은 노력이 필요합니다.그래서 우리는 이걸 정말 제품처럼 취급했어요.제품 관리자 또는 로드맵이 있습니다.저희는 정말 항상 노력하고 있습니다.
고객의 의견을 수렴하여 최신 제품을 만들게 되어 매우 기대가 될 수도 있습니다.
정보 검색 파이프라인이 점진적으로 개선되고 있습니다.하지만 고객들은 “여러분의 QA 환경이 좋으면 좋겠습니다.” 라고 말합니다.
조금 더 안정적이었어요.그래서 우리가 그러지 않도록 하기 위해서요.
IT의 엔지니어링과 기술에 너무 몰두하고 있지만, 고객에 대한 확고한 입장을 유지하는 것이 중요했습니다.그러자 기대치가 급격히 높아졌습니다.그래서 처음에는 회의적인 시각이 있었죠. 어떤 사람들은
Lilly의 개발자 여러분, 이건 마치 그들을 채널링하는 새로운 접근 방식 같아요.
특정 플랫폼에 몇 가지 제약을 가하고 있습니다.하지만, 아시다시피, 회의적인 시각은
금방 돌아섰어요. 아, 알겠어요, 속도가 빨라지고 있네요.
빨라졌어요. 더 쉬워지고 있어요.승인을 받고
입양이 눈앞에 다가왔는데, 아, 이렇게 변했어요
이제 이거, 이거, 이거 갖고 싶은데 문서 어딨어요?공식 SLA란 무엇입니까?따라서 이러한 접근 방식을 취하고 있다면 미리 생각해 보아야 한다고 말씀드리고 싶습니다.플랫폼을 구축하는 중이라면 문서를 잘 읽고 지원 서비스를 계속 받아가세요. 아시다시피 SLA를 제대로 전달하기 전에 미리 준비하세요.특히 릴리 같은 곳에서는 참을성 있는 얼굴을 마주하는 일이 많기 때문이죠.
우리는 제조 제품을 가지고 있고, 연중무휴로 기대가 됩니다.그리고 마지막으로 말씀드렸죠.
사이버 파트너십과 이를 위한 초기 투자
제 생각에는 이 플랫폼 성공의 핵심이었다고 생각해요.
특히 릴리와 같이 사이버 또는 개인 정보 보호 법률 등에 대한 기준이 높은 회사에서는 더욱 그렇습니다.그래서 그걸로 할 수 있을 것 같아요.
라마한테 다시 넘겨 줘. - 고마워요 카스, 고마워요
시간을 내어 그 멋진 통찰력을 우리와 공유해 주셨어요.그래서 이번 프레젠테이션 내내 많은 이야기를 나눴던 부분이 있습니다.
고객들이 저희와 통합하기를 원한다는 것이었습니다.
원하는 ML 기능을 달성하기 위한 EKS 기반 솔루션으로 사용됩니다.그래서 많은 고객들이 우리에게 방법에 대한 지침을 요청했습니다.
이러한 솔루션 통합을 빠르게 시작하기 위해서요.모범 사례는 무엇인가요?
통합을 위한 건가요?원하시는 템플릿과 패턴을 제공해 주실 수 있나요?
성공을 거두고 계신가요?그래서 우리가 론칭한 거예요.
EKS 프로젝트에 관한 데이터.오픈 소스 프로젝트로, 고객 전반에서 본 다양한 세대 AI 패턴을 게시합니다.청사진을 게시하고,
일부 통합을 쉽게 시작하는 데 사용할 수 있는 패턴, 테라폼 템플릿
이러한 ML 관련 문제를 해결하기 위해 널리 사용되는 OSS 솔루션이 EKS와 통합되고 있는 것으로 보입니다.그래서 우리는 다음과 같은 제품을 출시했습니다.
지난 한 해 동안 이런 패턴이 많이 생겼습니다.우리는 계속해서 더 많은 것을 만들고 있습니다.
패턴은 실제 세계에서 그 패턴으로 성공을 거두고 있습니다.우리는 추론에 특화된 정보를 공개하는 데 특히 초점을 맞추고 있습니다.
EKS 기반의 패턴.우리가 하는 것들 중 일부는
최근에 출시한 것은 VLLM을 탑재한 엔비디아, VLLM을 탑재한 레이서, EKS의 엔비디아 NIMS 등입니다.그럼 우리의 데이터를 확인해 보세요
EKS 웹 사이트에서는 추론, 학습 및 미세 조정을 위한 다양한 패턴을 볼 수 있습니다.이를 통해 거의 모든 것이 가능해졌습니다.
자, 이제 끝내세요.흥미로운 게 몇 가지 있어요.
EKS, 아마존 EKS와 함께 준비될 세션들
코드로서의 인프라 gitops 세션도 이에 관한 것입니다.그리고 한 가지 흥미로운 점이 있습니다.
EKS에 세대 AI를 구현한 S&P Global의 세션과 해당 설정을 사용하여 주당 수백만 건의 추론 요청을 충족하도록 확장하는 방법또한 쿠버네티스의 미래 온 AWS 세션도 내일 예정되어 있으니 확인해 보세요.다음 교육 과정 중 일부를 통해 Amazon EKS 학습을 계속할 수 있습니다.
워크숍이나 워크숍을 통해 이용할 수 있는 리소스
디지털 배지와 모범 사례 가이드도 받을 수 있습니다.이를 통해 우리는
세션이 끝났습니다.인내심을 갖고 경청해 주셔서 감사합니다. 질문이 있으시면
한동안 방 밖에서 놀게 될 거예요우리에게 다가오시면 어떤 질문에든 답변해 드릴 수 있습니다.
질문도 해주시면 감사하겠습니다.감사합니다.(박수를 치는 사람들)